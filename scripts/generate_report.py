"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å—
åŠŸèƒ½ï¼šç”ŸæˆMarkdownæ ¼å¼çš„æ¨èæŠ¥å‘Šï¼ŒåŒ…å«æ¨èç†ç”±å’Œç¿»è¯‘
"""

import json
import os
import logging
from typing import List, Dict
from datetime import datetime
from zhipuai import ZhipuAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_config() -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), '../config/config.json')
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def call_zhipu_ai(prompt: str, model: str = "glm-4-flash") -> str:
    """è°ƒç”¨æ™ºè°±AI"""
    config = load_config()
    client = ZhipuAI(api_key=config['zhipu_api_key'])
    
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=2000
    )
    
    return response.choices[0].message.content


def generate_recommendation(topic: Dict) -> str:
    """
    ç”Ÿæˆè¯é¢˜æ¨èç†ç”±ï¼ˆä¸è¶…è¿‡300å­—ï¼‰
    
    Args:
        topic: è¯é¢˜å¯¹è±¡
    
    Returns:
        str: æ¨èç†ç”±
    """
    logger.info(f"æ­£åœ¨ç”Ÿæˆæ¨èç†ç”±: {topic['canonical_name']}")
    
    # è·å–è¯¥è¯é¢˜çš„å‰3ç¯‡æ–‡ç« æ ‡é¢˜ä½œä¸ºå‚è€ƒ
    article_titles = [art['title'] for art in topic['articles'][:3]]
    
    prompt = f"""è¯·ä¸ºä»¥ä¸‹æŠ€æœ¯è¯é¢˜æ’°å†™ä¸€æ®µå…¬ä¼—å·é€‰é¢˜æ¨èç†ç”±ï¼ˆä¸¥æ ¼ä¸è¶…è¿‡300å­—ï¼‰ï¼š

è¯é¢˜ï¼š{topic['canonical_name']}
ç±»åˆ«ï¼š{topic['category']}
çƒ­åŠ›å€¼ï¼š{topic['heat_score']}/100
ç›¸å…³æ–‡ç« æ•°ï¼š{topic['total_mentions']}ç¯‡
å¹³å‡è®¨è®ºæ·±åº¦ï¼š{topic['avg_depth']:.2f}/1.0

ä»£è¡¨æ€§æ–‡ç« æ ‡é¢˜ï¼š
{chr(10).join(['- ' + title for title in article_titles])}

æ’°å†™è¦æ±‚ï¼š
1. è¯´æ˜ä¸ºä»€ä¹ˆè¿™ä¸ªè¯é¢˜å€¼å¾—å…³æ³¨ï¼ˆæŠ€æœ¯ä»·å€¼ã€è¡Œä¸šæ„ä¹‰ï¼‰
2. è¯¥è¯é¢˜ç›®å‰çš„è®¨è®ºçƒ­åº¦å’Œæ·±åº¦
3. å¯¹å…¬ä¼—å·è¯»è€…çš„ä»·å€¼ï¼ˆæŠ€æœ¯æ´å¯Ÿã€è¡Œä¸šè¶‹åŠ¿ã€å®æˆ˜æ„ä¹‰ç­‰ï¼‰
4. è¯­æ°”ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚åˆç§‘æŠ€åª’ä½“é£æ ¼
5. **ä¸¥æ ¼æ§åˆ¶åœ¨300å­—ä»¥å†…**
6. ä¸è¦ä½¿ç”¨"æœ¬è¯é¢˜"ã€"è¯¥è¯é¢˜"ç­‰è¯æ±‡ï¼Œç›´æ¥æè¿°è¯é¢˜å†…å®¹

æ³¨æ„ï¼šåªè¿”å›æ¨èç†ç”±æ–‡æœ¬ï¼Œä¸è¦åŒ…å«"æ¨èç†ç”±ï¼š"ç­‰æ ‡é¢˜ã€‚"""

    try:
        recommendation = call_zhipu_ai(prompt)
        
        # ç¡®ä¿ä¸è¶…è¿‡300å­—
        if len(recommendation) > 300:
            recommendation = recommendation[:297] + "..."
        
        return recommendation.strip()
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¨èç†ç”±å¤±è´¥: {e}")
        return f"{topic['canonical_name']}è·å¾—{topic['heat_score']}çš„é«˜çƒ­åŠ›å€¼ï¼Œ{topic['total_mentions']}ç¯‡æ–‡ç« æ·±å…¥è®¨è®ºï¼Œå€¼å¾—å…³æ³¨ã€‚"


def translate_articles_batch(articles: List[Dict]) -> List[Dict]:
    """
    æ‰¹é‡ç¿»è¯‘æ–‡ç« æ ‡é¢˜å’Œæ‘˜è¦
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
    
    Returns:
        List[Dict]: ç¿»è¯‘åçš„æ–‡ç« åˆ—è¡¨
    """
    logger.info(f"æ­£åœ¨æ‰¹é‡ç¿»è¯‘ {len(articles)} ç¯‡æ–‡ç« ...")
    
    # æ„å»ºè¾“å…¥
    articles_input = []
    for i, art in enumerate(articles):
        articles_input.append({
            'id': i,
            'title': art['title'],
            'summary': art.get('summary', '')[:500]  # å¢åŠ æ‘˜è¦é•¿åº¦ä¾›ç¿»è¯‘å‚è€ƒ
        })
    
    prompt = f"""è¯·å°†ä»¥ä¸‹{len(articles)}ç¯‡è‹±æ–‡æŠ€æœ¯æ–‡ç« çš„æ ‡é¢˜å’Œæ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ã€‚

ç¿»è¯‘è¦æ±‚ï¼š
1. ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ï¼ˆå¦‚LLMã€APIã€GPUç­‰å¯ä¿ç•™è‹±æ–‡ï¼‰
2. æ ‡é¢˜è¦ç®€æ´æœ‰åŠ›
3. â—æ‘˜è¦æ§åˆ¶åœ¨200-300å­—ï¼Œè¯¦ç»†ä»‹ç»æ–‡ç« æ ¸å¿ƒå†…å®¹ã€ä¸»è¦è§‚ç‚¹å’ŒæŠ€æœ¯ç»†èŠ‚
4. è¯­è¨€æµç•…è‡ªç„¶ï¼Œç¬¦åˆä¸­æ–‡ç§‘æŠ€åª’ä½“ä¹ æƒ¯

è¾“å…¥JSONï¼š
{json.dumps(articles_input, ensure_ascii=False, indent=2)}

è¾“å‡ºæ ¼å¼ï¼ˆJSONæ•°ç»„ï¼‰ï¼š
[
  {{"id": 0, "title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ‘˜è¦ï¼ˆ200-300å­—ï¼‰"}},
  {{"id": 1, "title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ‘˜è¦ï¼ˆ200-300å­—ï¼‰"}}
]

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    try:
        response = call_zhipu_ai(prompt)
        
        # æ¸…ç†å“åº”
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:]
        if response.startswith('```'):
            response = response[3:]
        if response.endswith('```'):
            response = response[:-3]
        response = response.strip()
        
        translations = json.loads(response)
        
        # åˆå¹¶ç¿»è¯‘ç»“æœ
        translated_articles = []
        for art, trans in zip(articles, translations):
            translated_articles.append({
                **art,
                'title_cn': trans.get('title', art['title']),
                'summary_cn': trans.get('summary', art.get('summary', ''))
            })
        
        logger.info(f"âœ“ ç¿»è¯‘å®Œæˆ")
        return translated_articles
    
    except Exception as e:
        logger.error(f"æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}")
        # é™çº§ï¼šè¿”å›åŸæ–‡
        return [{**art, 'title_cn': art['title'], 'summary_cn': art.get('summary', '')} 
                for art in articles]


def format_depth_stars(depth: float) -> str:
    """å°†è®¨è®ºæ·±åº¦è½¬æ¢ä¸ºæ˜Ÿçº§è¡¨ç¤º"""
    stars = int(depth * 5)
    return 'â˜…' * stars + 'â˜†' * (5 - stars)


def generate_topic_section(rank: int, topic: Dict, recommendation: str, articles: List[Dict]) -> str:
    """
    ç”Ÿæˆå•ä¸ªè¯é¢˜çš„Markdownæ®µè½
    
    Args:
        rank: æ’å
        topic: è¯é¢˜å¯¹è±¡
        recommendation: æ¨èç†ç”±
        articles: ç¿»è¯‘åçš„æ–‡ç« åˆ—è¡¨ï¼ˆæœ€å¤š5ç¯‡ï¼‰
    
    Returns:
        str: Markdownæ®µè½
    """
    depth_stars = format_depth_stars(topic['avg_depth'])
    
    section = f"""### ğŸ”¥ è¯é¢˜{rank}ï¼š{topic['canonical_name']} [çƒ­åŠ›å€¼: {topic['heat_score']}/100]

**åˆ†ç±»**ï¼š{topic['category']} | **æåŠæ¬¡æ•°**ï¼š{topic['total_mentions']}ç¯‡ | **è®¨è®ºæ·±åº¦**ï¼š{depth_stars}

**æ¨èç†ç”±**ï¼š

{recommendation}

**ç›¸å…³æ–‡ç« **ï¼š

"""
    
    # æ·»åŠ æ–‡ç« åˆ—è¡¨
    for i, art in enumerate(articles, 1):
        # åˆ¤æ–­æ˜¯å¦æ˜¯æ·±åº¦æ–‡ç« ï¼ˆdepth > 0.7ï¼‰
        depth_tag = "ğŸ”¬ " if art.get('depth', 0) > 0.7 else ""
        
        # æ ¼å¼åŒ–å‘å¸ƒæ—¥æœŸ
        published = art.get('published', '')
        if published:
            try:
                from dateutil import parser as date_parser
                pub_date = date_parser.parse(published)
                published_str = pub_date.strftime('%Y-%m-%d %H:%M')
            except:
                published_str = published[:16] if len(published) > 16 else published
        else:
            published_str = 'æœªçŸ¥'
        
        # è·å–æ ‡é¢˜ï¼ˆä¼˜å…ˆç”¨ä¸­æ–‡ç¿»è¯‘ï¼‰
        title = art.get('title_cn', art['title'])
        link = art['link']
        
        # è·å–æ‘˜è¦ï¼ˆå¢åŠ å­—æ•°åˆ°300ï¼‰
        summary = art.get('summary_cn', art.get('summary', ''))[:300]
        
        section += f"""{i}. {depth_tag}**[{title}]({link})**  
   ğŸ“° æ¥æºï¼š{art['source']} | ğŸ“… å‘å¸ƒï¼š{published_str}  
   ğŸ“ {summary}

"""
    
    return section


def generate_report(top_topics: List[Dict], time_slot: str, date_str: str, total_articles: int):
    """
    ç”Ÿæˆå®Œæ•´çš„MarkdownæŠ¥å‘Š
    
    Args:
        top_topics: Top Nè¯é¢˜åˆ—è¡¨
        time_slot: æ—¶é—´æ®µï¼ˆæ—©é—´/åˆé—´/æ™šé—´ï¼‰
        date_str: æ—¥æœŸå­—ç¬¦ä¸²
        total_articles: æ€»æ–‡ç« æ•°
    """
    config = load_config()
    logger.info(f"å¼€å§‹ç”Ÿæˆ {time_slot} æ¨èæŠ¥å‘Š...")
    
    # å½“å‰æ—¶é—´
    now = datetime.now().strftime('%H:%M')
    
    # ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨
    report_header = f"""## ğŸ“Š {time_slot}æ¨è ({now}æ›´æ–°)

> åŸºäºå½“æ—¥ç´¯è®¡æ–‡ç« åˆ†æï¼Œæˆªè‡³{now}å…±ç›‘æ§åˆ°**{total_articles}ç¯‡**æ–°æ–‡ç« 

"""
    
    # ç”Ÿæˆæ¯ä¸ªè¯é¢˜çš„æ®µè½
    sections = []
    
    for i, topic in enumerate(top_topics, 1):
        # ç”Ÿæˆæ¨èç†ç”±
        recommendation = generate_recommendation(topic)
        
        # é€‰å‡ºTop 5æ–‡ç« ï¼ˆæŒ‰depthæ’åºï¼‰
        sorted_articles = sorted(
            topic['articles'],
            key=lambda x: x.get('depth', 0),
            reverse=True
        )[:config['output_config']['articles_per_topic']]
        
        # ç¿»è¯‘æ–‡ç« 
        translated_articles = translate_articles_batch(sorted_articles)
        
        # ç”Ÿæˆæ®µè½
        section = generate_topic_section(i, topic, recommendation, translated_articles)
        sections.append(section)
    
    # ç»„åˆå®Œæ•´æŠ¥å‘Š
    full_report = report_header + '\n'.join(sections)
    
    # å†™å…¥åˆ°å½“å¤©çš„æŠ¥å‘Šæ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
    write_daily_report(date_str, time_slot, full_report, total_articles, len(top_topics))
    
    logger.info(f"âœ… {time_slot}æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")


def write_daily_report(date_str: str, time_slot: str, content: str, 
                       total_articles: int, total_topics: int):
    """
    å°†æŠ¥å‘Šå†…å®¹å†™å…¥å½“å¤©çš„Markdownæ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰

    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²
        time_slot: æ—¶é—´æ®µ
        content: æŠ¥å‘Šå†…å®¹
        total_articles: æ€»æ–‡ç« æ•°
        total_topics: æ€»è¯é¢˜æ•°
    """
    config = load_config()
    reports_dir = os.path.join(os.path.dirname(__file__), config['reports_dir'])
    os.makedirs(reports_dir, exist_ok=True)
    
    report_file = os.path.join(reports_dir, f'{date_str}.md')
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(report_file):
        # æ–‡ä»¶å·²å­˜åœ¨ï¼Œéœ€è¦è¿½åŠ 
        with open(report_file, 'r', encoding='utf-8') as f:
            existing_content = f.read()
        
        # æ‰¾åˆ°æœ€åä¸€ä¸ªæ€»è§ˆéƒ¨åˆ†ä¹‹å‰
        if '## ğŸ“ˆ ä»Šæ—¥æ•°æ®æ€»è§ˆ' in existing_content:
            # ç§»é™¤æ—§çš„æ€»è§ˆ
            existing_content = existing_content.split('## ğŸ“ˆ ä»Šæ—¥æ•°æ®æ€»è§ˆ')[0]
        
        # è¿½åŠ æ–°å†…å®¹
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(existing_content)
            f.write(content)
            f.write("\n---\n\n")
    else:
        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# æŠ€æœ¯åšå®¢è¯é¢˜ç›‘æ§ - {date_str}

---

""")
            f.write(content)
            f.write("\n---\n\n")
    
    # æ›´æ–°æ€»è§ˆï¼ˆåœ¨æ–‡ä»¶æœ«å°¾ï¼‰
    update_daily_summary(report_file, date_str, time_slot, total_articles, total_topics)
    
    logger.info(f"æŠ¥å‘Šå·²æ›´æ–°åˆ°: {report_file}")


def update_daily_summary(report_file: str, date_str: str, time_slot: str, 
                        total_articles: int, total_topics: int):
    """æ›´æ–°æ¯æ—¥æ•°æ®æ€»è§ˆ"""
    
    # è¯»å–ç°æœ‰å†…å®¹
    with open(report_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ç§»é™¤æ—§çš„æ€»è§ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if '## ğŸ“ˆ ä»Šæ—¥æ•°æ®æ€»è§ˆ' in content:
        content = content.split('## ğŸ“ˆ ä»Šæ—¥æ•°æ®æ€»è§ˆ')[0]
    
    # æ·»åŠ æ–°çš„æ€»è§ˆ
    config = load_config()
    opml_path = os.path.join(os.path.dirname(__file__), config['opml_file'])
    
    # ç»Ÿè®¡RSSæºæ•°é‡
    from fetch_blogs import parse_opml_file
    rss_sources = parse_opml_file(opml_path)
    
    summary = f"""## ğŸ“ˆ ä»Šæ—¥æ•°æ®æ€»è§ˆ

- **ç›‘æ§RSSæºæ•°é‡**ï¼š{len(rss_sources)}ä¸ª
- **æŠ“å–æ–‡ç« æ€»æ•°**ï¼š{total_articles}ç¯‡
- **è¯†åˆ«è¯é¢˜æ•°**ï¼š{total_topics}ä¸ª
- **ç”ŸæˆæŠ¥å‘Šæ—¶é—´**ï¼š{time_slot} {datetime.now().strftime('%H:%M')}

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}*  
*æ•°æ®æ¥æºï¼š{len(rss_sources)}ä¸ªæŠ€æœ¯åšå®¢RSSæº*  
*ç”± Blog Topic Monitor Skill è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    # å†™å›æ–‡ä»¶
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(content + summary)


if __name__ == '__main__':
    import sys
    from calculate_heat import get_top_topics
    
    # è·å–å‚æ•°
    date_str = sys.argv[1] if len(sys.argv) > 1 else datetime.now().strftime('%Y-%m-%d')
    time_slot = sys.argv[2] if len(sys.argv) > 2 else 'æ—©é—´'
    
    # åŠ è½½çƒ­åŠ›å€¼æ•°æ®
    config = load_config()
    data_dir = os.path.join(os.path.dirname(__file__), config['data_dir'])
    heat_file = os.path.join(data_dir, 'processed', date_str, f'heat_scores_{time_slot}.json')
    
    with open(heat_file, 'r', encoding='utf-8') as f:
        heat_data = json.load(f)
    
    scored_topics = heat_data['topics']
    
    # è·å–Top Nï¼ˆæ ¹æ®é…ç½®ï¼‰
    top_n = config['output_config']['topics_per_report']
    top_topics = scored_topics[:top_n]
    
    print(f"å°†ç”Ÿæˆ {len(top_topics)} ä¸ªè¯é¢˜çš„æ¨èæŠ¥å‘Š")
    
    # è®¡ç®—æ€»æ–‡ç« æ•°
    from fetch_blogs import load_articles
    articles = load_articles(date_str)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(top_topics, time_slot, date_str, len(articles))