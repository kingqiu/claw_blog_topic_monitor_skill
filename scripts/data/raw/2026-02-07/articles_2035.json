{
  "metadata": {
    "fetch_time": "2026-02-07T20:35:01.963618+08:00",
    "time_window_start": "2026-02-06T20:35:01.963618+08:00",
    "time_window_end": "2026-02-07T20:35:01.963618+08:00",
    "hours_range": 24,
    "total_articles": 14
  },
  "articles": [
    {
      "title": "Quoting Tom Dale",
      "link": "https://simonwillison.net/2026/Feb/6/tom-dale/#atom-everything",
      "summary": "<blockquote cite=\"https://twitter.com/tomdale/status/2019828626972131441\"><p>I don't know why this week became the tipping point, but nearly every software engineer I've talked to is experiencing some degree of mental health crisis.</p>\n<p>[...] Many people assuming I meant job loss anxiety but that's just one presentation. I'm seeing near-manic episodes triggered by watching software shift from scarce to abundant. Compulsive behaviors around agent usage. Dissociative awe at the temporal compression of change. It's not fear necessarily just the cognitive overload from living in an inflection point.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/tomdale/status/2019828626972131441\">Tom Dale</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
      "content": "<blockquote cite=\"https://twitter.com/tomdale/status/2019828626972131441\"><p>I don't know why this week became the tipping point, but nearly every software engineer I've talked to is experiencing some degree of mental health crisis.</p>\n<p>[...] Many people assuming I meant job loss anxiety but that's just one presentation. I'm seeing near-manic episodes triggered by watching software shift from scarce to abundant. Compulsive behaviors around agent usage. Dissociative awe at the temporal compression of change. It's not fear necessarily just the cognitive overload from living in an inflection point.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/tomdale/status/2019828626972131441\">Tom Dale</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/careers\">careers</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
      "published": "2026-02-06T23:41:31+00:00",
      "source": "simonwillison.net",
      "source_url": "http://simonwillison.net/",
      "word_count": 1129
    },
    {
      "title": "Running Pydantic's Monty Rust sandboxed Python subset in WebAssembly",
      "link": "https://simonwillison.net/2026/Feb/6/pydantic-monty/#atom-everything",
      "summary": "<p>There's a jargon-filled headline for you! Everyone's <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">building sandboxes</a> for running untrusted code right now, and Pydantic's latest attempt, <a href=\"https://github.com/pydantic/monty\">Monty</a>, provides a custom Python-like language (a subset of Python) in Rust and makes it available as both a Rust library and a Python package. I got it working in WebAssembly, providing a sandbox-in-a-sandbox.</p>\n<p>Here's <a href=\"https://github.com/pydantic/monty\">how they describe Monty</a>:</p>\n<blockquote>\n<p>Monty avoids the cost, latency, complexity and general faff of using full container based sandbox for running LLM generated code.</p>\n<p>Instead, it let's you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>\n<p>What Monty <strong>can</strong> do:</p>\n<ul>\n<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>\n<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>\n<li>Call functions on the host - only functions you give it access to [...]</li>\n</ul>\n</blockquote>\n<p>A quick way to try it out is via <a href=\"https://github.com/astral-sh/uv\">uv</a>:</p>\n<pre><code>uv run --with pydantic-monty python -m asyncio\n</code></pre>\n<p>Then paste this into the Python interactive prompt - the <code>-m asyncio</code> enables top-level await:</p>\n<pre><span>import</span> <span>pydantic_monty</span>\n<span>code</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'print(\"hello \" + str(4 * 5))'</span>)\n<span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(<span>code</span>)</pre>\n<p>Monty supports a <em>very</em> small subset of Python - it doesn't even support class declarations yet!</p>\n<p>But, given its target use-case, that's not actually a problem.</p>\n<p>The neat thing about providing tools like this for LLMs is that they're really good at iterating against error messages. A coding agent can run some Python code, get an error message telling it that classes aren't supported and then try again with a different approach.</p>\n<p>I wanted to try this in a browser, so I fired up <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">a code research task</a> in Claude Code for web and kicked it off with the following:</p>\n<blockquote>\n<p>Clone <a href=\"https://github.com/pydantic/monty\">https://github.com/pydantic/monty</a> to /tmp and figure out how to compile it into a python WebAssembly wheel that can then be loaded in Pyodide. The wheel file itself should be checked into the repo along with build scripts and passing pytest playwright test scripts that load Pyodide from a CDN and the wheel from a “python -m http.server” localhost and demonstrate it working</p>\n</blockquote>\n<p>Then a little later:</p>\n<blockquote>\n<p>I want an additional WASM file that works independently of Pyodide, which is also usable in a web browser - build that too along with playwright tests that show it working. Also build two HTML files - one called demo.html and one called pyodide-demo.html - these should work similar to <a href=\"https://tools.simonwillison.net/micropython\">https://tools.simonwillison.net/micropython</a> (download that code with curl to inspect it) - one should load the WASM build, the other should load Pyodide and have it use the WASM wheel. These will be served by GitHub Pages so they can load the WASM and wheel from a relative path since the .html files will be served from the same folder as the wheel and WASM file</p>\n</blockquote>\n<p>Here's <a href=\"https://gisthost.github.io/?22d88e6367d7e002c4fb383c213c2df2/page-001.html\">the transcript</a>, and the <a href=\"https://github.com/simonw/research/tree/main/monty-wasm-pyodide\">final research report</a> it produced.</p>\n<p>I now have the Monty Rust code compiled to WebAssembly in two different shapes - as a <code>.wasm</code> bundle you can load and call from JavaScript, and as a <code>monty-wasm-pyodide/pydantic_monty-0.0.3-cp313-cp313-emscripten_4_0_9_wasm32.whl</code> wheel file which can be loaded into <a href=\"https://pyodide.org/\">Pyodide</a> and then called from Python in Pyodide in WebAssembly in a browser.</p>\n<p>Here are those two demos, hosted on GitHub Pages:</p>\n<ul>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/demo.html\">Monty WASM demo</a> - a UI over JavaScript that loads the Rust WASM module directly.</li>\n<li>\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/pyodide-demo.html\">Monty Pyodide demo</a> - this one provides an identical interface but here the code is <a href=\"https://github.com/simonw/research/blob/3add1ffec70b530711fa237d91f546da5bcf1f1c/monty-wasm-pyodide/pyodide-demo.html#L257-L280\">loading Pyodide and then installing the Monty WASM wheel</a>.</li>\n</ul>\n<p><img alt=\"Screenshot of a web app titled &quot;Monty via Pyodide&quot; with description &quot;Run Monty (a sandboxed Python interpreter by Pydantic) inside Pyodide (CPython compiled to WebAssembly). This loads the pydantic-monty wheel and uses its full Python API. Code is saved in the URL for sharing.&quot; A green banner reads &quot;Code executed successfully!&quot; Below are example buttons labeled &quot;Basic&quot;, &quot;Inputs&quot;, &quot;Reuse&quot;, &quot;Error Handling&quot;, &quot;Fibonacci&quot;, and &quot;Classes&quot;. A code editor labeled &quot;Python Code (runs inside Monty sandbox via Pyodide):&quot; contains: &quot;import pydantic_monty\\n\\n# Create interpreter with input variables\\nm = pydantic_monty.Monty('x + y', inputs=['x', 'y'])\\n\\n# Run with different inputs\\nresult1 = m.run(inputs={&quot;x&quot;: 10, &quot;y&quot;: 20})\\nprint(f&quot;10 + 20 = {result1}&quot;)\\n\\nresult2 = m.run(inputs={&quot;x&quot;: 100, &quot;y&quot;: 200})&quot; with &quot;Run Code&quot; and &quot;Clear&quot; buttons. The Output section shows &quot;10 + 20 = 30&quot; and &quot;100 + 200 = 300&quot; with a &quot;Copy&quot; button. Footer reads &quot;Executed in 4.0ms&quot;.\" src=\"https://static.simonwillison.net/static/2026/monty-pyodide.jpg\" /></p>\n<p>As a connoisseur of sandboxes - the more options the better! - this new entry from Pydantic ticks a lot of my boxes. It's small, fast, widely available (thanks to Rust and WebAssembly) and provides strict limits on memory usage, CPU time and access to disk and network.</p>\n<p>It was also a great excuse to spin up another demo showing how easy it is these days to turn compiled code like C or Rust into WebAssembly that runs in both a browser and a Pyodide environment.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/rust\">rust</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/pyodide\">pyodide</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/pydantic\">pydantic</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>",
      "content": "<p>There's a jargon-filled headline for you! Everyone's <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">building sandboxes</a> for running untrusted code right now, and Pydantic's latest attempt, <a href=\"https://github.com/pydantic/monty\">Monty</a>, provides a custom Python-like language (a subset of Python) in Rust and makes it available as both a Rust library and a Python package. I got it working in WebAssembly, providing a sandbox-in-a-sandbox.</p>\n<p>Here's <a href=\"https://github.com/pydantic/monty\">how they describe Monty</a>:</p>\n<blockquote>\n<p>Monty avoids the cost, latency, complexity and general faff of using full container based sandbox for running LLM generated code.</p>\n<p>Instead, it let's you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>\n<p>What Monty <strong>can</strong> do:</p>\n<ul>\n<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>\n<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>\n<li>Call functions on the host - only functions you give it access to [...]</li>\n</ul>\n</blockquote>\n<p>A quick way to try it out is via <a href=\"https://github.com/astral-sh/uv\">uv</a>:</p>\n<pre><code>uv run --with pydantic-monty python -m asyncio\n</code></pre>\n<p>Then paste this into the Python interactive prompt - the <code>-m asyncio</code> enables top-level await:</p>\n<pre><span>import</span> <span>pydantic_monty</span>\n<span>code</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'print(\"hello \" + str(4 * 5))'</span>)\n<span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(<span>code</span>)</pre>\n<p>Monty supports a <em>very</em> small subset of Python - it doesn't",
      "published": "2026-02-06T22:31:31+00:00",
      "source": "simonwillison.net",
      "source_url": "http://simonwillison.net/",
      "word_count": 7731
    },
    {
      "title": "An Update on Heroku",
      "link": "https://simonwillison.net/2026/Feb/6/an-update-on-heroku/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.heroku.com/blog/an-update-on-heroku/\">An Update on Heroku</a></strong></p>\nAn ominous headline to see on the official Heroku blog and yes, it's bad news.</p>\n<blockquote>\n<p>Today, Heroku is transitioning to a sustaining engineering model focused on stability, security, reliability, and support. Heroku remains an actively supported, production-ready platform, with an emphasis on maintaining quality and operational excellence rather than introducing new features. We know changes like this can raise questions, and we want to be clear about what this means for customers.</p>\n</blockquote>\n<p>Based on context I'm guessing a \"sustaining engineering model\" (this definitely isn't a widely used industry term) means that they'll keep the lights on and that's it.</p>\n<p>This is a very frustrating piece of corporate communication. \"We want to be clear about what this means for customers\" - then proceeds to <em>not be clear</em> about what this means for customers.</p>\n<p>Why are they doing this? Here's their explanation:</p>\n<blockquote>\n<p>We’re focusing our product and engineering investments on areas where we can deliver the greatest long-term customer value, including helping organizations build and deploy enterprise-grade AI in a secure and trusted way.</p>\n</blockquote>\n<p>My blog is the only project I have left running on Heroku. I guess I'd better migrate it away (probably to Fly) before Salesforce lose interest completely.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/salesforce\">salesforce</a>, <a href=\"https://simonwillison.net/tags/heroku\">heroku</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a></p>",
      "content": "<p><strong><a href=\"https://www.heroku.com/blog/an-update-on-heroku/\">An Update on Heroku</a></strong></p>\nAn ominous headline to see on the official Heroku blog and yes, it's bad news.</p>\n<blockquote>\n<p>Today, Heroku is transitioning to a sustaining engineering model focused on stability, security, reliability, and support. Heroku remains an actively supported, production-ready platform, with an emphasis on maintaining quality and operational excellence rather than introducing new features. We know changes like this can raise questions, and we want to be clear about what this means for customers.</p>\n</blockquote>\n<p>Based on context I'm guessing a \"sustaining engineering model\" (this definitely isn't a widely used industry term) means that they'll keep the lights on and that's it.</p>\n<p>This is a very frustrating piece of corporate communication. \"We want to be clear about what this means for customers\" - then proceeds to <em>not be clear</em> about what this means for customers.</p>\n<p>Why are they doing this? Here's their explanation:</p>\n<blockquote>\n<p>We’re focusing our product and engineering investments on areas where we can deliver the greatest long-term customer value, including helping organizations build and deploy enterprise-grade AI in a secure and trusted way.</p>\n</blockquote>\n<p>My blog is the only project I have left running on Heroku. I guess I'd better migrate it away (probably to Fly) before Salesforce lose interest completely.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/salesforce\">salesforce</a>, <a href=\"https://simonwillison.net/tags/heroku\">heroku</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a></p>",
      "published": "2026-02-06T18:44:21+00:00",
      "source": "simonwillison.net",
      "source_url": "http://simonwillison.net/",
      "word_count": 1676
    },
    {
      "title": "The first good Raspberry Pi Laptop",
      "link": "https://www.jeffgeerling.com/blog/2026/the-first-good-raspberry-pi-laptop/",
      "summary": "<p>Ever since the <a href=\"https://www.raspberrypi.com/products/compute-module-5/\">Raspberry Pi Compute Module 5</a> was introduced, I wondered why nobody built a decent laptop chassis around it.</p>\n<figure class=\"insert-image\"><img alt=\"Argon ONE UP laptop with Raspberry Pi mug on desk\" height=\"auto\" src=\"https://www.jeffgeerling.com/blog/2026/the-first-good-raspberry-pi-laptop/argon-one-up-laptop-hero-raspberry-pi-mug.jpg\" width=\"700\" />\n</figure>\n\n<p>You could swap out a low spec CM5 for a higher spec, and get an instant computer upgrade. Or, assuming a CM6 comes out someday in the same form factor, the laptop chassis could get an entirely new life with that upgrade.</p>",
      "content": "<p>Ever since the <a href=\"https://www.raspberrypi.com/products/compute-module-5/\">Raspberry Pi Compute Module 5</a> was introduced, I wondered why nobody built a decent laptop chassis around it.</p>\n<figure class=\"insert-image\"><img alt=\"Argon ONE UP laptop with Raspberry Pi mug on desk\" height=\"auto\" src=\"https://www.jeffgeerling.com/blog/2026/the-first-good-raspberry-pi-laptop/argon-one-up-laptop-hero-raspberry-pi-mug.jpg\" width=\"700\" />\n</figure>\n\n<p>You could swap out a low spec CM5 for a higher spec, and get an instant computer upgrade. Or, assuming a CM6 comes out someday in the same form factor, the laptop chassis could get an entirely new life with that upgrade.</p>",
      "published": "2026-02-06T09:00:00-06:00",
      "source": "jeffgeerling.com",
      "source_url": "https://www.jeffgeerling.com/",
      "word_count": 683
    },
    {
      "title": "Pluralistic: End of the line for video essays (07 Feb 2026)",
      "link": "https://pluralistic.net/2026/02/07/aimsters-revenge/",
      "summary": "Today's links End of the line for video essays: America's worst copyright law keeps getting even worse. Hey look at this: Delights to delectate. Object permanence: Payphone phaseout; Nvidia sock-puppets; Love picking; Fake locksmiths. Upcoming appearances: Where to find me. Recent appearances: Where I've been. Latest books: You keep readin' em, I'll keep writin' 'em. Upcoming books: Like I said, I'll keep writin' 'em. Colophon: All the rest. End of the line for video essays (permalink) What if there was a way for a business to transform any conduct it disliked into a felony, harnessing the power of the state to threaten anyone who acted in a way that displeased the company with a long prison sentence and six-figure fines? Surprise! That actually exists! It's called Section 1201 of the Digital Millennium Copyright Act, the \"anticircumvention\" clause, which establishes five-year sentences and $500k fines for anyone who bypasses an \"effective access control\" for a copyrighted work. Let's unpack that: every digital product has a \"copyrighted work\" at its core, because software is copyrighted. Digital systems are intrinsically very flexible: just overwrite, augment, or delete part of the software that powers the device or product, and you change how the product works. You can alter your browser to block ads; or alter your Android phone to run a privacy-respecting OS like Graphene; or alter your printer to accept generic ink, rather than checking each cartridge to confirm that it's the original manufacturer's product. However, if the device is designed to prevent this &#8211; if it has an \"access control\" that restricts your ability to change the software &#8211; then DMCA 1201 makes those modifications into crimes. The act of providing someone with a tool to change how their own property works (\"trafficking in circumvention devices\") is a felony. But there's a tiny saving grace here: for DMCA 1201 to kick in, the \"access control\" must be \"effective.\" What's \"effective?\" There's the rub: no one knows. The penalties for getting crosswise with DMCA 1201 are so grotendous that very few people have tried to litigate any of its contours. Whenever the issue comes up, defendants settle, or fold, or disappear. Despite the fact that DMCA 1201 has been with us for more than a quarter of a century, and despite the fact that the activities it restricts are so far-reaching, there's precious little case law clarifying Congress's vague statutory language. When it comes to \"effectiveness\" in access controls, the jurisprudence is especially thin. As far as I know, there's just one case that addressed the issue, and boy was it a weird one. Back in 2000, a \"colorful\" guy named Johnny Deep founded a Napster-alike service that piggybacked on the AOL Instant Messenger network. He called his service \"Aimster.\" When AOL threatened him with a trademark suit, he claimed that Aimster was his daughter Amiee's AOL handle, and that the service was named for her. Then he changed the service's name to Madster, claiming that it was also named after his daughter. At the time, a lot of people assumed he was BSing, but I just found his obituary and it turns out his daughter's name was, indeed, \"Amiee (Madeline) Deep\": https://www.timesunion.com/news/article/Madster-creator-Cohoes-native-who-fought-record-11033636.php Aimster was one of the many services that the record industry tried to shut down, both by filing suit against the company and by flooding it with takedown notices demanding that individual tracks be removed. Deep responded by \"encoding\" all of the track names on his network in pig-Latin. Then he claimed that by \"decoding\" the files (by moving the last letter of the track name to the first position), the record industry was \"bypassing an effective access control for a copyrighted work\" and thus violating DMCA 1201: https://abcnews.go.com/Entertainment/story?id=108454&#38;amp;page=1 The court didn't buy this. The judge ruled that pig Latin isn't an \"effective access control.\" Since then, we've known that at least some access controls aren't \"effective\" but we haven't had any clarity on where \"effectiveness\" starts. After all, there's a certain circularity to the whole idea of \"effective\" access controls: if a rival engineer can figure out how to get around an access control, can we really call it \"effective?\" Surely, the fact that someone figured out how to circumvent your access control is proof that it's not effective (at least when it comes to that person). All this may strike you as weird inside baseball, and that's not entirely wrong, but there's one unresolved \"effectiveness\" question that has some very high stakes indeed: is Youtube's javascript-based obfuscation an \"effective access control?\" Youtube, of course, is the internet's monopoly video platform, with a commanding majority of video streams. It was acquired by Google in 2006 for $1.65b. At the time, the service was hemorrhaging money and mired in brutal litigation, but it had one virtue that made it worth nine figures: people liked it. Specifically, people liked it in a way they didn't like Google Video, which was one of the many, many, many failed internally developed Google products that tanked, and was replaced by a product developed by a company that Google bought, because Google sucks at developing products. They're not Willy Wonka's idea factory &#8211; they're Rich Uncle Pennybags, buying up other kids' toys: https://www.theatlantic.com/ideas/archive/2023/02/google-ai-chatbots-microsoft-bing-chatgpt/673052/ Google operationalized Youtube and built it up to the world's most structurally important video platform. Along the way, Google added some javascript that was intended to block people from \"downloading\" its videos. I put \"downloading\" in scare-quotes because \"streaming\" is a consensus hallucination: there is no way for your computer to display a video that resides on a distant server without downloading it &#8211; the internet is not made up of a cunning series of paper-towel rolls and mirrors that convey photons to your screen without sending you the bits that make up the file. \"Streaming\" is just \"downloading\" with the \"save file\" button removed. In this case, the \"save file\" button is removed by some javascript on every Youtube page. This isn't hard to bypass: there are dozens of \"stream-ripping\" sites that let you save any video that's accessible on Youtube. I use these all the time &#8211; indeed, I used one last week to gank the video of my speech in Ottawa so I could upload it to my own Youtube channel: https://www.youtube.com/watch?v=iZxbaCNIwg8 (As well as the Internet Archive, natch): https://archive.org/details/disenshittification-nation Now, all of this violates Youtube's terms of service, which means that someone who downloads a stream for an otherwise lawful purpose (like I did) is still hypothetically at risk of being punished by Google. We're relying on Google to be reasonable about all this, which, admittedly, isn't the best bet, historically. But at least the field of people who can attack us is limited to this one company. That's good, because there's zillions of people who rely on stream-rippers, and many of them are Youtube's most popular creators. Youtube singlehandedly revived the form of the \"video essay,\" popularizing it in many guises, from \"reaction videos\" to full-fledged, in-depth documentaries that make extensive use of clips to illuminate, dispute, and expand on the messages of other Youtube videos. These kinds of videos are allowed under US copyright law. American copyright law has a broad set of limitation and exceptions, which include \"fair use,\" an expansive set of affirmative rights to access and use copyrighted works, even against the wishes of the copyright's proprietor. As the Supreme Court stated in Eldred, the only way copyright (a government-backed restriction on who can say certain words) can be reconciled with the First Amendment (a ban on government restrictions on speech) is through fair use, the \"escape valve\" for free expression embedded in copyright: https://en.wikipedia.org/wiki/Eldred_v._Ashcroft Which is to say that including clips from a video you're criticizing in your own video is canonical fair use. What else is fair use? Well, it's \"fact intensive,\" which is a lawyer's way of saying, \"it depends.\" One thing that is 100% true, though, is that fair use is not limited to the \"four factors\" enumerated in the statute and anyone who claims otherwise has no idea what they're talking about and can be safely ignored: https://pluralistic.net/2024/06/27/nuke-first/#ask-questions-never Now, fair use or not, there are plenty of people who get angry about their videos being clipped for critical treatment in other videos, because lots of people hate being criticized. This is precisely why fair use exists: if you had to secure someone's permission before you were allowed to criticize them, critical speech would be limited to takedowns of stoics and masochists. This means that the subjects of video essays can't rely on copyright to silence their critics. They also can't use the fact that those critics violated Youtube's terms of service by clipping their videos, because only Youtube has standing to ask a court to uphold its terms of service, and Youtube has (wisely) steered clear of embroiling itself in fights between critics and the people they criticize. But that hasn't stopped the subjects of criticism from seeking legal avenues to silence their critics. In a case called Cordova v. Huneault, the proprietor of \"Denver Metro Audits\" is suing the proprietor of \"Frauditor Troll Channel\" for clipping the former's videos for \"reaction videos.\" One of the plaintiff's claims here is that the defendant violated Section 1201 of the DMCA by saving videos from Youtube. They argue that Youtube's javascript obfuscator (a \"rolling cipher\") is an \"effective access control\" under the statute. Magistrate Judge Virginia K DeMarchi (Northern District of California) agreed with the plaintiff: https://torrentfreak.com/images/Cordova-v.-Huneault-25-cv-04685-VKD-Order-on-Motion-to-Dismiss.pdf As Torrentfreak reports, this ruling \"gives creators who want to sue rivals an option to sue for more than just simple copyright infringement\": https://torrentfreak.com/ripping-clips-for-youtube-reaction-videos-can-violate-the-dmca-court-rules/ Remember, DMCA 1201 applies whether or not you infringe someone's copyright. It is a blanket prohibition on the circumvention of any \"effective access control\" for any copyrighted work, even when no one's rights are being violated. It's a way to transform otherwise lawful conduct into a felony. It's what Jay Freeman calls \"Felony contempt of business model.\" If the higher court upholds this magistrate judge's ruling, then all clipping becomes a crime, and the subjects of criticism will have a ready tool to silence any critic. This obliterates fair use, wipes it off the statute-book. It welds shut copyright's escape valve for free expression. Now, it's true that the US Copyright Office holds hearings every three years where it grants exemptions to DMCA 1201, and it has indeed granted an exemption for ripping video for critical and educational purposes. But this process is deceptive! The exemptions that the Copyright Office grants are \"use exemptions\" &#8211; they allow you to \"make the use.\" However, they are not \"tools exemptions\" &#8211; they do not give you permission to acquire or share the tool needed to make the use: https://pluralistic.net/2024/10/28/mcbroken/#my-milkshake-brings-all-the-lawyers-to-the-yard Which means that you are allowed to rip a stream, but you're not allowed to use a stream-ripping service. If Youtube's rolling cipher is an \"effective access control\" then all of those stream-ripping services are wildly illegal, felonies carrying a five-year sentence and a $500k fine for a first offense under DMCA 1201. Under the US Copyright Office's exemption process, if you want to make a reaction video, then you, personally must create your own stream-ripper. You are not allowed to discuss how to do this with anyone else, and you can't share your stream-ripper with anyone else, and if you do, you've committed a felony. So this is a catastrophic ruling. If it stands, it will make the production of video essays, reaction videos, and other critical videos into a legal minefield, by giving everyone whose video is clipped and criticized a means to threaten their critics with long prison sentences, fair use be damned. The only people who will safely be able to make this kind of critical video are skilled programmers who can personally defeat Youtube's \"rolling cipher.\" And unlike claims about stream-ripping violating Youtube's terms of service &#8211; which can only be brought by Youtube &#8211; DMCA 1201 claims can be brought by anyone whose videos get clipped and criticized. Is Youtube's rolling cipher an \"effective access control?\" Well, I don't know how to bypass it, but there are dozens of services that have independently figured out how to get around it. That seems like good evidence that the access control is not \"effective.\" When the DMCA was enacted in 1998, this is exactly the kind of thing experts warned would happen: https://pluralistic.net/2025/05/13/ctrl-ctrl-ctrl/#free-dmitry And here we are, more than a quarter-century later, living in the prison of lawmakers' reckless disregard for evidence and expertise, a world where criticism can be converted into a felony. It's long past time we get rid of this stupid, stupid law: https://pluralistic.net/2026/01/01/39c3/#the-new-coalition (Image: Electronic Frontier Foundation, CC BY 4.0) Hey look at this (permalink) 10 Reasons This Is the Worst Crypto Winter Ever https://archive.is/U5ede#selection-1246.0-1246.1 The Finance Industry Is a Grift. Let’s Start Treating It That Way. https://www.nytimes.com/2026/02/06/opinion/capitalism-industry-financialization.html?unlocked_article_code=1.KFA.Vslp.8Xqe7KWGEwRu&#38;amp;smid=nytcore-ios-share Ron Wyden Only Talks Like This When The Spies Do Something Real Bad https://www.forever-wars.com/ron-wyden-only-talks-like-this-when-the-spies-do-something-real-bad/ Hollywood Is Losing Audiences to AI Fatigue https://www.wired.com/story/hollywood-is-losing-audiences-to-ai-fatigue/ Waymo Exec Admits Remote Operators in Philippines Help Guide US Robotaxis https://eletric-vehicles.com/waymo/waymo-exec-admits-remote-operators-in-philippines-help-guide-us-robotaxis/ Object permanence (permalink) #25yrsago Bellsouth phases out pay-phones https://web.archive.org/web/20010211165636/http://dailynews.yahoo.com/h/ap/20010202/bs/bellsouth_pay_phones_1.html #20yrsago Man who shattered museum vases asked not to come back http://www.chinadaily.com.cn/english/doc/2006-02/07/content_517885.htm #20yrsago Dozens of Web 2.0 companies’ logos https://flickr.com/photos/torrez/95124293/ #20yrsago Did Nvidia hire an army of message-board sock-puppets? https://web.archive.org/web/20060208045150/https://www.consumerist.com/consumer/evil/did-nvidia-hire-online-actors-to-promote-their-products-152874.php #15yrsago Sarah Palin Circle-R wants a trademark on her name https://www.loweringthebar.net/2011/02/sarah-palin-tm-having-trouble-with-registration.html #10yrsago Love Picking: Locksport meets love locks https://toool.us/love-locks/ #10yrsago Superb investigative report on the fake locksmith scam https://www.nytimes.com/2016/01/31/business/fake-online-locksmiths-may-be-out-to-pick-your-pocket-too.html?_r=1 #5yrsago Klobuchar wants to bust her some fuckin' trusts https://pluralistic.net/2021/02/06/calera/#fuck-bork Upcoming appearances (permalink) Salt Lake City: Enshittification at the Utah Museum of Fine Arts (Tanner Humanities Center), Feb 18 https://tanner.utah.edu/center-events/cory-doctorow/ Montreal (remote): Fedimtl, Feb 24 https://fedimtl.ca/ Victoria: 28th Annual Victoria International Privacy &#38; Security Summit, Mar 3-5 https://www.rebootcommunications.com/event/vipss2026/ Berkeley: Bioneers keynote, Mar 27 https://conference.bioneers.org/ Berlin: Re:publica, May 18-20 https://re-publica.com/de/news/rp26-sprecher-cory-doctorow Berlin: Enshittification at Otherland Books, May 19 https://www.otherland-berlin.de/de/event-details/cory-doctorow.html Hay-on-Wye: HowTheLightGetsIn, May 22-25 https://howthelightgetsin.org/festivals/hay/big-ideas-2 Recent appearances (permalink) Everything Wrong With the Internet and How to Fix It, with Tim Wu (Ezra Klein) https://www.nytimes.com/2026/02/06/opinion/ezra-klein-podcast-doctorow-wu.html How the Internet Got Worse (Masters in Business) https://www.youtube.com/watch?v=auXlkuVhxMo Enshittification (Jon Favreau/Offline): https://crooked.com/podcast/the-enshittification-of-the-internet-with-cory-doctorow/ Why Big Tech is a Trap for Independent Creators (Stripper News) https://www.youtube.com/watch?v=nmYDyz8AMZ0 Enshittification (Creative Nonfiction podcast) https://brendanomeara.com/episode-507-enshittification-author-cory-doctorow-believes-in-a-new-good-internet/ Latest books (permalink) \"Canny Valley\": A limited edition collection of the collages I create for Pluralistic, self-published, September 2025 \"Enshittification: Why Everything Suddenly Got Worse and What to Do About It,\" Farrar, Straus, Giroux, October 7 2025 https://us.macmillan.com/books/9780374619329/enshittification/ \"Picks and Shovels\": a sequel to \"Red Team Blues,\" about the heroic era of the PC, Tor Books (US), Head of Zeus (UK), February 2025 (https://us.macmillan.com/books/9781250865908/picksandshovels). \"The Bezzle\": a sequel to \"Red Team Blues,\" about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (thebezzle.org). \"The Lost Cause:\" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (http://lost-cause.org). \"The Internet Con\": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (http://seizethemeansofcomputation.org). Signed copies at Book Soup (https://www.booksoup.com/book/9781804291245). \"Red Team Blues\": \"A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before.\" Tor Books http://redteamblues.com. \"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin\", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 https://chokepointcapitalism.com Upcoming books (permalink) \"Unauthorized Bread\": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026 \"Enshittification, Why Everything Suddenly Got Worse and What to Do About It\" (the graphic novel), Firstsecond, 2026 \"The Memex Method,\" Farrar, Straus, Giroux, 2026 \"The Reverse-Centaur's Guide to AI,\" a short book about being a better AI critic, Farrar, Straus and Giroux, June 2026 Colophon (permalink) Today's top sources: Currently writing: \"The Post-American Internet,\" a sequel to \"Enshittification,\" about the better world the rest of us get to have now that Trump has torched America (1010 words today, 24701 total) \"The Reverse Centaur's Guide to AI,\" a short book for Farrar, Straus and Giroux about being an effective AI critic. LEGAL REVIEW AND COPYEDIT COMPLETE. \"The Post-American Internet,\" a short book about internet policy in the age of Trumpism. PLANNING. A Little Brother short story about DIY insulin PLANNING This work &#8211; excluding any serialized fiction &#8211; is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net. https://creativecommons.org/licenses/by/4.0/ Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution. How to get Pluralistic: Blog (no ads, tracking, or data-collection): Pluralistic.net Newsletter (no ads, tracking, or data-collection): https://pluralistic.net/plura-list Mastodon (no ads, tracking, or data-collection): https://mamot.fr/@pluralistic Medium (no ads, paywalled): https://doctorow.medium.com/ Twitter (mass-scale, unrestricted, third-party surveillance and advertising): https://twitter.com/doctorow Tumblr (mass-scale, unrestricted, third-party surveillance and advertising): https://mostlysignssomeportents.tumblr.com/tagged/pluralistic \"When life gives you SARS, you make sarsaparilla\" -Joey \"Accordion Guy\" DeVilla READ CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies (\"BOGUS AGREEMENTS\") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer. ISSN: 3066-764X",
      "content": "<p><!--\nTags:\n1201, dmca 1201, youtube, streamripping, copyright, copyfight, felony contempt of business model, law, anticircumvention, adversarial interoperability, comcom, interop, interoperability\n\nSummary:\nEnd of the line for video essays; Hey look at this; Upcoming appearances; Recent appearances; Latest books; Upcoming books\n\nURL:\nhttps://pluralistic.net/2026/02/07/aimsters-revenge/\n\nTitle:\nPluralistic: End of the line for video essays (07 Feb 2026) aimsters-revenge\n\nBullet:\n&#x1f99a;\n\nSeparator:\n->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->\n\nTop Sources:\nNone\n\n--><br />\n<a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/\"><img class=\"xmasthead_link\" src=\"https://i0.wp.com/craphound.com/images/07Feb2026.jpg?w=840&#038;ssl=1\" /></a></p>\n<h1 class=\"toch1\">Today's links</h1>\n<ul class=\"toc\">\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#effective-means-of-access-control\">End of the line for video essays</a>: America's worst copyright law keeps getting even worse.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#linkdump\">Hey look at this</a>: Delights to delectate.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#retro\">Object permanence</a>: Payphone phaseout; Nvidia sock-puppets; Love picking; Fake locksmiths.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#upcoming\">Upcoming appearances</a>: Where to find me.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#recent\">Recent appearances</a>: Where I've been.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#latest\">Latest books</a>: You keep readin' em, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/07/aimsters-revenge/#upcoming-books\">Upcoming books</a>: Like I said, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralisti",
      "published": "2026-02-07T08:12:57+00:00",
      "source": "pluralistic.net",
      "source_url": "https://pluralistic.net",
      "word_count": 30937
    },
    {
      "title": "How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?, part 2",
      "link": "https://devblogs.microsoft.com/oldnewthing/20260206-00/?p=112045",
      "summary": "<p>Preventing the resize cursor from appearing.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260206-00/?p=112045\">How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?, part 2</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
      "content": "<p>Last time, we had figured out how to <a href=\"https://devblogs.microsoft.com/oldnewthing/20260205-00/?p=112042\" title=\"How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?\"> prevent the version 5 ListView Win32 common control from resizing columns</a>, but we noticed that the cursor still changes to a resize cursor that doesn&#8217;t work. How can we avoid misleading the user?</p>\n<p>I had a few ideas but decided that the easiest way would be to subclass the header control and override its <code>WM_SET­CURSOR</code> message with one that just sets the arrow cursor.</p>\n<pre>LRESULT CALLBACK AlwaysArrowSubclassProc(\n    HWND hWnd, UINT uMsg, WPARAM wParam, LPARAM lParam,\n    [[maybe_unused]] UINT_PTR uIdSubclass,\n    [[maybe_unused]] DWORD_PTR dwRefData)\n{\n    switch (uMsg) {\n    case WM_SETCURSOR:\n        SetCursor(LoadCursor(nullptr, IDC_ARROW));\n        return 1;\n    }\n    return DefSubclassProc(hWnd, uMsg, wParam, lParam);\n}\n\n    case WM_CREATE: // or WM_INITDIALOG if this is a dialog procedure\n        ⟦ ... ⟧\n\n        SetWindowSubclass(ListView_GetHeader(hwndLV),\n                          AlwaysArrowSubclassProc, 1, 0);\n\n        ⟦ ... ⟧\n        return 0;\n\n    case WM_DESTROY:\n        RemoveWindowSubclass(ListView_GetHeader(hwndLV),\n                             AlwaysArrowSubclassProc, 1);\n\n        ⟦ ... ⟧\n        return 0;\n</pre>\n<p>Alternatively, we could have the subclass procedure be self-destroying.</p>\n<pre>LRESULT CALLBACK AlwaysArrowSubclassProc(\n    HWND hWnd, UINT uMsg, WPARAM wParam, LPARAM lParam,\n    UINT_PTR uIdSubclass, [[maybe_unused]] DWORD_PTR dwRefData)\n\n{\n    switch (uMsg) {\n    <span style=\"border-bottom: none;\">case WM_NCDESTROY:                                                   </span>\n    <span style=\"border-style: none solid;\">    RemoveWindowSubclass(hWnd, AlwaysArrowSubclassProc, uIdSubclass);</span>\n    <span style=\"border-top: none;\">    break;                                 ",
      "published": "2026-02-06T15:00:00+00:00",
      "source": "devblogs.microsoft.com/oldnewthing",
      "source_url": "https://devblogs.microsoft.com/oldnewthing",
      "word_count": 5226
    },
    {
      "title": "Eigenvalue homework problems are backward",
      "link": "https://www.johndcook.com/blog/2026/02/06/eigenvalue-roots/",
      "summary": "<p>Classroom When you take a linear algebra course and get to the chapter on eigenvalues, your homework problems will include a small matrix A and you will be asked to find the eigenvalues. You do this by computing the determinant det(A − λI) = P(λ) and getting P(λ), a polynomial in λ. The roots of [&#8230;]</p>\nThe post <a href=\"https://www.johndcook.com/blog/2026/02/06/eigenvalue-roots/\">Eigenvalue homework problems are backward</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
      "content": "<h2>Classroom</h2>\n<p>When you take a linear algebra course and get to the chapter on eigenvalues, your homework problems will include a small matrix <em>A</em> and you will be asked to find the eigenvalues. You do this by computing the determinant</p>\n<p style=\"padding-left: 40px;\">det(<em>A</em> − λ<em>I</em>) = <em>P</em>(λ)</p>\n<p>and getting <em>P</em>(λ), a polynomial in λ. The roots of <em>P</em> are the eigenvalues of <em>A</em>.</p>\n<p>Either <em>A</em> will be a 2 × 2 matrix, in which case you can find the roots using the quadratic formula, or the matrix will have been carefully selected so that <em>P</em>(λ) will be easy to factor. Otherwise, finding the roots of a polynomial is hard.</p>\n<h2>Real world</h2>\n<p>Numerical algorithms to find eigenvalues have gotten really good. In practice, you don&#8217;t compute determinants or find roots of polynomials. Instead you do something like the <em>QR</em> algorithm.</p>\n<p>Finding all the roots of a polynomial is a challenging problem, and so what you might do in practice is find the roots by constructing a matrix, called the <strong>companion matrix</strong>, whose eigenvalues correspond to the roots you&#8217;re after.</p>\n<h2>Summary</h2>\n<p>As a classroom exercise, you calculate roots of polynomials to find eigenvalues.</p>\n<p>In the real world, you might use an eigenvalue solver to find the roots of polynomials.</p>\n<p>I wrote a <a href=\"https://www.johndcook.com/blog/2020/08/03/conceptual-vs-numerical/\">similar post</a> a few years ago. It explains that textbooks definite hyperbolic functions using <em>e</em><sup><em>x</em></sup>, but you might want to compute <em>e</em><sup><em>x</em></sup> using hyperbolic functions.</p>The post <a href=\"https://www.johndcook.com/blog/2026/02/06/eigenvalue-roots/\">Eigenvalue homework problems are backward</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
      "published": "2026-02-06T23:31:11+00:00",
      "source": "johndcook.com",
      "source_url": "https://www.johndcook.com/blog",
      "word_count": 1913
    },
    {
      "title": "Writing an LLM from scratch, part 32d -- Interventions: adding attention bias",
      "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32d-interventions-adding-attention-bias",
      "summary": "<p>I'm still seeing what I can do to improve the test loss for a from-scratch GPT-2 small base model, trained on code based on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".\nThis is the third intervention I'm trying: adding bias to the attention weight matrices.</p>\n\n<p>In the code from the book, we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">MultiHeadAttention</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span>\n        <span class=\"n\">context_length</span><span class=\"p\">,</span>\n        <span class=\"n\">dropout</span><span class=\"p\">,</span>\n        <span class=\"n\">num_heads</span><span class=\"p\">,</span>\n        <span class=\"n\">qkv_bias</span><span class=\"o\">=</span><span class=\"kc\">False</span>\n    <span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_query</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_key</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_value</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n\n        <span class=\"o\">...</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n        <span class=\"n\">keys</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_key</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">queries</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_query</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">values</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_value</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>So: we initialise the weights <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math> as linear layers rather than\nsimple matrices of weights, and have a parameter <code>qkv_bias</code> to say whether or not we should\nadd bias to those.  In all of our trains so far we've set that to <code>False</code>.</p>\n\n<p>Why do we have this parameter, and where did it come from?</p>\n<h3 id=\"the-background\">The background</h3>\n\n<p>In Raschka's book, the use of the <code>nn.Linear</code> for these weights is introduced in section 3.4.2\nwith the wording:</p>\n\n<blockquote>\n  <p>We can improve the <code>SelfAttention_v1</code> implementation further by utilizing PyTorch's\n  <code>nn.Linear</code> layers, which effectively perform matrix multiplication when the\n  bias units are disabled.  Additionally, a significant advantage of using <code>nn.Linear</code>\n  instead of manually implementing <code>nn.Parameter(torch.rand(...))</code> is that <code>nn.Linear</code>\n  has an optimized weight initialization scheme, contributing to more stable and\n  effective model training.</p>\n</blockquote>\n\n<p>So, it's presented essentially as a way of getting better weights for our untrained\nmodel, which makes good sense in and of itself -- but, if that's the only reason,\nwhy don't we just hard-wire it to have <code>bias=False</code>?  That would be the sensible thing\nto do if the initialisation were the only reason, but clearly there's more to it\nthan that.</p>\n\n<p>Section 4.1 has a bit more information:</p>\n\n<blockquote>\n  <p><code>qkv_bias</code> determines whether to include a bias vector in the <code>Linear</code> layers\n  of the multi-head attention ... We will initially disable this, following the norms\n  of modern LLMs, but we will revisit it in chapter 6 when we load pretrained\n  GPT-2 weights from OpenAI into our model.</p>\n</blockquote>\n\n<p>That looks like a typo, as the real explanation is in chapter 5, section 5\n(page 164 in my copy), where we do indeed load the OpenAI weights:</p>\n\n<blockquote>\n  <p>OpenAI used bias vectors in the multi-head attention module's linear layers to\n  implement the query, key and value matrix computations.  Bias vectors are not\n  commonly used in LLMs anymore as they don't improve the modeling performance\n  and are thus unnecessary.</p>\n</blockquote>\n\n<p>So, that all makes sense so far.  QKV bias was part of the original GPT-2 models,\nperhaps just because it was standard at the time, inherited from something else,\nor perhaps for some other reason -- I can't find any reference to it in\n<a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">the actual paper</a>.\nBut people have found it doesn't help, so no-one uses it these days.</p>\n\n<p>But... is there some way in which an LLM of this\nspecific size, or in some other way similar to the GPT-2 small model that we're\ntraining, might in some way benefit from having bias?</p>\n\n<p>That's what this experiment is for :-)</p>\n\n<h3 id=\"parameters\">Parameters</h3>\n\n<p>One thing that occurred to me while setting this up is that we have been training\non a Chinchilla-optimal number of tokens, 20x the number of parameters.  Without\nQKV bias, we have 163,009,536 parameters, so we've been training on 3,260,190,720 tokens,\nrounded up to the nearest batch size, which is 3,260,252,160 in our current setup for\nthese experiments (per-GPU micro-batches of 12, with 8 GPUs, so a total batch size of 96).</p>\n\n<p>These extra bias terms will be parameters, though!  We're essentially making our\nmodel larger by adding them, which changes the Chinchilla calculation.  How much?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;vocab_size&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50257</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1024</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;emb_dim&quot;</span><span class=\"p\">:</span> <span class=\"mi\">768</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;n_heads&quot;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;n_layers&quot;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;drop_rate&quot;</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;qkv_bias&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"p\">}</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">gpt</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">GPTModel</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPTModel</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">())</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"mi\">163037184</span>\n</code></pre>\n</div>\n\n<p>OK, that's essentially nothing -- 27,648 extra total paramaters on top of 163 million.\nI make it less than two hundredths of a percentage\npoint larger!  The correct number of tokens goes up to 3,260,743,680, so if we wanted\nto be very pedantic, we're under-training.  But I feel like training on a larger dataset\nis worse in terms of comparability between the baseline and our \"intervened-on\" model\nwith QKV bias.</p>\n\n<p>So: we'll train a model with QKV bias on 3,260,252,160 tokens, accepting that it's a tiny bit less than\nChinchilla-optimal.  Let's see how it goes!</p>\n\n<h3 id=\"the-run\">The run</h3>\n\n<p>Here's the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/main/runs/8xa100m40-qkv-bias/model.json\"><code>model.json</code> config file</a> for this train.\nRunning it gives this training chart:</p>\n\n<p><img alt=\"Training run with QKV bias\" src=\"/post-assets/llm-from-scratch-32d-interventions-adding-attention-bias/training-run-chart.png\" title=\"Training run with QKV bias\" /></p>\n\n<p>Pretty standard, though the loss spikes look less prominent than they have been in\nthe other trains.  Might QKV bias actually help with model stability in some way...?</p>\n\n<p>The train finished with these stats:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,329.557 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 264,426 tokens/second</span>\n<span class=\"go\">Final train loss: 3.719</span>\n</code></pre>\n</div>\n\n<p>Timing-wise, pretty much indistinguishable from the baseline train's 12,243.523 seconds.  The final train\nloss looks a tad better, but we can't rely on that -- the test set loss is the important\none.</p>\n\n<p>So it was time to download it, <a href=\"https://huggingface.co/gpjt/8xa100m40-qkv-bias\">upload it to Hugging Face Hub</a>, and then on\nto the evals.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>Firstly, our normal \"how should you continue <code>Every effort moves you</code>\":</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/model.json<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you toward success. The right questions are asked to become your business coach and help shape the future of their</span>\n</code></pre>\n</div>\n\n<p>Not bad at all, borderline coherent!  Next, the loss on the test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/model.json<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 1701.54it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:52&lt;00:00, 10.95it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.669</span>\n</code></pre>\n</div>\n\n<p>Well, crap!  Now that's a surprise.  Let's look at that in the context of the other interventions to see\nhow surprising that is, given Raschka's comments (which were undoubtedly backed\nup by serious research):</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test set loss</th>\n  <th>Improvement vs baseline</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>8xa100m40-baseline</td>\n  <td>3.692</td>\n  <td>-</td>\n</tr>\n<tr>\n  <td>8xa100m40-gradient-clipping</td>\n  <td>3.678</td>\n  <td>0.014</td>\n</tr>\n<tr>\n  <td>8xa100m40-qkv-bias</td>\n  <td>3.669</td>\n  <td>0.023</td>\n</tr>\n<tr>\n  <td>8xa100m40-remove-dropout</td>\n  <td>3.641</td>\n  <td>0.051</td>\n</tr>\n</tbody>\n</table>\n\n<p>So, adding QKV bias actually improved our test set loss by more than gradient clipping\ndid!</p>\n\n<p>The loss spikes in the training chart look smaller than in the other trains <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>, so, speculating\nwildly, perhaps with a model of this size, the bias stabilises things somehow?  Or perhaps\nwhat we're seeing is the model become that tiny bit smarter because it has some extra parameters\n-- albeit less than 0.02 <em>percent</em> more?</p>\n\n<p>I'm not going to spend time investigating things now, but this is a really interesting result.\nOne extra thing that does occur to me is that the direction research has taken since GPT-2 has\ndefinitely been in the direction of larger models.  The attention weight matrices are\nsized <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>d</mi><mtext>emb</mtext></msub><mi>&#x000d7;</mi><msub><mi>d</mi><mtext>emb</mtext></msub></mrow></math>, so excluding bias they have <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msubsup><mi>d</mi><mtext>emb</mtext><mn>2</mn></msubsup></mrow></math> weights\neach.  Bias adds on another <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>d</mi><mtext>emb</mtext></msub></mrow></math>.  So, as a model scales up, the attention-related\nnon-bias weights will scale quadratically -- doubling <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>d</mi><mtext>emb</mtext></msub></mrow></math> will square their number --\nwhile the bias weights will scale linearly.</p>\n\n<p>So perhaps it's just that the effect -- whatever causes it -- gets rapidly swamped\nas you scale out of toy-model territory.  That, at least, seems pretty plausible.</p>\n\n<p>One final note to self, though: these improvements are small enough that I do find\nmyself wondering whether or not it might be some kind of noise, despite the setting of\nthe random seeds I'm doing:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">42</span>\n    <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">manual_seed_all</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>I think that at the end of this, before I do a final train, it would be worth doing\nanother baseline train and measuring the test set loss again, and doing another comparison.\nIf it comes out exactly the same -- and I can bump up the number of significant figures\nin the output, it's just a formatting parameter -- then I don't need to worry.  But if\nthey vary to some degree, perhaps I'll need to update my mental model of what level of\nfinding is significant, and what isn't.</p>\n\n<h3 id=\"summing-up\">Summing up</h3>\n\n<p>I think it goes without saying that QKV bias definitely goes onto the list of interventions\nwe want to add when training our best-possible GPT-2 small-scale model, assuming that the\nrandom seed test goes well.  That surprises\nme a bit, I was expecting it to have negligible impact!  That, of course, is why it's worth\ndoing these tests.</p>\n\n<p>Next up, I think, is trying to understand how we can tweak the learning rate, and its associated\nparameters like weight decay.  This will need a bit of a deep dive, so you can expect the next\npost late next week, or perhaps even later.  I'm sure you can't wait ;-)</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>Note to self: is there some way I could quantitatively measure those?&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
      "content": "<p>I'm still seeing what I can do to improve the test loss for a from-scratch GPT-2 small base model, trained on code based on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".\nThis is the third intervention I'm trying: adding bias to the attention weight matrices.</p>\n\n<p>In the code from the book, we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">MultiHeadAttention</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span>\n        <span class=\"n\">context_length</span><span class=\"p\">,</span>\n        <span class=\"n\">dropout</span><span class=\"p\">,</span>\n        <span class=\"n\">num_heads</span><span class=\"p\">,</span>\n        <span class=\"n\">qkv_bias</span><span class=\"o\">=</span><span class=\"kc\">False</span>\n    <span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_query</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_key</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linea",
      "published": "2026-02-06T23:55:00+00:00",
      "source": "gilesthomas.com",
      "source_url": "https://www.gilesthomas.com/",
      "word_count": 18461
    },
    {
      "title": "Reading List 02/06/2026",
      "link": "https://www.construction-physics.com/p/reading-list-02062026",
      "summary": "Welcome to the reading list, a look at what happened this week in infrastructure, buildings, and building things.",
      "content": "<p></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!1UOc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b7cca37-4b45-4739-85b9-d33039f05165_680x471.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"471\" src=\"https://substackcdn.com/image/fetch/$s_!1UOc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b7cca37-4b45-4739-85b9-d33039f05165_680x471.png\" width=\"680\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div><",
      "published": "2026-02-06T13:03:43+00:00",
      "source": "construction-physics.com",
      "source_url": "https://www.construction-physics.com",
      "word_count": 9954
    },
    {
      "title": "A Quiet Townhouse, A Great Gift",
      "link": "https://feed.tedium.co/link/15204/17271801/new-york-plaques-historic-moments",
      "summary": "A mostly unknown townhouse in Manhattan was the site of a small but significant moment in the history of 20th-century American literature. It also gives insight into how modern society defines its history.",
      "content": "<h2>A mostly unknown townhouse in Manhattan was the site of a small but significant moment in the history of 20th-century American literature. It also gives insight into how modern society defines its history.</h2><img alt=\"A Quiet Townhouse, A Great Gift\" src=\"https://static.tedium.co/uploads/tedium_020626.gif\" /><div class=\"whitebox\"><p><em>Hey all, Ernie here with a piece from an old friend—<a href=\"https://tedium.co/author/andrew/\">Andrew Egan</a>. This is his first piece since 2023, and we’re happy to have him back on here once again. I’ll be back at the bottom of the piece with some interesting links. Anyway, over to Andrew:</em></p><p><strong>A favorite pastime of tourists</strong> visiting New York City is learning the names and locations of various, usually famous, neighborhoods. They often get them wrong, but when in Rome it helps to speak some Latin. Some neighborhoods are pretty well-defined, like TriBeca and SoHo. Many others are not.</p><p>Take the area just north of the United Nations Headquarters, as an example. This area, north of 43rd Street and south of 53rd, and bordered by the East River and Lexington Avenue to the west, is understandably home to many diplomats and support staff for the United Nations. Permanent missions and consuls dot the area. Most commonly known as Turtle Bay, the name does change with slight boundary variations. And, of course, areas change over time.</p><p>This part of Manhattan saw its first European settlement in the 1600s as a Dutch farm. During the American Revolution, British General William Howe established his headquarters in the area. It was here that Nathan Hale, spy and hero of the Independence movement, said his famous,<br />\npossibly apocryphal, last words, “I regret that I have but one life to lose for my country.” Last words are not the only aspect of Hale’s life in dispute, as the exact location of his death is not known either, but it is immortalized on First Avenue between 49th and 50th.</p><figure><img al",
      "published": "2026-02-06T23:29:20+00:00",
      "source": "tedium.co",
      "source_url": "https://tedium.co/",
      "word_count": 16243
    },
    {
      "title": "Premium: The Hater's Guide To Microsoft",
      "link": "https://www.wheresyoured.at/premium-the-haters-guide-to-microsoft/",
      "summary": "<p>Have you ever looked at something too long and felt like you were sort of seeing through it? Has anybody actually looked at a company this much in a way that wasn&#x2019;t some sort of obsequious profile of a person who worked there? I don&#x2019;t mean</p>",
      "content": "<p>Have you ever looked at something too long and felt like you were sort of seeing through it? Has anybody actually looked at a company this much in a way that wasn&#x2019;t some sort of obsequious profile of a person who worked there? I don&#x2019;t mean this as a way to fish for compliments &#x2014; this experience is just so <em>peculiar,</em> because when you look at them hard enough, you begin to wonder why everybody isn&#x2019;t just <em>screaming</em> all the <em>time.&#xa0;</em></p><p>Yet I really do enjoy it. When you push aside all the marketing and the interviews and all that and stare at what a company actually does and what its users and employees say, you really get a feel of the guts of a company. I&#x2019;m enjoying it. The Hater&#x2019;s Guides are a lot of fun, and I&#x2019;m learning all sorts of things about the ways in which companies try to hide their nasty little accidents and proclivities.&#xa0;</p><p>Today, I focus on one of the largest.&#xa0;</p><p>In the last year I&#x2019;ve spoken to over a hundred different tech workers, and the ones I hear most consistently from are the current and former victims of Microsoft, a company with a culture in decline, in large part thanks to its obsession with AI. Every single person I talk to about this company has venom on their tongue, whether they&#x2019;re a regular user of Microsoft Teams or somebody who was unfortunate to work at the company any time in the last decade.</p><p>Microsoft exists as a kind of dark presence over business software and digital infrastructure. You inevitably have to interact with one of its products &#x2014; maybe it&#x2019;s because somebody you work with uses Teams, maybe it&#x2019;s because you&#x2019;re forced to use SharePoint, or perhaps you&#x2019;re suffering at the hands of PowerBI &#x2014; because Microsoft is the <em>king of software sales.</em> It exists entirely to seep into the veins of an organization and force every computer to use Microsoft 365, or sit on e",
      "published": "2026-02-06T17:34:14+00:00",
      "source": "wheresyoured.at",
      "source_url": "https://www.wheresyoured.at/",
      "word_count": 32280
    },
    {
      "title": "Ultima IX",
      "link": "https://www.filfre.net/2026/02/ultima-ix/",
      "summary": "This article tells part of the story of the Ultima series. Years ago, [Origin Systems] released Strike Commander, a high-concept flight sim that, while very entertaining from a purely theoretical point of view, was so resource-demanding that no one in the country actually owned a machine that could play it. Later, in Ultima VIII, the [&#8230;]",
      "content": "<p><a href=\"https://www.filfre.net/2026/02/ultima-ix/4051234-ultima-ix-ascension-windows-front-cover/\" rel=\"attachment wp-att-6637\"><img alt=\"\" class=\"aligncenter size-medium wp-image-6637\" height=\"600\" src=\"https://www.filfre.net/wp-content/uploads/2026/01/4051234-ultima-ix-ascension-windows-front-cover-488x600.jpg\" width=\"488\" /></a></p>\n<hr />\n<div style=\"margin-bottom: 8px; text-align: center;\"><strong>This article tells part of the story of <a href=\"/tag/ultima/?order=asc\">the <em>Ultima</em> series</a>.</strong></div>\n<div style=\"margin-bottom: 32px;\">\n<hr />\n</div>\n<blockquote><p>Years ago, [Origin Systems] released <a href=\"https://www.mobygames.com/game/1365/strike-commander/\">Strike Commander</a>, a high-concept flight sim that, while very entertaining from a purely theoretical point of view, was so resource-demanding that no one in the country actually owned a machine that could play it. Later, in <a href=\"https://www.mobygames.com/game/723/pagan-ultima-viii/\">Ultima VIII</a>, the company decided to try to increase their sales numbers by adding action sequences straight out of a platform game to their ultra-deep RPG. The results managed to piss just about everyone off. With Ultima IX: Ascension, the company has made both mistakes again, but this time on a scale that is likely to make everyone finally forget about the company&#8217;s past mistakes and concentrate their efforts on making fun of this one.</p>\n<p style=\"text-align: right;\">&#8212; Trent C. Ward, writing for IGN</p>\n<p>Appalling voice-acting. Clunky dialog-tree system. Over-simplistic, poorly implemented combat system. Disjointed story line&#8230; A huge slap in the face for all longtime Ultima fans&#8230; Insulting and contemptuous.</p>\n<p style=\"text-align: right;\">&#8212; Julian Schoffel, writing from the Department of &#8220;Other Than That, It Was Great&#8221; at Growling Dog Gaming</p>\n</blockquote>\n<p>The late 1990s introduced a new phenomenon to the culture of gaming: the truly epic fa",
      "published": "2026-02-06T17:09:05+00:00",
      "source": "filfre.net",
      "source_url": "https://www.filfre.net",
      "word_count": 53500
    },
    {
      "title": "Study Finds Obvious Truth Everybody Knows",
      "link": "https://blog.jim-nielsen.com/2026/study-finds-obvious-truth/",
      "summary": "<p>Researchers at Anthropic published their findings around <a href=\"https://www.anthropic.com/research/AI-assistance-coding-skills\">how AI assistance impacts the formation of coding skills</a>:</p>\n<blockquote>\n<p>We found that using AI assistance led to a statistically significant decrease in mastery […] Using AI sped up the task slightly, but this didn’t reach the threshold of statistical significance.</p>\n</blockquote>\n<p>Wait, what? Let me read that again: </p>\n<blockquote>\n<p>using AI assistance led to a statistically significant decrease in mastery</p>\n</blockquote>\n<p>Ouch.</p>\n<p>Honestly, the entire articles reads like those pieces you find on the internet with titles such as “Study Finds Exercise Is Good for Your Health” or “Being Kind to Others Makes People Happier”.</p>\n<p>Here’s another headline for you: Study Finds Doing Hard Things Leads to Mastery.</p>\n<blockquote>\n<p>Cognitive effort—and even getting painfully stuck—is likely important for fostering mastery.</p>\n</blockquote>\n<p>We already know this. Do we really need a study for this?</p>\n<p>So what are their recommendations? Here’s one:</p>\n<blockquote>\n<p>Managers should think intentionally about how to deploy AI tools at scale</p>\n</blockquote>\n<p>Lol, yeah that’s gonna happen. You know what’s gonna happen instead? What always happens when organizational pressures and incentives are aligned to deskill workers.</p>\n<p>Oh wait, they already came to that conclusion in the article:</p>\n<blockquote>\n<p>Given time constraints and organizational pressures, junior developers or other professionals may rely on AI to complete tasks as fast as possible at the cost of skill development</p>\n</blockquote>\n<p>AI is like a creditor: they give you a bunch of money and don’t talk about the trade-offs, just the fact that you’ll be more “rich” after they get involved.</p>\n<p>Or maybe a better analogy is <a href=\"https://en.wikipedia.org/wiki/Rumpelstiltskin\">Rumpelstilskin</a>: the promise is gold, but beware the hidden cost might be your first-born child.</p>\n\n    <hr />\n    \n\n    <p>\n      Reply via:\n      \n\n      <a href=\"mailto:jimniels%2Bblog@gmail.com?subject=Re:%20blog.jim-nielsen.com/2026/study-finds-obvious-truth/\">Email</a>\n      · <a href=\"https://mastodon.social/@jimniels\">Mastodon</a> ·\n\n      <a href=\"https://bsky.app/profile/jim-nielsen.com\">Bluesky</a>\n    </p>",
      "content": "<p>Researchers at Anthropic published their findings around <a href=\"https://www.anthropic.com/research/AI-assistance-coding-skills\">how AI assistance impacts the formation of coding skills</a>:</p>\n<blockquote>\n<p>We found that using AI assistance led to a statistically significant decrease in mastery […] Using AI sped up the task slightly, but this didn’t reach the threshold of statistical significance.</p>\n</blockquote>\n<p>Wait, what? Let me read that again: </p>\n<blockquote>\n<p>using AI assistance led to a statistically significant decrease in mastery</p>\n</blockquote>\n<p>Ouch.</p>\n<p>Honestly, the entire articles reads like those pieces you find on the internet with titles such as “Study Finds Exercise Is Good for Your Health” or “Being Kind to Others Makes People Happier”.</p>\n<p>Here’s another headline for you: Study Finds Doing Hard Things Leads to Mastery.</p>\n<blockquote>\n<p>Cognitive effort—and even getting painfully stuck—is likely important for fostering mastery.</p>\n</blockquote>\n<p>We already know this. Do we really need a study for this?</p>\n<p>So what are their recommendations? Here’s one:</p>\n<blockquote>\n<p>Managers should think intentionally about how to deploy AI tools at scale</p>\n</blockquote>\n<p>Lol, yeah that’s gonna happen. You know what’s gonna happen instead? What always happens when organizational pressures and incentives are aligned to deskill workers.</p>\n<p>Oh wait, they already came to that conclusion in the article:</p>\n<blockquote>\n<p>Given time constraints and organizational pressures, junior developers or other professionals may rely on AI to complete tasks as fast as possible at the cost of skill development</p>\n</blockquote>\n<p>AI is like a creditor: they give you a bunch of money and don’t talk about the trade-offs, just the fact that you’ll be more “rich” after they get involved.</p>\n<p>Or maybe a better analogy is <a href=\"https://en.wikipedia.org/wiki/Rumpelstiltskin\">Rumpelstilskin</a>: the promise is gold, but beware the ",
      "published": "2026-02-06T19:00:00+00:00",
      "source": "blog.jim-nielsen.com",
      "source_url": "https://blog.jim-nielsen.com",
      "word_count": 2370
    },
    {
      "title": "The Discovery Phase Is All There Is",
      "link": "https://worksonmymachine.ai/p/the-discovery-phase-is-all-there",
      "summary": "Or: Documents Retrieved from the Department of Best Practices, Third Sub-Basement, The Building",
      "content": "<div class=\"pullquote\"><p>The following materials were recovered from a decommissioned Slack workspace. The workspace itself was migrated to a new platform six hours after these documents were created. The new platform has since been deprecated. We have preserved them here for historical purposes. We realize &#8220;historical&#8221; may not be the right word for things that happened last week. Some of this may be familiar.</p></div><h2>I. Orientation</h2><p>You will receive your Mission on your first day.</p><p>This is not technically true. You will receive <em>a</em> Mission. Whether it is <em>your</em> Mission depends on factors that will not be explained to you, because explaining them would compromise the Mission, or because no one remembers, or because the explanation was stored in a system we no longer use.</p><p>The Building has many floors. Some are numbered. Some are lettered. Some are named after concepts that were important when the floor was constructed but have since been deprecated. You may hear references to &#8220;The RAG Floor&#8221; or &#8220;The Prompt Engineering Wing&#8221; or &#8220;The Chamber of System Prompts.&#8221; These places exist. They also don&#8217;t exist. The architecture is&#8230; <em>responsive.</em></p><p>Do not ask for a map. You&#8217;ll ask anyway.</p><p>Maps are available.</p><pre><code>                    THE BUILDING\n                    ============\n                    \n         [Floor 7: Advanced Techniques]\n                      &#8593; &#8595; &#8592; &#8594; &#8634;\n         [Floor 6: This floor has been merged with Floor 8]\n                      &#8593; &#8595; &#8592; &#8594; &#8634;  \n         [Floor 5: Core Concepts (revised)]\n                      &#8593; &#8595; &#8592; &#8594; &#8634;\n         [Floor 5: Core Concepts (original) (deprecated) (restored) (see memo)]\n                      &#8593; &#8595; &#8592; &#8594; &#8634;\n         [Loading additional floors...]</code></pre><p>The loading screen is a feature no",
      "published": "2026-02-06T14:44:29+00:00",
      "source": "worksonmymachine.substack.com",
      "source_url": "https://worksonmymachine.ai",
      "word_count": 27561
    }
  ]
}