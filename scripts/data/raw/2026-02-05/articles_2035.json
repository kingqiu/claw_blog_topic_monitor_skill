{
  "metadata": {
    "fetch_time": "2026-02-05T20:35:02.230062+08:00",
    "time_window_start": "2026-02-04T20:35:02.230062+08:00",
    "time_window_end": "2026-02-05T20:35:02.230062+08:00",
    "hours_range": 24,
    "total_articles": 13
  },
  "articles": [
    {
      "title": "Spotlighting The World Factbook as We Bid a Fond Farewell",
      "link": "https://simonwillison.net/2026/Feb/5/the-world-factbook/#atom-everything",
      "summary": "<p><strong><a href=\"https://www.cia.gov/stories/story/spotlighting-the-world-factbook-as-we-bid-a-fond-farewell/\">Spotlighting The World Factbook as We Bid a Fond Farewell</a></strong></p>\nSomewhat devastating news today from CIA:</p>\n<blockquote>\n<p>One of CIA’s oldest and most recognizable intelligence publications, The World Factbook, has sunset.</p>\n</blockquote>\n<p>There's not even a hint as to <em>why</em> they decided to stop maintaining this publication, which has been their most useful public-facing initiative since 1971 and a cornerstone of the public internet since 1997.</p>\n<p>In a bizarre act of cultural vandalism they've not just removed the entire site (including the archives of previous versions) but they've also set every single page to be a 302 redirect to their closure announcement.</p>\n<p>The Factbook has been released into the public domain since the start. There's no reason not to continue to serve archived versions - a banner at the top of the page saying it's no longer maintained would be much better than removing all of that valuable content entirely.</p>\n<p>Up until 2020 the CIA published annual zip file archives of the entire site. Those are available (along with the rest of the Factbook) <a href=\"https://web.archive.org/web/20260203124934/https://www.cia.gov/the-world-factbook/about/archives/\">on the Internet Archive</a>.</p>\n<p>I downloaded the 384MB <code>.zip</code> file for the year 2020 and extracted it into a new GitHub repository, <a href=\"https://github.com/simonw/cia-world-factbook-2020/\">simonw/cia-world-factbook-2020</a>. I've enabled GitHub Pages for that repository so you can browse the archived copy at <a href=\"https://simonw.github.io/cia-world-factbook-2020\">simonw.github.io/cia-world-factbook-2020/</a>.</p>\n<p><img alt=\"Screenshot of the CIA World Factbook website homepage. Header reads &quot;THE WORLD FACTBOOK&quot; with a dropdown labeled &quot;Please select a country to view.&quot; Navigation tabs: ABOUT, REFERENCES, APPENDICES, FAQs. Section heading &quot;WELCOME TO THE WORLD FACTBOOK&quot; followed by descriptive text: &quot;The World Factbook provides information on the history, people and society, government, economy, energy, geography, communications, transportation, military, and transnational issues for 267 world entities. The Reference tab includes: a variety of world, regional, country, ocean, and time zone maps; Flags of the World; and a Country Comparison function that ranks the country information and data in more than 75 Factbook fields.&quot; A satellite image of Earth is displayed on the right. Below it: &quot;WHAT'S NEW :: Today is: Wednesday, February 4.&quot; Left sidebar links with icons: WORLD TRAVEL FACTS, ONE-PAGE COUNTRY SUMMARIES, REGIONAL AND WORLD MAPS, FLAGS OF THE WORLD, GUIDE TO COUNTRY COMPARISONS. Right side shows news updates dated December 17, 2020 about Electricity access and new Economy fields, and December 10, 2020 about Nepal and China agreeing on the height of Mount Everest at 8,848.86 meters. A &quot;VIEW ALL UPDATES&quot; button appears at the bottom.\" src=\"https://static.simonwillison.net/static/2025/factbook-2020.jpg\" /></p>\n<p>Here's a neat example of the editorial voice of the Factbook from the <a href=\"https://simonw.github.io/cia-world-factbook-2020/docs/whatsnew.html\">What's New page</a>, dated December 10th 2020:</p>\n<blockquote>\n<p>Years of wrangling were brought to a close this week when officials from Nepal and China announced that they have agreed on the height of Mount Everest. The mountain sits on the border between Nepal and Tibet (in western China), and its height changed slightly following an earthquake in 2015. The new height of 8,848.86 meters is just under a meter higher than the old figure of 8,848 meters. <em>The World Factbook</em> rounds the new measurement to 8,849 meters and this new height has been entered throughout the <em>Factbook</em> database.</p>\n</blockquote>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46891794\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/cia\">cia</a>, <a href=\"https://simonwillison.net/tags/github\">github</a>, <a href=\"https://simonwillison.net/tags/internet-archive\">internet-archive</a></p>",
      "content": "<p><strong><a href=\"https://www.cia.gov/stories/story/spotlighting-the-world-factbook-as-we-bid-a-fond-farewell/\">Spotlighting The World Factbook as We Bid a Fond Farewell</a></strong></p>\nSomewhat devastating news today from CIA:</p>\n<blockquote>\n<p>One of CIA’s oldest and most recognizable intelligence publications, The World Factbook, has sunset.</p>\n</blockquote>\n<p>There's not even a hint as to <em>why</em> they decided to stop maintaining this publication, which has been their most useful public-facing initiative since 1971 and a cornerstone of the public internet since 1997.</p>\n<p>In a bizarre act of cultural vandalism they've not just removed the entire site (including the archives of previous versions) but they've also set every single page to be a 302 redirect to their closure announcement.</p>\n<p>The Factbook has been released into the public domain since the start. There's no reason not to continue to serve archived versions - a banner at the top of the page saying it's no longer maintained would be much better than removing all of that valuable content entirely.</p>\n<p>Up until 2020 the CIA published annual zip file archives of the entire site. Those are available (along with the rest of the Factbook) <a href=\"https://web.archive.org/web/20260203124934/https://www.cia.gov/the-world-factbook/about/archives/\">on the Internet Archive</a>.</p>\n<p>I downloaded the 384MB <code>.zip</code> file for the year 2020 and extracted it into a new GitHub repository, <a href=\"https://github.com/simonw/cia-world-factbook-2020/\">simonw/cia-world-factbook-2020</a>. I've enabled GitHub Pages for that repository so you can browse the archived copy at <a href=\"https://simonw.github.io/cia-world-factbook-2020\">simonw.github.io/cia-world-factbook-2020/</a>.</p>\n<p><img alt=\"Screenshot of the CIA World Factbook website homepage. Header reads &quot;THE WORLD FACTBOOK&quot; with a dropdown labeled &quot;Please select a country to view.&quot; Navigation tabs: ABOUT, REFERENCES, A",
      "published": "2026-02-05T00:23:38+00:00",
      "source": "simonwillison.net",
      "source_url": "http://simonwillison.net/",
      "word_count": 4274
    },
    {
      "title": "Voxtral transcribes at the speed of sound",
      "link": "https://simonwillison.net/2026/Feb/4/voxtral-2/#atom-everything",
      "summary": "<p><strong><a href=\"https://mistral.ai/news/voxtral-transcribe-2\">Voxtral transcribes at the speed of sound</a></strong></p>\nMistral just released Voxtral Transcribe 2 - a family of two new models, one open weights, for transcribing audio to text. This is the latest in their Whisper-like model family, and a sequel to the original Voxtral which they released <a href=\"https://simonwillison.net/2025/Jul/16/voxtral/\">in July 2025</a>.</p>\n<p>Voxtral Realtime - official name <code>Voxtral-Mini-4B-Realtime-2602</code> - is the open weights (Apache-2.0) model, available as a <a href=\"https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602\">8.87GB download from Hugging Face</a>.</p>\n<p>You can try it out in this <a href=\"https://huggingface.co/spaces/mistralai/Voxtral-Mini-Realtime\">live demo</a> - don't be put off by the \"No microphone found\" message, clicking \"Record\" should have your browser request permission and then start the demo working. I was very impressed by the demo - I talked quickly and used jargon like Django and WebAssembly and it correctly transcribed my text within moments of me uttering each sound. </p>\n<p>The closed weight model is called <code>voxtral-mini-latest</code> and can be accessed via the Mistral API, using calls that look something like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -X POST <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://api.mistral.ai/v1/audio/transcriptions<span class=\"pl-pds\">\"</span></span> \\\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Authorization: Bearer <span class=\"pl-smi\">$MISTRAL_API_KEY</span><span class=\"pl-pds\">\"</span></span> \\\n  -F model=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>voxtral-mini-latest<span class=\"pl-pds\">\"</span></span> \\\n  -F file=@<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Pelican talk at the library.m4a<span class=\"pl-pds\">\"</span></span> \\\n  -F diarize=true \\\n  -F context_bias=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Datasette<span class=\"pl-pds\">\"</span></span> \\\n  -F timestamp_granularities=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>segment<span class=\"pl-pds\">\"</span></span></pre></div>\n\n<p>It's priced at $0.003/minute, which is $0.18/hour.</p>\n<p>The Mistral API console now has a <a href=\"https://console.mistral.ai/build/audio/speech-to-text\">speech-to-text playground</a> for exercising the new model and it is <em>excellent</em>. You can upload an audio file and promptly get a diarized transcript in a pleasant interface, with options to download the result in text, SRT or JSON format.</p>\n<p><img alt=\"Screenshot of a speech-to-text transcription interface for a file named &quot;Pelican talk at the library.m4a&quot;. The toolbar shows &quot;Speech to text&quot; with Code, Transcribe, and Download buttons. The transcript shows timestamped segments from 5:53 to 6:53 with a speaker icon, reading: &quot;5:53 – 6:01 So pelicans love to, they're very good at getting the most they can out of the topography when they're flying. 6:01 – 6:06 And our winds come in from the northwest and they hit those bluffs and they're deflected up. 6:07 – 6:18 And they will sit right, they'll fly north into a wind like five feet off those bluffs, but just five or ten feet off the surface because the winds dissipate. 6:19 – 6:22 And they will surf that bluff all the way north. 6:23 – 6:30 So you'll see a wind from the north at 15 miles an hour, and the pelicans are flying north into that wind and not flapping their wings. 6:31 – 6:33 And it's one of the coolest things. 6:33 – 6:35 You can only find it on San Francisco Coast. 6:36 – 6:39 Where right where the bluffs are steep. 6:41 – 6:43 Pacifica, you can find them there. 6:43 – 6:51 They like their, what we call pier bums, which are typically pelicans that have, are in some sort of trouble. 6:51 – 6:53 They're unable to catch food.&quot; The segment at 6:41–6:43 is highlighted in yellow. An audio waveform is shown at the bottom with a playhead near 6:40. Stats in the lower right show 53.90s, 7946.00s, and #45833.\" src=\"https://static.simonwillison.net/static/2025/mistral-transcript-ui.jpg\" />\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46886735\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/hugging-face\">hugging-face</a>, <a href=\"https://simonwillison.net/tags/mistral\">mistral</a>, <a href=\"https://simonwillison.net/tags/speech-to-text\">speech-to-text</a></p>",
      "content": "<p><strong><a href=\"https://mistral.ai/news/voxtral-transcribe-2\">Voxtral transcribes at the speed of sound</a></strong></p>\nMistral just released Voxtral Transcribe 2 - a family of two new models, one open weights, for transcribing audio to text. This is the latest in their Whisper-like model family, and a sequel to the original Voxtral which they released <a href=\"https://simonwillison.net/2025/Jul/16/voxtral/\">in July 2025</a>.</p>\n<p>Voxtral Realtime - official name <code>Voxtral-Mini-4B-Realtime-2602</code> - is the open weights (Apache-2.0) model, available as a <a href=\"https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602\">8.87GB download from Hugging Face</a>.</p>\n<p>You can try it out in this <a href=\"https://huggingface.co/spaces/mistralai/Voxtral-Mini-Realtime\">live demo</a> - don't be put off by the \"No microphone found\" message, clicking \"Record\" should have your browser request permission and then start the demo working. I was very impressed by the demo - I talked quickly and used jargon like Django and WebAssembly and it correctly transcribed my text within moments of me uttering each sound. </p>\n<p>The closed weight model is called <code>voxtral-mini-latest</code> and can be accessed via the Mistral API, using calls that look something like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -X POST <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://api.mistral.ai/v1/audio/transcriptions<span class=\"pl-pds\">\"</span></span> \\\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Authorization: Bearer <span class=\"pl-smi\">$MISTRAL_API_KEY</span><span class=\"pl-pds\">\"</span></span> \\\n  -F model=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>voxtral-mini-latest<span class=\"pl-pds\">\"</span></span> \\\n  -F file=@<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Pelican talk at the library.m4a<span class=\"pl-pds\">\"</span></span> \\\n  -F diarize=true \\\n  -F context_bias=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Datasette<span c",
      "published": "2026-02-04T22:42:34+00:00",
      "source": "simonwillison.net",
      "source_url": "http://simonwillison.net/",
      "word_count": 4654
    },
    {
      "title": "Distributing Go binaries like sqlite-scanner through PyPI using go-to-wheel",
      "link": "https://simonwillison.net/2026/Feb/4/distributing-go-binaries/#atom-everything",
      "summary": "<p>I've been exploring Go for building small, fast and self-contained binary applications recently. I'm enjoying how there's generally one obvious way to do things and the resulting code is boring and readable - and something that LLMs are very competent at writing. The one catch is distribution, but it turns out publishing Go binaries to PyPI means any Go binary can be just a <code>uvx package-name</code> call away.</p>\n<h4 id=\"sqlite-scanner\">sqlite-scanner</h4>\n<p><a href=\"https://github.com/simonw/sqlite-scanner\">sqlite-scanner</a> is my new Go CLI tool for scanning a filesystem for SQLite database files.</p>\n<p>It works by checking if the first 16 bytes of the file exactly match the SQLite magic number sequence <code>SQLite format 3\\x00</code>. It can search one or more folders recursively, spinning up concurrent goroutines to accelerate the scan. It streams out results as it finds them in plain text, JSON or newline-delimited JSON. It can optionally display the file sizes as well.</p>\n<p>To try it out you can download a release from the <a href=\"https://github.com/simonw/sqlite-scanner/releases\">GitHub releases</a> - and then <a href=\"https://support.apple.com/en-us/102445\">jump through macOS hoops</a> to execute an \"unsafe\" binary. Or you can clone the repo and compile it with Go. Or... you can run the binary like this:</p>\n<pre><code>uvx sqlite-scanner\n</code></pre>\n<p>By default this will search your current directory for SQLite databases. You can pass one or more directories as arguments:</p>\n<pre><code>uvx sqlite-scanner ~ /tmp\n</code></pre>\n<p>Add <code>--json</code> for JSON output, <code>--size</code> to include file sizes or <code>--jsonl</code> for newline-delimited JSON. Here's a demo:</p>\n<pre><code>uvx sqlite-scanner ~ --jsonl --size\n</code></pre>\n<p><img alt=\"running that command produces a sequence of JSON objects, each with a path and a size key\" src=\"https://static.simonwillison.net/static/2025/sqlite-scanner-demo.gif\" /></p>\n<p>If you haven't been uv-pilled yet you can instead install <code>sqlite-scanner</code> using <code>pip install sqlite-scanner</code> and then run <code>sqlite-scanner</code>.</p>\n<p>To get a permanent copy with <code>uv</code> use <code>uv tool install sqlite-scanner</code>.</p>\n<h4 id=\"how-the-python-package-works\">How the Python package works</h4>\n<p>The reason this is worth doing is that <code>pip</code>, <code>uv</code> and <a href=\"https://pypi.org/\">PyPI</a> will work together to identify the correct compiled binary for your operating system and architecture.</p>\n<p>This is driven by file names. If you visit <a href=\"https://pypi.org/project/sqlite-scanner/#files\">the PyPI downloads for sqlite-scanner</a> you'll see the following files:</p>\n<ul>\n<li><code>sqlite_scanner-0.1.1-py3-none-win_arm64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-win_amd64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-musllinux_1_2_x86_64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-musllinux_1_2_aarch64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-manylinux_2_17_x86_64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-manylinux_2_17_aarch64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-macosx_10_9_x86_64.whl</code></li>\n</ul>\n<p>When I run <code>pip install sqlite-scanner</code> or <code>uvx sqlite-scanner</code> on my Apple Silicon Mac laptop Python's packaging magic ensures I get that <code>macosx_11_0_arm64.whl</code> variant.</p>\n<p>Here's <a href=\"https://tools.simonwillison.net/zip-wheel-explorer?url=https%3A%2F%2Ffiles.pythonhosted.org%2Fpackages%2F88%2Fb1%2F17a716635d2733fec53ba0a8267f85bd6b6cf882c6b29301bc711fba212c%2Fsqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl#sqlite_scanner/__init__.py\">what's in the wheel</a>, which is a zip file with a <code>.whl</code> extension.</p>\n<p>In addition to the <code>bin/sqlite-scanner</code> the most important file is <code>sqlite_scanner/__init__.py</code> which includes the following:</p>\n<pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">get_binary_path</span>():\n    <span class=\"pl-s\">\"\"\"Return the path to the bundled binary.\"\"\"</span>\n    <span class=\"pl-s1\">binary</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">path</span>.<span class=\"pl-c1\">join</span>(<span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">path</span>.<span class=\"pl-c1\">dirname</span>(<span class=\"pl-s1\">__file__</span>), <span class=\"pl-s\">\"bin\"</span>, <span class=\"pl-s\">\"sqlite-scanner\"</span>)\n \n    <span class=\"pl-c\"># Ensure binary is executable on Unix</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">platform</span> <span class=\"pl-c1\">!=</span> <span class=\"pl-s\">\"win32\"</span>:\n        <span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">stat</span>(<span class=\"pl-s1\">binary</span>).<span class=\"pl-c1\">st_mode</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">not</span> (<span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">&amp;</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXUSR</span>):\n            <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">chmod</span>(<span class=\"pl-s1\">binary</span>, <span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXUSR</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXGRP</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXOTH</span>)\n \n    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">binary</span>\n \n \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-s\">\"\"\"Execute the bundled binary.\"\"\"</span>\n    <span class=\"pl-s1\">binary</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">get_binary_path</span>()\n \n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">platform</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s\">\"win32\"</span>:\n        <span class=\"pl-c\"># On Windows, use subprocess to properly handle signals</span>\n        <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">exit</span>(<span class=\"pl-s1\">subprocess</span>.<span class=\"pl-c1\">call</span>([<span class=\"pl-s1\">binary</span>] <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">argv</span>[<span class=\"pl-c1\">1</span>:]))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c\"># On Unix, exec replaces the process</span>\n        <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">execvp</span>(<span class=\"pl-s1\">binary</span>, [<span class=\"pl-s1\">binary</span>] <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">argv</span>[<span class=\"pl-c1\">1</span>:])</pre>\n<p>That <code>main()</code> method - also called from <code>sqlite_scanner/__main__.py</code> - locates the binary and executes it when the Python package itself is executed, using the <code>sqlite-scanner = sqlite_scanner:main</code> entry point defined in the wheel.</p>\n<h4 id=\"which-means-we-can-use-it-as-a-dependency\">Which means we can use it as a dependency</h4>\n<p>Using PyPI as a distribution platform for Go binaries feels a tiny bit abusive, albeit <a href=\"https://simonwillison.net/2022/May/23/bundling-binary-tools-in-python-wheels/\">there is plenty of precedent</a>.</p>\n<p>I’ll justify it by pointing out that this means <strong>we can use Go binaries as dependencies</strong> for other Python packages now.</p>\n<p>That's genuinely useful! It means that any functionality which is available in a cross-platform Go binary can now be subsumed into a Python package. Python is really good at running subprocesses so this opens up a whole world of useful tricks that we can bake into our Python tools.</p>\n<p>To demonstrate this, I built <a href=\"https://github.com/simonw/datasette-scan\">datasette-scan</a> - a new Datasette plugin which depends on <code>sqlite-scanner</code> and then uses that Go binary to scan a folder for SQLite databases and attach them to a Datasette instance.</p>\n<p>Here's how to use that (without even installing anything first, thanks <code>uv</code>) to explore any SQLite databases in your Downloads folder:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uv run --with datasette-scan datasette scan <span class=\"pl-k\">~</span>/Downloads</pre></div>\n<p>If you peek at the code you'll see it <a href=\"https://github.com/simonw/datasette-scan/blob/1a2b6d1e6b04c8cd05f5676ff7daa877efd99f08/pyproject.toml#L14\">depends on sqlite-scanner</a> in <code>pyproject.toml</code> and calls it using <code>subprocess.run()</code> against <code>sqlite_scanner.get_binary_path()</code> in its own <a href=\"https://github.com/simonw/datasette-scan/blob/1a2b6d1e6b04c8cd05f5676ff7daa877efd99f08/datasette_scan/__init__.py#L38-L58\">scan_directories() function</a>.</p>\n<p>I've been exploring this pattern for other, non-Go binaries recently - here's <a href=\"https://github.com/simonw/tools/blob/main/python/livestream-gif.py\">a recent script</a> that depends on <a href=\"https://pypi.org/project/static-ffmpeg/\">static-ffmpeg</a> to ensure that <code>ffmpeg</code> is available for the script to use.</p>\n<h4 id=\"building-python-wheels-from-go-packages-with-go-to-wheel\">Building Python wheels from Go packages with go-to-wheel</h4>\n<p>After trying this pattern myself a couple of times I realized it would be useful to have a tool to automate the process.</p>\n<p>I first <a href=\"https://claude.ai/share/2d9ced56-b3e8-4651-83cc-860b9b419187\">brainstormed with Claude</a> to check that there was no existing tool to do this. It pointed me to <a href=\"https://www.maturin.rs/bindings.html#bin\">maturin bin</a> which helps distribute Rust projects using Python wheels, and <a href=\"https://github.com/Bing-su/pip-binary-factory\">pip-binary-factory</a> which bundles all sorts of other projects, but did not identify anything that addressed the exact problem I was looking to solve.</p>\n<p>So I <a href=\"https://gisthost.github.io/?41f04e4eb823b1ceb888d9a28c2280dd/index.html\">had Claude Code for web build the first version</a>, then refined the code locally on my laptop with the help of more Claude Code and a little bit of OpenAI Codex too, just to mix things up.</p>\n<p>The full documentation is in the <a href=\"https://github.com/simonw/go-to-wheel\">simonw/go-to-wheel</a> repository. I've published that tool to PyPI so now you can run it using:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx go-to-wheel --help</pre></div>\n<p>The <code>sqlite-scanner</code> package you can <a href=\"https://pypi.org/project/sqlite-scanner/\">see on PyPI</a> was built using <code>go-to-wheel</code> like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx go-to-wheel <span class=\"pl-k\">~</span>/dev/sqlite-scanner \\\n  --set-version-var main.version \\\n  --version 0.1.1 \\\n  --readme README.md \\\n  --author <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Simon Willison<span class=\"pl-pds\">'</span></span> \\\n  --url https://github.com/simonw/sqlite-scanner \\\n  --description <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Scan directories for SQLite databases<span class=\"pl-pds\">'</span></span></pre></div>\n<p>This created a set of wheels in the <code>dist/</code> folder. I tested one of them like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uv run --with dist/sqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl \\\n  sqlite-scanner --version</pre></div>\n<p>When that spat out the correct version number I was confident everything had worked as planned, so I pushed the whole set of wheels to PyPI using <code>twine upload</code> like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx twine upload dist/<span class=\"pl-k\">*</span></pre></div>\n<p>I had to paste in a PyPI API token I had saved previously and that was all it took.</p>\n<h4 id=\"i-expect-to-use-this-pattern-a-lot\">I expect to use this pattern a lot</h4>\n<p><code>sqlite-scanner</code> is very clearly meant as a proof-of-concept for this wider pattern - Python is very much capable of recursively crawling a directory structure looking for files that start with a specific byte prefix on its own!</p>\n<p>That said, I think there's a <em>lot</em> to be said for this pattern. Go is a great complement to Python - it's fast, compiles to small self-contained binaries, has excellent concurrency support and a rich ecosystem of libraries.</p>\n<p>Go is similar to Python in that it has a strong standard library. Go is particularly good for HTTP tooling - I've built several HTTP proxies in the past using Go's excellent <code>net/http/httputil.ReverseProxy</code> handler.</p>\n<p>I've also been experimenting with <a href=\"https://github.com/wazero/wazero\">wazero</a>, Go's robust and mature zero dependency WebAssembly runtime as part of my ongoing quest for the ideal sandbox for running untrusted code. <a href=\"https://github.com/simonw/research/tree/main/wasm-repl-cli\">Here's my latest experiment</a> with that library.</p>\n<p>Being able to seamlessly integrate Go binaries into Python projects without the end user having to think about Go at all - they <code>pip install</code> and everything Just Works - feels like a valuable addition to my toolbox.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/go\">go</a>, <a href=\"https://simonwillison.net/tags/packaging\">packaging</a>, <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/pypi\">pypi</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sqlite\">sqlite</a>, <a href=\"https://simonwillison.net/tags/datasette\">datasette</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/uv\">uv</a></p>",
      "content": "<p>I've been exploring Go for building small, fast and self-contained binary applications recently. I'm enjoying how there's generally one obvious way to do things and the resulting code is boring and readable - and something that LLMs are very competent at writing. The one catch is distribution, but it turns out publishing Go binaries to PyPI means any Go binary can be just a <code>uvx package-name</code> call away.</p>\n<h4 id=\"sqlite-scanner\">sqlite-scanner</h4>\n<p><a href=\"https://github.com/simonw/sqlite-scanner\">sqlite-scanner</a> is my new Go CLI tool for scanning a filesystem for SQLite database files.</p>\n<p>It works by checking if the first 16 bytes of the file exactly match the SQLite magic number sequence <code>SQLite format 3\\x00</code>. It can search one or more folders recursively, spinning up concurrent goroutines to accelerate the scan. It streams out results as it finds them in plain text, JSON or newline-delimited JSON. It can optionally display the file sizes as well.</p>\n<p>To try it out you can download a release from the <a href=\"https://github.com/simonw/sqlite-scanner/releases\">GitHub releases</a> - and then <a href=\"https://support.apple.com/en-us/102445\">jump through macOS hoops</a> to execute an \"unsafe\" binary. Or you can clone the repo and compile it with Go. Or... you can run the binary like this:</p>\n<pre><code>uvx sqlite-scanner\n</code></pre>\n<p>By default this will search your current directory for SQLite databases. You can pass one or more directories as arguments:</p>\n<pre><code>uvx sqlite-scanner ~ /tmp\n</code></pre>\n<p>Add <code>--json</code> for JSON output, <code>--size</code> to include file sizes or <code>--jsonl</code> for newline-delimited JSON. Here's a demo:</p>\n<pre><code>uvx sqlite-scanner ~ --jsonl --size\n</code></pre>\n<p><img alt=\"running that command produces a sequence of JSON objects, each with a path and a size key\" src=\"https://static.simonwillison.net/static/2025/sqlite-scanner-demo.gif\" /></p>\n<p>If you haven't",
      "published": "2026-02-04T14:59:47+00:00",
      "source": "simonwillison.net",
      "source_url": "http://simonwillison.net/",
      "word_count": 14053
    },
    {
      "title": "Getting the main thing right",
      "link": "https://seangoedecke.com/getting-the-main-thing-right/",
      "summary": "<p>When you’re running a project in a tech company, understanding that your main job is to <strong>ship the project</strong> goes a surprisingly long way. So many engineers spend their time on peripheral questions (like the choice of technology X or Y) when core questions about shipping the product (for instance, how all the critical paths will actually work) are still unanswered<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"#fn-1\">1</a></sup>.</p>\n<p>If you’re able to reliably ship projects, you can get away with being slightly abrasive, or not filling out your Jira tickets correctly, or any number of other small faults that would cause other engineers to be punished.</p>\n<p>You could see this as a special case of the <a href=\"https://en.wikipedia.org/wiki/Pareto_principle\">Pareto principle</a>: the idea that 80% of consequences often come from 20% of causes. But I think in many contexts it’s even more extreme, closer to 90/10 or even 99/1. <strong>If you get the “main thing” right, you can get away with a lot of mistakes.</strong></p>\n<p>This principle holds in many other areas. When saving money, it doesn’t matter if you save a few dollars by hunting for deals if you then buy a car or house that’s on the edge of your budget. If you’re writing, clearly expressing your point will make up for awkward grammar or other mistakes, but even beautiful prose is bad writing if it doesn’t say what you mean. If you’re trying to get fit, consistency and avoiding injury is far more important than finding the most efficient program or the best gear. And so on.</p>\n<h3>Identifying the “main thing”</h3>\n<p><strong>How do you identify the main thing?</strong> This is a pretty deep question. I have written <em>extensively</em> about this when it comes to working in large tech companies: you can read <a href=\"/where-the-money-comes-from\"><em>Knowing where your engineer salary comes from</em></a>, or browse my posts tagged <a href=\"/tags/tech%20companies\">“tech companies”</a>. In under twenty words, I think it’s “delivering projects in order to increase shareholder value and make the ~2 layers of management above you happy”.</p>\n<p>From the way I’ve phrased it, it should be clear that I think this is the “main thing” <em>for working in tech companies</em>. It’s not the main thing for life in general, or for being a fulfilled software craftsperson, and so on. Those two domains have completely different main things<sup id=\"fnref-2\"><a class=\"footnote-ref\" href=\"#fn-2\">2</a></sup>.</p>\n<p>Sometimes the main thing seems too simple to be important. Plenty of software engineers think something like “of course it’s important to ship the project, but that only happens as a result of writing all the code”, underrating the set of complex factors (both in code and elsewhere) that have to come together for a successful ship.</p>\n<p>The only general reliable method I know is to carefully look at cases of success and failure, and to identify what the successes had in common. <strong>Pay particular attention to successes or failures that surprise you.</strong> If you thought a project was going really well but the people who ran it weren’t rewarded, or you thought a project was a complete disaster but it ended up being celebrated, that probably indicates that you’re mistaken about what the “main thing” is. Did someone get a staff promotion but you think they’re terrible? Is someone beloved by senior leadership, but you can’t see them doing anything that useful? Those people are probably getting the main thing right<sup id=\"fnref-3\"><a class=\"footnote-ref\" href=\"#fn-3\">3</a></sup>.</p>\n<h3>It’s hard to even try</h3>\n<p>The first step in correctly identifying the main thing is to <em>try</em>. In my experience, <strong>it is surprisingly hard to motivate yourself to focus on the main thing</strong>. It’s much more natural to just jump into something that looks probably useful and start working immediately. Why is this?</p>\n<p>One obvious reason is that it just feels bad to sit around contemplating all the things you could focus on. It’s much easier to account for your time - both to others and to yourself - if you look busy. What if you can’t come up with anything, and you’ve just wasted all the time you spent reflecting?</p>\n<p>Another, less obvious reason is that <strong>many people are afraid that they might not like the main thing</strong>. Recall my description of the main thing at tech companies:</p>\n<blockquote>\n<p>“delivering projects in order to increase shareholder value and make the ~2 layers of management above you happy”</p>\n</blockquote>\n<p>Lots of software engineers really hate that this is the most important thing. I wrote about this at length in <a href=\"/a-little-bit-cynical\"><em>Software engineers should be a little bit cynical</em></a> and <a href=\"/knowing-how-to-drive-the-car\"><em>You have to know how to drive the car</em></a>. If you don’t like this goal at all, it’s going to be tough to spend time thinking about how you can achieve it.</p>\n<p>In fact, I think <strong>it’s actually more important to think about the “main thing” if you hate it</strong>. This is why I’m suspicious of “do what you love” advice. If you love performance engineering but your company doesn’t, I think you’re better off doing it in your spare time and creating shareholder value at work, instead of trying to do as much performance engineering at work as you can.</p>\n<p>Half-assing creating shareholder value a few hours a day (and doing performance engineering the rest of the time) is more valuable than locking in to the wrong “main thing” for ten hours a day. In my experience, it’s also likely more burnout-resistant, since there’s no faster path to burnout than working really hard on something that isn’t valued.</p>\n<h3>Caution: the “main thing” can rapidly change</h3>\n<p>In 2015, being easy to work with was the most important thing in many tech companies. If you were a pleasant colleague, you had to be <em>really</em> bad at other aspects of the job to face serious professional consequences. On the other hand, if you were abrasive and hard to work with, it didn’t really matter how technically competent you were. Many engineers made successful careers by maximizing pleasantness: attending and hosting work social events, making friendly connections in different teams, and in general becoming a known engineer in the company.</p>\n<p>In 2026, it’s still important to be pleasant. But now that tech companies are <a href=\"/good-times-are-over\">tightening their belts</a> and feeling more pressure to ship, the <em>most</em> important thing has shifted to being capable of <a href=\"/how-to-ship\">delivering projects</a>. If you’re able to do that, it can go a long way towards redeeming a difficult personality. Like love, shipping <a href=\"https://www.biblegateway.com/passage/?search=Proverbs%2010%3A11-13&#x26;version=NKJV\">covers all sins</a>. This transition has been a bumpy ride for many software engineers.</p>\n<p>A lot of very pleasant “known engineers” have been laid off in the last three years. I suppose the lesson here is something like this: <strong>even if you’re doing great and are well-adapted to your niche, the environment can change and screw you over anyway</strong>. What can you do about it? If you’ve spent a good chunk of your career developing one set of skills, you can’t instantly transfer all that experience to a different set of skills when the environment changes. Maybe the underlying lesson is more like this: <strong>instead of over-specializing to a single niche, hedge your bets by being pretty good at multiple things</strong>.</p>\n<h3>Final thoughts</h3>\n<p>The lesson here is that <strong>you should spend a lot of time and effort trying to figure out what to focus on</strong>. In the extreme case, even spending half of your time doing this is worthwhile, if it puts you on the right track and you’d otherwise be neglecting the main thing.</p>\n<p>This can seem pretty unintuitive. It feels safer and more productive to be doing <em>something</em>. But if you can force yourself to focus on the meta-question of what you ought to be doing - even if you don’t like the answer - you’ll be in a better position to achieve your goals.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>I write about this at length in <a href=\"/how-to-ship\"><em>How I ship projects at large tech companies</em></a>.</p>\n<a class=\"footnote-backref\" href=\"#fnref-1\">↩</a>\n</li>\n<li id=\"fn-2\">\n<p>I leave filling out what those are as an exercise to the reader.</p>\n<a class=\"footnote-backref\" href=\"#fnref-2\">↩</a>\n</li>\n<li id=\"fn-3\">\n<p>Or some people just get lucky! But that’s rarer than you might think. Getting the main thing right often looks like “constantly getting lucky” from the outside.</p>\n<a class=\"footnote-backref\" href=\"#fnref-3\">↩</a>\n</li>\n</ol>\n</div>",
      "content": "<p>When you’re running a project in a tech company, understanding that your main job is to <strong>ship the project</strong> goes a surprisingly long way. So many engineers spend their time on peripheral questions (like the choice of technology X or Y) when core questions about shipping the product (for instance, how all the critical paths will actually work) are still unanswered<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"#fn-1\">1</a></sup>.</p>\n<p>If you’re able to reliably ship projects, you can get away with being slightly abrasive, or not filling out your Jira tickets correctly, or any number of other small faults that would cause other engineers to be punished.</p>\n<p>You could see this as a special case of the <a href=\"https://en.wikipedia.org/wiki/Pareto_principle\">Pareto principle</a>: the idea that 80% of consequences often come from 20% of causes. But I think in many contexts it’s even more extreme, closer to 90/10 or even 99/1. <strong>If you get the “main thing” right, you can get away with a lot of mistakes.</strong></p>\n<p>This principle holds in many other areas. When saving money, it doesn’t matter if you save a few dollars by hunting for deals if you then buy a car or house that’s on the edge of your budget. If you’re writing, clearly expressing your point will make up for awkward grammar or other mistakes, but even beautiful prose is bad writing if it doesn’t say what you mean. If you’re trying to get fit, consistency and avoiding injury is far more important than finding the most efficient program or the best gear. And so on.</p>\n<h3>Identifying the “main thing”</h3>\n<p><strong>How do you identify the main thing?</strong> This is a pretty deep question. I have written <em>extensively</em> about this when it comes to working in large tech companies: you can read <a href=\"/where-the-money-comes-from\"><em>Knowing where your engineer salary comes from</em></a>, or browse my posts tagged <a href=\"/tags/tech%20companies\">“tech companies”</a>. In und",
      "published": "2026-02-05T00:00:00+00:00",
      "source": "seangoedecke.com",
      "source_url": "https://seangoedecke.com",
      "word_count": 8876
    },
    {
      "title": "Pluralistic: Justin Key's \"The Hospital at the End Of the World\" (04 Feb 2026)",
      "link": "https://pluralistic.net/2026/02/04/slice-bees/",
      "summary": "Today's links Justin Key's \"The Hospital at the End Of the World\": A biopunk medical thriller from a major new talent. Hey look at this: Delights to delectate. Object permanence: Coconut volunteers; Astro Noise; Rich old men behind \"Millennials Rising\"; Stop the \"Stop the Steal\" steal; \"Chasing Shadows.\" Upcoming appearances: Where to find me. Recent appearances: Where I've been. Latest books: You keep readin' em, I'll keep writin' 'em. Upcoming books: Like I said, I'll keep writin' 'em. Colophon: All the rest. Justin Key's \"The Hospital at the End Of the World\" (permalink) Justin C. Key is one of the most exciting new science fiction writers of this decade and today, Harpercollins publishes his debut novel, The Hospital at the End of the World: https://www.harpercollins.com/products/the-hospital-at-the-end-of-the-world-justin-c-key?variant=43822999928866 I've followed Key's work for more than a decade, ever since I met him as a student while teaching at the Clarion West writers' workshop in Seattle. At the time, Key impressed me &#8211; a standout writer in a year full of standouts &#8211; and I wasn't surprised in the least when Harpercollins published a collection of his afrofuturist/Black horror stories, The World Wasn't Ready For You, in 2023: https://pluralistic.net/2023/09/19/justin-c-key/#clarion-west-2015 This is virtually unheard of. Major genre publishers generally don't publish short story collections at all, let alone short story collections by writers who haven't already established themselves as novelists. The exceptions are rare as hell, and they're names to conjure with: Ted Chiang, say, or Kelly Link: https://pluralistic.net/2024/02/13/the-kissing-song/#wrack-and-roll But anyone who read World Wasn't Ready immediately understood why Key's work qualified him for an exception to this iron law of publishing. Key is an MD and a practicing psychiatrist, and he combines keen insights into personal relations and human frailty with a wild imagination, deep compassion, and enviable prose chops. Hospital at the End of the World is Key's first novel, and it's terrific. Set in a not-so-distant future in which an AI-driven health monopolist called The Shepherd Organization controls much of the lives of everyday Americans, Hospital follows Pok, a young New Yorker who dreams of becoming an MD. Pok's father is also a doctor, famous for his empathic, human-centric methods and his scientific theories about the role that \"essence\" (a psychospiritual connection between doctors and patients) plays in clinical settings. The story opens with Pok hotly anticipating an acceptance letter from The Shepherd Organization, and the beginning of his new life as a medical student. But when word arrives, Pok learns that he has been rejected from every medical school in the TSO orbit. In desperate confusion, he works with shadowy hackers in a bid to learn why his impeccable application and his top grades resulted in this total rejection. That's when he learns that someone had sabotaged his application and falsified his grades, and, not long thereafter, he learns that the saboteur was his father. To make things worse, Pok's father has fallen grievously ill &#8211; so ill, in fact, that he ends up in a Shepherd Organization hospital, despite his deep enmity for TSO and its AI-driven practice of medicine. Pok doesn't accompany his father, though &#8211; he has secured a chance to sit a make-up exam in a desperate bid to get into med school. By the time he is finished with his exam, though, he learns that his father has died, and all that is left of him is an AI-powered chatbot that is delivered to Pok's apartment along with a warning to flee, because he is in terrible danger from the Shepherd Organization. Thus begins Pok's tale as he goes underground in a ubiquitous AI surveillance dystopia, seeking sanctuary in New Orleans, hoping to make it to the Hippocrates, the last holdout from America's AI-based medicine and surveillance dystopia. Pok's father learned to practice medicine at Hippocrates, and had urged Pok to study there, even securing a full-ride scholarship for him. But Pok had no interest in the mystical, squishy, sentimental ethos of the Hippocrates, and had been determined to practice the Shepherd Organization's rigorous, cold, data-driven form of medicine. Now, Pok has no choice. Hitchhiking, hopping freight cars, falling into company with other fugitives, Pok makes his way to New Orleans, a city guarded by tall towers that radiate energy that dampens both the punishing weather events that would otherwise drown the city and the data signals by which the Shepherd Organization tracks and controls the American people. This is the book's second act, a medical technothriller that sees Pok as an untrusted outsider in the freshman class at Hippocrates med school, amidst a strange and alarming plague that has sickened the other refugees from TSO America who have taken up residence in New Orleans. Pok has to navigate factions within the med school and in New Orleans society, even as he throws himself into the meat grinder of med school and unravels the secrets of his father and his own birth. What follows is a masterful and suspenseful work of science fiction informed by Key's own medical training and his keen sense of the human psyche. It's one part smart whodunnit, one part heist thriller, and one part revolutionary epic, and at its core is a profound series of provocations and thought experiments about the role that deep human connection and empathy play in medical care. It's a well-structured, well-paced sf novel that probes big, urgent contemporary themes while still engrossing the reader in the intimate human relations of its principals. A wonderful debut novel from a major new writer.` Hey look at this (permalink) Ken MacLeod: Imagined Futures https://plutopia.io/ken-macleod-imagined-futures/ Elbows Up: How Canada Can Disenshittify Its Tech, Reclaim Its Sovereignty, and Launch a New Tech Sector Into a Stable Orbit https://archive.org/details/disenshittification-nation HOPE IS NOW A 501(C)(3) NON-PROFIT ORGANIZATION https://2600.com/content/hope-now-501c3-non-profit-organization Department of Justice appeals Google search monopoly ruling https://www.theverge.com/tech/873438/google-antitrust-case-doj-states-appeal List of Kennedy Center cancellations during the Trump administration https://en.wikipedia.org/wiki/List_of_Kennedy_Center_cancellations_during_the_Trump_administration (h/t Amanda Marcotte) Object permanence (permalink) #20yrsago AOL/Yahoo: our email tax will make the net as good as the post office! https://www.nytimes.com/2006/02/05/technology/postage-is-due-for-companies-sending-email.html #20yrsago Volunteers ferry 15k coconuts every day to Indian temple http://news.bbc.co.uk/2/hi/south_asia/4677320.stm #15yrsago Wikileaks ACTA cables confirm it was a screwjob for the global poor https://arstechnica.com/tech-policy/2011/02/secret-us-cables-reveal-acta-was-far-too-secret/ #10yrsago Laura Poitras’s Astro Noise: indispensable book and gallery show about mass surveillance https://www.wired.com/2016/02/snowdens-chronicler-reveals-her-own-life-under-surveillance/ #10yrsago How to prepare to join the Internet of the dead https://archive.org/details/Online_No_One_Knows_Youre_Dead #10yrsago Who funds the “Millennials Rising” Super PAC? Rich old men. https://web.archive.org/web/20160204223020/https://theintercept.com/2016/02/04/millennials-rising-super-pac-is-95-funded-by-old-men/ #10yrsago They promised us a debate over TPP, then they signed it without any debate https://www.techdirt.com/2016/02/03/countries-sign-tpp-whatever-happened-to-debate-we-were-promised-before-signing/ #5yrsago Stop the \"Stop the Steal\" steal https://pluralistic.net/2021/02/04/vote-machine-tankies/#ess #5yrsago Organic fascism https://pluralistic.net/2021/02/04/vote-machine-tankies/#pastel-q #5yrsago Ron Deibert's \"Chasing Shadows\" https://pluralistic.net/2025/02/04/citizen-lab/#nso-group Upcoming appearances (permalink) Salt Lake City: Enshittification at the Utah Museum of Fine Arts (Tanner Humanities Center), Feb 18 https://tanner.utah.edu/center-events/cory-doctorow/ Montreal (remote): Fedimtl, Feb 24 https://fedimtl.ca/ Victoria: 28th Annual Victoria International Privacy &#38; Security Summit, Mar 3-5 https://www.rebootcommunications.com/event/vipss2026/ Berkeley: Bioneers keynote, Mar 27 https://conference.bioneers.org/ Berlin: Re:publica, May 18-20 https://re-publica.com/de/news/rp26-sprecher-cory-doctorow Berlin: Enshittification at Otherland Books, May 19 https://www.otherland-berlin.de/de/event-details/cory-doctorow.html Hay-on-Wye: HowTheLightGetsIn, May 22-25 https://howthelightgetsin.org/festivals/hay/big-ideas-2 Recent appearances (permalink) Why Everything Got Worse and What to Do About It (Jordan Harbinger) https://www.jordanharbinger.com/cory-doctorow-why-everything-got-worse-and-what-to-do-about-it/ How the Internet Got Worse (Masters in Business) https://www.youtube.com/watch?v=auXlkuVhxMo Enshittification (Jon Favreau/Offline): https://crooked.com/podcast/the-enshittification-of-the-internet-with-cory-doctorow/ Why Big Tech is a Trap for Independent Creators (Stripper News) https://www.youtube.com/watch?v=nmYDyz8AMZ0 Enshittification (Creative Nonfiction podcast) https://brendanomeara.com/episode-507-enshittification-author-cory-doctorow-believes-in-a-new-good-internet/ Latest books (permalink) \"Canny Valley\": A limited edition collection of the collages I create for Pluralistic, self-published, September 2025 \"Enshittification: Why Everything Suddenly Got Worse and What to Do About It,\" Farrar, Straus, Giroux, October 7 2025 https://us.macmillan.com/books/9780374619329/enshittification/ \"Picks and Shovels\": a sequel to \"Red Team Blues,\" about the heroic era of the PC, Tor Books (US), Head of Zeus (UK), February 2025 (https://us.macmillan.com/books/9781250865908/picksandshovels). \"The Bezzle\": a sequel to \"Red Team Blues,\" about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (thebezzle.org). \"The Lost Cause:\" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (http://lost-cause.org). \"The Internet Con\": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (http://seizethemeansofcomputation.org). Signed copies at Book Soup (https://www.booksoup.com/book/9781804291245). \"Red Team Blues\": \"A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before.\" Tor Books http://redteamblues.com. \"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin\", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 https://chokepointcapitalism.com Upcoming books (permalink) \"Unauthorized Bread\": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026 \"Enshittification, Why Everything Suddenly Got Worse and What to Do About It\" (the graphic novel), Firstsecond, 2026 \"The Memex Method,\" Farrar, Straus, Giroux, 2026 \"The Reverse-Centaur's Guide to AI,\" a short book about being a better AI critic, Farrar, Straus and Giroux, June 2026 Colophon (permalink) Today's top sources: Currently writing: \"The Post-American Internet,\" a sequel to \"Enshittification,\" about the better world the rest of us get to have now that Trump has torched America (1011 words today, 21655 total) \"The Reverse Centaur's Guide to AI,\" a short book for Farrar, Straus and Giroux about being an effective AI critic. LEGAL REVIEW AND COPYEDIT COMPLETE. \"The Post-American Internet,\" a short book about internet policy in the age of Trumpism. PLANNING. A Little Brother short story about DIY insulin PLANNING This work &#8211; excluding any serialized fiction &#8211; is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net. https://creativecommons.org/licenses/by/4.0/ Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution. How to get Pluralistic: Blog (no ads, tracking, or data-collection): Pluralistic.net Newsletter (no ads, tracking, or data-collection): https://pluralistic.net/plura-list Mastodon (no ads, tracking, or data-collection): https://mamot.fr/@pluralistic Medium (no ads, paywalled): https://doctorow.medium.com/ Twitter (mass-scale, unrestricted, third-party surveillance and advertising): https://twitter.com/doctorow Tumblr (mass-scale, unrestricted, third-party surveillance and advertising): https://mostlysignssomeportents.tumblr.com/tagged/pluralistic \"When life gives you SARS, you make sarsaparilla\" -Joey \"Accordion Guy\" DeVilla READ CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies (\"BOGUS AGREEMENTS\") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer. ISSN: 3066-764X",
      "content": "<p><!--\nTags:\nnola, louisiana, afrofuturism, medicine, health, science fiction, new orleans, books, reviews, ai, gift guide, justin key, justin c key,\n\nSummary:\nJustin Key's \"The Hospital at the End Of the World\"; Hey look at this; Upcoming appearances; Recent appearances; Latest books; Upcoming books\n\nURL:\nhttps://pluralistic.net/2026/02/04/slice-bees/\n\nTitle:\nPluralistic: Justin Key's \"The Hospital at the End Of the World\" (04 Feb 2026) slice-bees\n\nBullet:\n&#x200d;&#x1f9b8;&#x1f3fb;\n\nSeparator:\n->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->\n\nTop Sources:\nNone\n\n--><br />\n<a href=\"https://pluralistic.net/2026/02/04/slice-bees/\"><img class=\"xmasthead_link\" src=\"https://i0.wp.com/craphound.com/images/04Feb2026.jpg?w=840&#038;ssl=1\" /></a></p>\n<h1 class=\"toch1\">Today's links</h1>\n<ul class=\"toc\">\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#hippocrates\">Justin Key's \"The Hospital at the End Of the World\"</a>: A biopunk medical thriller from a major new talent.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#linkdump\">Hey look at this</a>: Delights to delectate.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#retro\">Object permanence</a>: Coconut volunteers; Astro Noise; Rich old men behind \"Millennials Rising\"; Stop the \"Stop the Steal\" steal; \"Chasing Shadows.\"\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#upcoming\">Upcoming appearances</a>: Where to find me.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#recent\">Recent appearances</a>: Where I've been.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#latest\">Latest books</a>: You keep readin' em, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#upcoming-books\">Upcoming books</a>: Like I said, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net",
      "published": "2026-02-04T15:48:33+00:00",
      "source": "pluralistic.net",
      "source_url": "https://pluralistic.net",
      "word_count": 21832
    },
    {
      "title": "Super Bowl LX creates an opportunity for symphonic friendly wagering",
      "link": "https://devblogs.microsoft.com/oldnewthing/20260204-01/?p=112039",
      "summary": "<p>Betting classical music.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-01/?p=112039\">Super Bowl LX creates an opportunity for symphonic friendly wagering</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
      "content": "<p>This upcoming Sunday is Super Bowl LX, the championship game of the top professional <a href=\"https://en.wikipedia.org/wiki/American_football\"> American football</a> league. The Super Bowl thinks that it is so important that it uses Roman numerals.)</p>\n<p>The Super Bowl is the single largest sporting event in the United States. <a href=\"https://devblogs.microsoft.com/oldnewthing/20070202-17/?p=28173\" title=\"Super Bowl Sunday: The day the entire country stops doing anything\"> The entire country grinds to a halt when the game is on</a>. If you aren&#8217;t interested in the game, <a href=\"https://devblogs.microsoft.com/oldnewthing/20150130-01/?p=44783\" title=\"Got errands? Now is the time\"> it&#8217;s a great time to do public photography or run errands</a>.</p>\n<p>Traditionally, the mayors of the home cities of the two teams competing in the game make a friendly wager, with each mayor offering to send the other mayor some local products if their team loses. For example, in 2014, <a href=\"https://www.sbnation.com/nfl/2014/1/27/5351434/super-bowl-2014-seattle-denver-mayor-bet-seahawks-broncos-macklemore\" title=\"Super Bowl 2014: Seattle, Denver mayors agree to bet\"> the mayors of Seattle and Denver wagered local foods and products as well as having to wear clothing inspired by the other team&#8217;s city</a>.</p>\n<p>Sometimes other city organizations get into the friendly wagering spirit. In 2018, <a href=\"https://www.wqxr.org/story/its-philadelphia-orchestra-and-boston-symphony-have-super-bowl-bet/\" title=\"It's On: The Philadelphia Orchestra and the Boston Symphony Have a Super Bowl Bet\"> the Philadelphia Orchestra and Boston Symphony agreed that the losing city&#8217;s conductor would have to wear the winning city&#8217;s jersey at their next rehearsal</a>.</p>\n<p>But certainly we can do better than that.</p>\n<p>The two teams competing in Super Bowl LX are the Seattle Seahawks and the New England Patriots (based near Boston). I think the Seattle Symphony and the Bo",
      "published": "2026-02-04T15:00:01+00:00",
      "source": "devblogs.microsoft.com/oldnewthing",
      "source_url": "https://devblogs.microsoft.com/oldnewthing",
      "word_count": 2554
    },
    {
      "title": "How can I prevent the user from changing the widths of ListView columns?",
      "link": "https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037",
      "summary": "<p>You can ask the header to be non-resizing.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037\">How can I prevent the user from changing the widths of ListView columns?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
      "content": "<p>Suppose you are using a Win32 ListView control in report mode, and you&#8217;ve got all your columns set up perfectly, and you don&#8217;t want the user to resize them. How do you do that?</p>\n<p>There is no ListView style for preventing column resize, but there <i>is</i> a header control style to prevent sizing: <code>HDS_NOSIZING</code>. This style requires Common Controls version 6, but I&#8217;m sure you&#8217;re all using that version already, right?</p>\n<pre>auto hdr = ListView_GetHeader(hwndLV);\nSetWindowLong(hdr, GWL_STYLE,\n              GetWindowLong(hdr, GWL_STYLE) | HDS_NOSIZING);\n</pre>\n<p>Whether the columns can be resized is independent of whether the columns can be rearranged, which you specify by setting the <code>LVS_EX_HEADER­DRAG­DROP</code> ListView extended style.</p>\n<pre>ListView_SetExtendedListViewStyleEx(hwndLV,\n                                    LVS_EX_HEADERDRAGDROP,\n                                    LVS_EX_HEADERDRAGDROP);\n</pre>\n<p>Okay, but what if you&#8217;re stuck in the dark ages with version 5 of the Common Controls? We&#8217;ll look at that next time.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037\">How can I prevent the user from changing the widths of ListView columns?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
      "published": "2026-02-04T15:00:00+00:00",
      "source": "devblogs.microsoft.com/oldnewthing",
      "source_url": "https://devblogs.microsoft.com/oldnewthing",
      "word_count": 1375
    },
    {
      "title": "Sam Altman and the day Nvidia’s meteoric rise came to an end",
      "link": "https://garymarcus.substack.com/p/sam-altman-and-the-day-nvidias-meteoric",
      "summary": "Happy half anniversary, GPT-5",
      "content": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!zDkI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0509b4-a973-4538-850b-b76417f9cc49_1619x898.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"808\" src=\"https://substackcdn.com/image/fetch/$s_!zDkI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0509b4-a973-4538-850b-b76417f9cc49_1619x898.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></d",
      "published": "2026-02-04T20:31:13+00:00",
      "source": "garymarcus.substack.com",
      "source_url": "https://garymarcus.substack.com",
      "word_count": 7197
    },
    {
      "title": "Γ(1/n)",
      "link": "https://www.johndcook.com/blog/2026/02/04/gamma-reciprocal/",
      "summary": "<p>If n is a positive integer, then rounding Γ(1/n) up to the nearest integer gives n. In symbols, We an illustrate this with the following Python code. &#62;&#62;&#62; from scipy.special import gamma &#62;&#62;&#62; from math import ceil &#62;&#62;&#62; for n in range(1, 101): ... assert(ceil(gamma(1/n)) == n) You can find a full proof in [1]. I&#8217;ll [&#8230;]</p>\nThe post <a href=\"https://www.johndcook.com/blog/2026/02/04/gamma-reciprocal/\">Γ(1/n)</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
      "content": "<p>If <em>n</em> is a positive integer, then rounding Γ(1/<em>n</em>) up to the nearest integer gives <em>n</em>. In symbols,</p>\n<p><img alt=\"\\left\\lceil \\Gamma\\left( \\tfrac{1}{n}\\right) \\right\\rceil = n\" class=\"aligncenter\" height=\"36\" src=\"https://www.johndcook.com/gamma_recip.svg\" style=\"background-color: white;\" width=\"91\" /></p>\n<p>We an illustrate this with the following Python code.</p>\n<pre>&gt;&gt;&gt; from scipy.special import gamma\n&gt;&gt;&gt; from math import ceil\n&gt;&gt;&gt; for n in range(1, 101):\n    ... assert(ceil(gamma(1/n)) == n)\n</pre>\n<p>You can find a full proof in [1]. I&#8217;ll give a partial proof that may be more informative than the full proof.</p>\n<p>The asymptotic expansion of the gamma function near zero is</p>\n<p><img alt=\"\\Gamma(z) = \\frac{1}{z} - \\gamma + {\\cal O}(z^2)\" class=\"aligncenter\" height=\"40\" src=\"https://www.johndcook.com/gamma_recip2.svg\" style=\"background-color: white;\" width=\"172\" /></p>\n<p>where γ is the Euler-Mascheroni constant.</p>\n<p>So when we set <em>z</em> = 1/<em>n</em> we find Γ(1/<em>n</em>) ≈ <em>n</em> − γ + <em>O</em>(1/<em>n</em>²). Since 0 &lt; γ &lt; 1, the theorem above is true for sufficiently large <em>n</em>. And it turns out &#8220;sufficiently large&#8221; can be replaced with <em>n</em> ≥ 1.</p>\n<p>[1] Gamma at reciprocals of integers: 12225. American Mathematical Monthly. October 2022. pp 789–790.</p>The post <a href=\"https://www.johndcook.com/blog/2026/02/04/gamma-reciprocal/\">Γ(1/n)</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
      "published": "2026-02-05T03:42:12+00:00",
      "source": "johndcook.com",
      "source_url": "https://www.johndcook.com/blog",
      "word_count": 1563
    },
    {
      "title": "Writing an LLM from scratch, part 32b -- Interventions: gradient clipping",
      "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32b-interventions-gradient-clipping",
      "summary": "<p>I'm still working on training the best GPT-2 small sized base model that I can\nwith a number of FLOPs roughly equal to two days on my own machine -- my \"extra credit\"\nexercise after having worked through\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>In the <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">last post</a> I trained\na baseline model -- one with the same architecture and almost the same training code as in\nthe minimal training run in the book, just modified to run using DDP on an 8x A100 40 GiB/GPU\nmachine in the cloud.\nThere are a bunch of \"interventions\" I want to try to see if they'll make it better,\nas measured by the loss they get on a test set.  I'll do a post for each intervention,\nand this is the first: gradient clipping.</p>\n<h3 id=\"why\">Why?</h3>\n\n<p>In the training chart for the baseline model, you can see that there are three\nplaces where the loss suddenly spiked up, at around global steps 4,200, 13,000,\nand 23,000:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>There are a number of things that could cause loss spikes like that:</p>\n\n<ul>\n<li>A \"bad batch\" -- that is, one batch, or even one sequence in a batch, was\nmassively different in structure to the others that the model had seen, so it just\nhad much worse loss.  That doesn't seem likely in this case, though: the numbers\non the chart are averages over 617 global steps each, and it would take a truly pathological\nsequence to move the needle that much.</li>\n<li>Something weird in the optimiser.  That's not something I understand well, but\naccording to the various LLMs I'm working with, it's a possibility.</li>\n<li>Exploding gradients.  This is my working hypothesis, and so in this post I'll\ntry out gradient clipping, the normal solution to that problem.</li>\n</ul>\n\n<h3 id=\"what\">What?</h3>\n\n<p>Exploding gradients are common in RNNs, and also happen in LLMs like this one.  I spent a bit\nof time reading around to find out how they happen, and the ah-ha moment came when\nI came across <a href=\"https://medium.com/data-science/what-is-gradient-clipping-b8e815cdfb48\">this post from Wanshun Wong</a>.\nNot only is the post itself a good intro in terms of how it affects RNNs, but in the\n\"further reading\" at the end, there's some gold:</p>\n\n<blockquote>\n  <p>Chapter 10.11 of [1] has a good overview of how gradient clipping works.</p>\n  \n  <p>...</p>\n  \n  <p>References</p>\n  \n  <ol>\n  <li>I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning (2016), MIT Press.</li>\n  </ol>\n</blockquote>\n\n<p>Now, I bought a copy of \"<a href=\"https://www.deeplearningbook.org/\">Deep Learning</a>\" at the\nsame time as I bought Raschka's book, but I'd only glanced through it.  Now was the\ntime to get it down from the shelf -- and, indeed, section 10.11.1 is all about clipping\nto handle exploding gradients.  I'll put the explanation of how they happen into my\nown words, to see if I can clarify things (at least in my mind).</p>\n\n<p>Normally, when we learn about gradient descent, it's illustrated with nice smooth\nloss charts like this imaginary one for a single-parameter model:</p>\n\n<p><img alt=\"A simple loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/simple-loss-chart.png\" title=\"A simple loss chart\" /></p>\n\n<p>We're told that we might start at point A.  The gradient is quite high and negative,\nso we multiply it by our learning rate and subtract it from our parameter.  That\ngets us to point B.  This time around, the gradient is smaller as the curve is flatter\nthere, so when we do the same -- multiply by LR and subtract -- we take a smaller step, and\nwind up at C.  Rinse and repeat and we'll wind up near the minimum.</p>\n\n<p>The problem is, what if the loss curve actually looks like this:</p>\n\n<p><img alt=\"A more complex loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/more-complex-loss-chart.png\" title=\"A more complex loss chart\" /></p>\n\n<p>...?</p>\n\n<p>We start at A, with a small gradient, move a little to the right, and now we're at\nB halfway down a cliff!  The gradient is massive, and when we subtract it, even scaled\nby the learning rate, we can zoom off somewhere to the right -- maybe not even on the\nchart.  Indeed, you can imagine a cliff that is so steep that\nit would have vertical portions -- negative infinite gradients in this case -- and no matter what your learning\nrate is, you'll wind up with an infinite parameter update and everything will break.\nIt's hard to see how a model can continue training in a case like that.</p>\n\n<p>Now, what can cause steep cliffs like that?  The book says \"strongly nonlinear functions,\nsuch as those computed by a recurrent neural net over many time steps\".</p>\n\n<p>If you know about RNNs (I <a href=\"/2025/10/revisiting-karpathy-unreasonable-effectiveness-rnns\">wrote about them</a>\nif you'd like a summary), you'll remember that a single RNN might be quite\nshallow -- maybe three or four layers -- but when you're doing backpropagation,\nyou run a number of inputs through, one after the other, work out the overall loss, and then \"unroll\" it to\nsomething similar to a \"vanilla\" neural net to do the backward pass.  To put that in\nconcrete terms, a 3-layer neural network trained with a 100-element sequence would\nunroll to a 300-layer deep network.  Every one of those layers has several operations, including\n(in the implementation I was looking at in my post above), a <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow></math>.  It's not surprising that there are cliffs in the loss landscape -- it's\nmore surprising that there are any smooth bits!</p>\n\n<p>Now in LLMs, we don't have that unrolling through time -- but our network is deep enough\nas it is.  For the GPT-2 small model, disregarding the embeddings and the final output head, we have 12 Transformer layers,\neach of which is multiple matrix multiplications for attention, then a softmax, then another layer, and\nthen a feed-forward... mapping precisely to the equivalent vanilla NN is hard, but I think\nyou can treat each one as at least four layers, so we've got 48. And there are GELUs and logs and\nexps <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup> dotted around, so again -- we should expect cliffs.</p>\n\n<p>So if sometimes we'll get crazy gradients, what can we do about them?  We clip them.</p>\n\n<h3 id=\"how\">How?</h3>\n\n<p>Clipping gradients simply means that if they get larger than a particular number -- <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>,\nwhich we define -- we reduce them to that number.  In other words, we have a cap on how\nbig they can get.</p>\n\n<p>\"Deep Learning\" (\"DL\" from now on) suggests two ways to do it.  Remember that while in the\nexample above, we only had one parameter -- on the X axis -- for the GPT-2 small\nLLM we're training, we have 163 million of them.  So the gradients, instead of\nbeing one number, will be a 163M-long vector, one per parameter.  The two ways to clip are:</p>\n\n<ul>\n<li>We clip element-wise.  If any one of the gradients in the vector is larger than <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>,\nwe reduce it to <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>.</li>\n<li>We clip based on the norm: the length of the gradient vector in -- in our\ncase -- 163M-dimensional space.  That sounds harder than it is -- it's really\njust an extension of the Pythagorean equation that <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&#x0002b;</mo><msup><mi>b</mi><mn>2</mn></msup><mo>&#x0003d;</mo><msup><mi>c</mi><mn>2</mn></msup></mrow></math> to multiple\ndimensions.  If you want to work out the length of a vector <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo stretchy=\"false\">&#x00028;</mo><mi>a</mi><mo>&#x0002c;</mo><mi>b</mi><mo stretchy=\"false\">&#x00029;</mo></mrow></math> then you\ncan use Pythagoras to work out <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>c</mi><mo>&#x0003d;</mo><msqrt><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&#x0002b;</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></msqrt></mrow></math>, and that generalises\nto any number of dimensions.  So for our model we'd just square all 163M\nelements of the vector, sum those, and take the square root of the result, and that's the norm. <sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"#fn-2\">2</a></sup>\nIf the norm is greater than <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, we just divide every element of the gradient vector by the norm\nand multiply the result by <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, to produce\na new gradient vector whose norm is <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>.</li>\n</ul>\n\n<p>The second feels more elegant -- we're scaling all of the elements of the gradient\nvector by the same amount, so it still points in the same direction.  Interestingly, though,\nDL says that the two methods \"work similarly\", which I'll read as \"are pretty much\nthe same in practice\".</p>\n\n<p>DL then goes on to say how infinite or not-a-number gradients should be handled.\nWith the first way, clearly doing it naively would set every element in the gradient\nvector to <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, which would make the total size (norm) of the update very large.  With the\nsecond, it be even worse -- we'd still wind up with completely junk gradients, because\nthe norm would be infinite, and in Python <code>math.inf / math.inf</code> is <code>math.nan</code>, so\nwe'd be applying gradients with NaNs in them at best.  That would be likely to\nknock our model into unrecoverable territory, as any parameter that had that applied\nto it would be NaN forever.</p>\n\n<p>Their suggested solution is that if you get garbage gradients like that, you can take\na random step -- that is, create a new gradient to apply that has the norm <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>\nbut just points in a random direction. The idea is that this will move you away from\nthe cliff-ridden part of the loss landscape where you've found yourself (more about that later), and things will\ncontinue nicely.</p>\n\n<p>So, anyway, how to do this in practice?</p>\n\n<p>PyTorch has a function, <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a>,\nand that's what's referenced in almost every bit of writing I've found about how\nto clip gradients.  So I decided to use that, assuming it would do what was described\nin DL's second option and that it would do the random updates they suggest for non-finite\ngradients.  (I was half-correct -- see later.)</p>\n\n<p>As to how to use it -- if we had a normal training loop, where we were just using a normal optimiser, we would go from:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">y_logits</span><span class=\"p\">,</span> <span class=\"n\">target_y_ids</span><span class=\"p\">)</span>\n    <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...to something like</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">y_logits</span><span class=\"p\">,</span> <span class=\"n\">target_y_ids</span><span class=\"p\">)</span>\n    <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...where <code>clipping_max_norm</code> is the max value <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math> from above.</p>\n\n<p>However, for our training code using Automatic Mixed Precision (AMP),\nit's a little more complicated -- but luckily, the AMP explainer we've been using\n<a href=\"https://docs.pytorch.org/docs/stable/notes/amp_examples.html#gradient-clipping\">has a section explaining what to do</a>.</p>\n\n<p>Right now we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Per that explainer, we need to move to this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">unscale_</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>That looks a bit weird; we're \"unscaling\" the gradients,\nthen clipping them, then using the scaler to step the\noptimiser.  You'd think that you'd need to \"re-scale\" the scaler after clipping the gradients --\nto get back to where you started from before the optimiser step.\nFrom the help page I gather it keeps track of whether or not the gradients it has right now are\ncurrently scaled and handles them appropriately based on that state in <code>scaler.step</code>.</p>\n\n<p>Anyway, given that we know what the code looks like now, we need to implement it\nin a way that can be easily switched on for this experiment (and potentially in\nthe future), but which also allows us to not use it if we don't want to.</p>\n\n<p>The best way with our setup is to make it a training option, so we can do it this way:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">clipping_max_norm</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">unscale_</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...with <code>clipping_max_norm</code> extracted from the <code>train.json</code> file where we call it in\n<code>load_datasets_and_train</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train</span><span class=\"p\">(</span>\n        <span class=\"n\">run_dir</span><span class=\"p\">,</span>\n        <span class=\"n\">ddp_model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n        <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;clipping_max_norm&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">train_ds</span><span class=\"p\">,</span>\n        <span class=\"n\">global_step</span><span class=\"p\">,</span> <span class=\"n\">best_loss</span><span class=\"p\">,</span>\n        <span class=\"n\">checkpoint_interval</span><span class=\"o\">=</span><span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;checkpoint_interval&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">do_checkpoints</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and we can just pass in <code>None</code> for it in our <code>check_batch_size_works</code> function that\nwe use to find the maximum micro-batch size for our current hardware, as all we're\ntesting for there is memory usage -- we don't care if we're doing good updates.</p>\n\n<p>Here's <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/4abebdd2b81dbc7d0a3113fc1c5daf943361357e\">the code delta for that</a>,\nplus a <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/202a79a1d6ede40aab7a8adb19bc0b17bb5c9f5f\">bugfix</a> to allow\nfor <code>train.json</code> files without a <code>clipping_max_norm</code> in them.</p>\n\n<p>But it would also be useful to be able to track when it \"fired\" -- that is, when we\nhad to clip our gradients.  Then we can see two things:</p>\n\n<ol>\n<li>Whether we actually did wind up clipping them and fixing those loss spikes</li>\n<li>Whether we were clipping at other times -- we don't want to be doing it unnecessarily.</li>\n</ol>\n\n<p>Now, the docs for <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\"><code>clip_grad_norm_</code></a>\nsay that it returns the \"[t]otal norm of the parameter gradients (viewed as a single vector)\".\nIt doesn't say whether that's before or after the clipping, but given that the return value would\nalways be <code>clipping_max_norm</code> if it was after, I'm going to guess that it returns\nthe pre-clipping norm (ChatGPT agrees).</p>\n\n<p>So we can chart that; changes in these diffs: <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/6d5d0cd96b553b420cbf2c8d6c2d2af0f5a36582\">1</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/ac0ef28ac8879f3a6351da166344d918d38cfc76\">2</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/4d476239d9ed3aa1e0864536ea4c61632898b3bd\">3</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/326ccd2ef490aea57371394a949969a687c904ad\">4</a>.</p>\n\n<h3 id=\"how-much\">How much?</h3>\n\n<p>So we now have code to clip gradients to a given norm size and to chart the gradient\nnorms so that we know what they were before clipping.  The question is, what\nshould that clipping norm be?  Some googling around suggested that there was no standard way\nof saying \"for such-and-such a kind of model, gradients should be clipped at around\n<math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>x</mi></mrow></math>\".  For example, on <a href=\"https://www.reddit.com/r/MachineLearning/comments/kqgne3/choosing_gradient_norm_clip_value_d/\">this Reddit thread</a>,\n<code>GLVic</code> says \"Common values are 1, 3, 5, 8, 10\", and likewise sample code in\n<a href=\"https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem\">this tutorial</a>.\nhas 1, as does <a href=\"https://www.geeksforgeeks.org/deep-learning/understanding-gradient-clipping/\">this one</a>.</p>\n\n<p>So my initial thought was, let's just use 1.  But then I wondered, what actually are\nthe gradient norms that we're getting in normal training?  I decided to run a local short\ntrain on 3m tokens (a thousandth of the full training set, taking just less than four minutes) with very frequent checkpointing, and\ngradient clipping set to 1, and\nsee what happened.</p>\n\n<p><img alt=\"Small local train, gradient clipping at 1\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/gradient-clipping-1-locally.png\" title=\"Small local train, gradient clipping at 1\" /></p>\n\n<p>You can see that the \"grad max\" line is almost always above the \"grad clip\" -- we're\nalmost always clipping.   This doesn't sound right.  It looked like the range of the grad max\nwas generally beween 1.1 and a little above 3, so I set the <code>clipping_max_norm</code> to 3.5 and\ndid another train:</p>\n\n<p><img alt=\"Small local train, gradient clipping at 3.5\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/gradient-clipping-3.5-locally.png\" title=\"Small local train, gradient clipping at 3.5\" /></p>\n\n<p>Our loss is about the same, but we're no longer clipping -- and that's what we want;\nthere was no evidence of exploding gradients for that short run -- just big updates\nnear the start, as you'd expect.</p>\n\n<p>I then ran the same with no gradient clipping at all, and got exactly the same shape\nfor the loss chart as I did with gradient clipping at 3.5, and the same final loss -- that's a good signal that clipping is\nnot affecting the train when we stay inside the limit, which is exactly what we want.</p>\n\n<p>So, it was time to train our model!</p>\n\n<h3 id=\"running-the-train\">Running the train</h3>\n\n<p>I kicked off the train, and after a little while, I looked at the training chart,\nwhich is updated dynamically as the model trains:</p>\n\n<p><img alt=\"First run of cloud train, with missing max gradients\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/cloud-run-missing-gradient-maxes.png\" title=\"First run of cloud train, with missing max gradients\" /></p>\n\n<p>You can see the dotted green lines, both the light one and the dark one -- that is,\nthe \"grad max\" and the \"grad avg\" -- disappear starting just before global step\n4,000, only coming back at about 5,500 -- that is, these were not plotted for\nglobal steps 4,319 and 4,936, even though the loss was.  What was going on?</p>\n\n<p>I took a look at the checkpoint meta file for the first of those to see what the actual numbers\nwere, and saw this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;min_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">3.7176883220672607</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;max_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">5.877607822418213</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;avg_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.3170230991450085</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;max_grad_norms&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">I</span><span class=\"kc\">nf</span><span class=\"err\">i</span><span class=\"kc\">n</span><span class=\"err\">i</span><span class=\"kc\">t</span><span class=\"err\">y</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;avg_grad_norms&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">I</span><span class=\"kc\">nf</span><span class=\"err\">i</span><span class=\"kc\">n</span><span class=\"err\">i</span><span class=\"kc\">t</span><span class=\"err\">y</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;frac_clipped&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.0016207455429497568</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;global_step&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">4319</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;is_best&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>Aha!  The PyPlot code I was using could not handle infinite values, which is entirely\nreasonable.  That was easy enough to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/78a1e794a84c4e01cd7f812755c17ded07f35b93\">fix</a>,\nthough -- I just replaced positive infinity by 1,000,000 and negative infinity by -1,000,000,\nand then (in the interest of getting a proper from-scratch run) kicked everything\noff from the beginning.</p>\n\n<p>That training run completed with this chart:</p>\n\n<p><img alt=\"Second cloud run, showing clipping periods\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/second-cloud-run-showing-clipping.png\" title=\"Second cloud run, showing clipping periods\" /></p>\n\n<p>That's a little hard to read, but if you look closely at the green lines, you\ncan see that there are seven periods where gradients were either very large or\ninfinite.  Weirdly, though, out of the seven, two of them were two checkpoint periods long\n(that is, two periods of 617 global steps).  That felt weird, though of course\nwe're looking at the maximum gradient norm and the average gradient norm -- so\ntwo single infinite/high-gradient steps in successive 617-step periods would lead to that effect.</p>\n\n<p>What was even stranger, though,\nwas that if you look at the training chart for the run with no gradient clipping,\nwe have only three loss spikes rather than seven:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>...though it's also very noticeable that the gradient-clipped run had only two small loss\nspikes, unlike the three larger ones in the unclipped run.</p>\n\n<p>The training loss the gradient-clipped run reported at the end was better, too:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,343.442 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 264,128 tokens/second</span>\n<span class=\"go\">Final train loss: 3.728</span>\n</code></pre>\n</div>\n\n<p>...versus 3.743 at the end of the baseline train.</p>\n\n<p>So it was time to download it, and run the sequence-completion smoke test:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/model.json<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/checkpoints/best/model.safetensors\nEvery<span class=\"w\"> </span>effort<span class=\"w\"> </span>moves<span class=\"w\"> </span>you<span class=\"w\"> </span>further<span class=\"w\"> </span>afield<span class=\"w\"> </span>the<span class=\"w\"> </span>most<span class=\"w\"> </span>to<span class=\"w\"> </span>get<span class=\"w\"> </span>more<span class=\"w\"> </span><span class=\"nb\">time</span><span class=\"w\"> </span>and<span class=\"w\"> </span>space<span class=\"w\"> </span>out<span class=\"w\"> </span>of<span class=\"w\"> </span>your<span class=\"w\"> </span>pocket.<span class=\"w\"> </span>With<span class=\"w\"> </span>more<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>abundance\n</code></pre>\n</div>\n\n<p>Coherent enough!</p>\n\n<p>Next, we evaluate it against our held-back test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/model.json<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/checkpoints/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">1471</span>.81it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:58&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.72it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.678\n</code></pre>\n</div>\n\n<p>So, the loss had gone down -- but only from 3.743 to 3.678, a reduction of 0.065,\nor about 1.7%.</p>\n\n<p>That's not actually all that bad!\nAfter all, in my initial experiments on my local machine, training for a Chinchilla-optimal\nnumber of tokens from FineWeb-Edu (rather than the regular FineWeb I'm using now)\ngot a loss of 4.167 on the same dataset (weirdly worse with the more-curated training set),\nand training for a further Chinchilla-optimal number of tokens only brought that down to\n4.135, for a difference of 0.032, or 0.7%.</p>\n\n<p>It's not strictly comparable due to the different training sets, but speaking <em>very</em>\nloosely, we could say that gradient clipping for this train had more effect than doubling the training time for\nthe other one.  That's pretty nifty.</p>\n\n<p>But the question remained: why those long periods of high gradients, even with gradient\nclipping?  And why were there still loss spikes -- in particular the one just before\nglobal step 12,000, which lasted for two checkpoint periods?</p>\n\n<h3 id=\"chasing-infinity\">Chasing infinity</h3>\n\n<p>Remember that when I started the first run of this train, and got the chart with\nthe missing bits, it was because the logged <code>max_grad_norms</code> and <code>avg_grad_norms</code>\nwere infinite.</p>\n\n<p>What happens when <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a> gets an infinite gradient -- either one that has\nan infinity as one of its components, or one that (due to numerical overflow) winds up\nwith a norm of infinity anyway?  I'd been kind of assuming that it did what the authors\ndescribed in \"Deep Learning\" -- a random update of norm <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math> -- given that the book\nstated pretty confidently that you \"can\" do it but then appeared to consider the topic closed.</p>\n\n<p>But it doesn't!  If you check that link to the docs, you'll see that it has a parameter\n<code>error_if_nonfinite</code>, which is <code>False</code> by default.  If it's set to <code>True</code>, that will\nraise an exception if the norm is positive or negative infinity, or if it's not a number\n-- which catches both the infinite component and the norm overflow cases above.  But if\nit's not set -- and we weren't setting it -- and the norm or the gradients are non-finite, then <code>clip_grad_norm_</code> will essentially\nreturn garbage gradients.  Depending on the exact cause, elements will either be infinities\nof one sign or another, or NaNs.  And if these are added to parameters, then those\nparameters will become garbage too.</p>\n\n<p>Now that leads to the question, given that we know that somewhere in the period\nbetween the checkpoint at global step 4,319 and the previous one at 3,702 there was\nan infinite norm at some point, how on earth did the model manage to continue training\nafter that?  Loss went up at around the same time, but it wasn't completely broken as\nit would have been with NaNs or infinities in its parameters.</p>\n\n<p>Obscurely enough, the answer turned out to be in the <a href=\"https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-gradscaler\">AMP explainer</a>,\nin a comment in one of the bits of example code.  Regarding the <code>GradScaler</code> class we're using:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"c1\"># ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.</span>\n        <span class=\"c1\"># If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,</span>\n        <span class=\"c1\"># otherwise, optimizer.step() is skipped.</span>\n</code></pre>\n</div>\n\n<p>So what was happening was that the scaler -- something we introduced into our code\nto get a speedup by using 16-bit floats instead of 32-bit whenever PyTorch thought\nit would make sense -- was protecting us against infinite and NaN gradients as a\nside-effect.  It was skipping updates that would have polluted our weights with\nbad values from non-finite gradients.</p>\n\n<p>Hmph.</p>\n\n<h3 id=\"grumble\">Grumble</h3>\n\n<p>If the above comes across as a little frustrated, then it's because I am a bit!\nFrom a software engineering viewpoint, this situation really does feel a bit like a\nrather messy part of the API.</p>\n\n<p>There are three things that it's reasonable for a library to do with infinite/NaN\ngradients:</p>\n\n<ol>\n<li>Blindly apply them and expect the developer to sanitise their inputs.</li>\n<li>Raise an error.</li>\n<li>Take some kind of default sane action, like skipping the update.</li>\n</ol>\n\n<p>Now, if we look at that <code>error_if_nonfinite</code>, we can see that the first two of those\ncases are handled there; and the developer can choose which option to follow.\nIt's not where I'd personally put it (the <code>step</code> function on the optimiser seems more\nnatural) and I think I'd probably set the default to <code>True</code> too, but I can also imagine\ngood reasons for it being the way it is -- backward compatibility for one.</p>\n\n<p>But the \"skip non-finite gradients\" being a (not even optional!) behaviour that is\non a class designed for handling mixed-precision training just seems outright bonkers.\nI would be surprised if there weren't people out there who've spent days trying\nto work out why their training runs failed catastrophically when they decided to\nswitch from mixed-precision to \"full fat\" 32-bit floats, not realising that a\nhardly-even-documented feature of the scaler <sup class=\"footnote-ref\" id=\"fnref-3\"><a href=\"#fn-3\">3</a></sup> had been saving them from gradient issues\npreviously.</p>\n\n<p>Anyway, rant over.  What does this all mean?</p>\n\n<h3 id=\"so\">So...?</h3>\n\n<p>There are three ways a gradient can explode:</p>\n\n<ol>\n<li>It can get very large, still be finite, and have a finite norm.</li>\n<li>It can get very large, still be finite, but have an infinite norm (eg. due to numerical overflow)</li>\n<li>It can become infinite -- that is, at least one of the parameters' gradients is infinite (which\nof course means an infinite norm regardless of any numerical stuff).</li>\n</ol>\n\n<p>With both the baseline code and our new code, the <code>GradScaler</code> was saving us from\nthe last two of those, by skipping the optimiser steps with non-finite gradients.</p>\n\n<p>However, the baseline run was not protected against the first kind -- large but finite\ngradients with a finite norm -- while this run was protected.</p>\n\n<p>What I'm almost certain is happening here is that in all of my training runs so\nfar, there have been all three kinds of issues with exploding gradients.  The\n<code>GradScaler</code>, which again, we introduced for faster training, happened to be saving\nus from the infinite gradients/norms.  But we were still being bitten by the finite\nbut excessively large ones.</p>\n\n<p>And that, I think, is why this training run had a positive -- not huge, but certainly worthwhile\n-- effect on the test set loss.</p>\n\n<p>If I had more time, I think I'd do another run, logging all three of those categories\nof error to see how frequent they are, and charting the result.  That might go some way to\nexplaining the final question I had here: why is it that the renowned \"Deep Learning\"\nsuggests a random update to get away from the cliff where you've found yourself,\nwhile we seem to be getting away with just skipping the update, which is much simpler?\nWell, the book was written in 2016, and I guess rather a lot has changed in the last 10 years :-)</p>\n\n<p>My guess is that their solution might have been\na solid default in the age of RNNs, but might not make so much sense with the kind of models\nwe're training these days.</p>\n\n<p>I think I can see a way in which that makes sense.  Think of the illustration of a loss \"cliff\"\nin a one-parameter world that we had at the start of this post:</p>\n\n<p><img alt=\"A more complex loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/more-complex-loss-chart.png\" title=\"A more complex loss chart\" /></p>\n\n<p>If you happen to wind up on that cliff, you're in trouble.</p>\n\n<p>But imagine a two-parameter model -- the line of the loss function becomes a surface.\nJust as in the real world you might be able to walk along the edge at the top of a cliff and\nfind a nice easy slope down next to it, you can imagine that the cliff in the two-parameter\ncase might be less of a problem because you don't need to be lucky enough to jump down it --\nyou can walk around it.</p>\n\n<p>Extrapolating examples like this to higher dimensions is\nrisky, but I think it should hold that the more dimensions you're working with,\nthe less likely it is that a cliff is an issue -- you're more likely to be able to find\na way around it.  I've heard a very similar argument made for why local minima are\nless of an issue with lots of parameters.  It's certainly worth saying that this is\nfar from a mathematical proof, but I think it's a decent grounding for intuition.</p>\n\n<p>Now think about an RNN.  Although you're doing back-propagation through time over\nwhat amounts to a very deep network, there aren't actually all that many parameters,\ncertainly compared to an LLM like this.  Each parameter is involved in the back-propagation\nmultiple times.</p>\n\n<p>So, thinking of it that way, the gradient vector for the RNNs they were dealing with\nwas of much lower dimensionality than the ones we're dealing with, even for this\ntiny model.</p>\n\n<p>They say that the random step \"will typically move away from the numerically unstable\nconfiguration\".  I'm probably playing fast and loose here, but I'll take that as something\nlike: if you wound up on a cliff, you were likely in a very \"cliffy\"\narea of the loss landscape.  \"Teleporting\" randomly to somewhere some distance away\nwas a sensible way to handle that.</p>\n\n<p>In our situation, even if the area is \"cliffy\" in the direction that one particular\nbatch might push us, we have so many extra dimensions that it may well be that it\nwon't be so bad with the next one.  So just skipping the problematic update -- under\nall of those assumptions -- seems a perfectly reasonable way to handle it.</p>\n\n<h3 id=\"validation\">Validation</h3>\n\n<p>All of this, BTW, made me think back to validation loss.  In our previous training runs,\nwhere we were measuring it just before each checkpoint, its spikes were in general correlated\nwith but not identical to spikes in training loss:</p>\n\n<p><img alt=\"Loss in a run with validation\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xh100m80.png\" title=\"Loss in a run with validation\" /></p>\n\n<p>Now, of course, exploding gradients don't have to be related to high training loss --\nthere's enough non-linearity in there that we can treat them as being completely uncorrelated,\nI think.  But you definitely would expect them to have an effect on validation\nloss if applied.  Disregarding the infinite ones (which were being filtered out anyway),\nthe very high ones that we are now clipping would, in the unclipped baseline\ntrain, seem very likely to have caused validation loss spikes.</p>\n\n<p>So: if I hadn't stripped that out, we would likely have been able to see a clear\ndifference in the validation loss line between clipped and unclipped.  That would have\nbeen useful!</p>\n\n<p>I'm not going to re-introduce it, though.  Best to keep the number of code changes\nto a minimum if I'm trying to compare like with like over the course of these intervention\ntests.</p>\n\n<h3 id=\"anyway\">Anyway.</h3>\n\n<p>I think that's enough for gradient clipping.  I may come back and do the experiment\nanother time to see what the relative ratios of the different kinds of problematic\ngradients are.  Are there parts of the train where we get lots of them as a percentage (ie.\nwe're somewhere \"cliffy\" in the loss landscape)?  How many infinite gradient vs infinite norm\nvs big-but-not-infinite instances do we have relative to each other, and to normal\ngradient updates?  What do we see if we have validation loss?  And so on.</p>\n\n<p>But for now: gradient clipping definitely helps, and goes on the positive interventions list!</p>\n\n<p>I'm thinking I'll see what happens with switching off dropout next.  That should at\nleast be a bit easier...</p>\n\n<p>Stay tuned!</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p><a href=\"https://www.youtube.com/watch?v=DdRnMjfVQi0\">Oh my</a>.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-2\">\n<p>Technically the L2 norm -- if you used cubes/cube root it would be L3,\nand likewise for the power of four and L4 and so on.  But the L2 is the\none used for gradient clipping.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-2\" title=\"Jump back to footnote 2 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-3\">\n<p>Shades of <a href=\"https://www.goodreads.com/quotes/40705-but-the-plans-were-on-display-on-display-i-eventually\">Douglas Adams</a>, really:</p>\n\n<p>\"But the plans were on display...\"</p>\n\n<p>\"On display? I eventually had to go down to the cellar to find them.\"</p>\n\n<p>“That’s the display department.\"</p>\n\n<p>“With a flashlight.\"</p>\n\n<p>“Ah, well, the lights had probably gone.\"</p>\n\n<p>“So had the stairs.\"</p>\n\n<p>“But look, you found the notice, didn’t you?\"</p>\n\n<p>“Yes,\" said Arthur, “yes I did. It was on display in the bottom of a locked filing cabinet stuck in a disused lavatory with a sign on the door saying ‘Beware of the Leopard.\"&#160;<a class=\"footnoteBackLink\" href=\"#fnref-3\" title=\"Jump back to footnote 3 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
      "content": "<p>I'm still working on training the best GPT-2 small sized base model that I can\nwith a number of FLOPs roughly equal to two days on my own machine -- my \"extra credit\"\nexercise after having worked through\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>In the <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">last post</a> I trained\na baseline model -- one with the same architecture and almost the same training code as in\nthe minimal training run in the book, just modified to run using DDP on an 8x A100 40 GiB/GPU\nmachine in the cloud.\nThere are a bunch of \"interventions\" I want to try to see if they'll make it better,\nas measured by the loss they get on a test set.  I'll do a post for each intervention,\nand this is the first: gradient clipping.</p>\n<h3 id=\"why\">Why?</h3>\n\n<p>In the training chart for the baseline model, you can see that there are three\nplaces where the loss suddenly spiked up, at around global steps 4,200, 13,000,\nand 23,000:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>There are a number of things that could cause loss spikes like that:</p>\n\n<ul>\n<li>A \"bad batch\" -- that is, one batch, or even one sequence in a batch, was\nmassively different in structure to the others that the model had seen, so it just\nhad much worse loss.  That doesn't seem likely in this case, though: the numbers\non the chart are averages over 617 global steps each, and it would take a truly pathological\nsequence to move the needle that much.</li>\n<li>Something weird in the optimiser.  That's not something I understand well, but\naccording to the various LLMs I'm working with, it's a possibility.</li>\n<li",
      "published": "2026-02-05T01:20:00+00:00",
      "source": "gilesthomas.com",
      "source_url": "https://www.gilesthomas.com/",
      "word_count": 47880
    },
    {
      "title": "Logic for Programmers New Release and Next Steps",
      "link": "https://buttondown.com/hillelwayne/archive/logic-for-programmers-new-release-and-next-steps/",
      "summary": "<p><img alt=\"cover.jpg\" class=\"newsletter-image\" src=\"https://assets.buttondown.email/images/f821145f-d310-403c-88f4-327758a66606.jpg?w=480&amp;fit=max\" /></p>\n<p>It's taken four months, but the next release of <a href=\"https://logicforprogrammers.com\" target=\"_blank\">Logic for Programmers is now available</a>! v0.13 is over 50,000 words, making it both 20% larger than v0.12 and officially the longest thing I have ever written.<sup id=\"fnref:longest\"><a class=\"footnote-ref\" href=\"#fn:longest\">1</a></sup> Full release notes are <a href=\"https://github.com/logicforprogrammers/book-assets/blob/master/CHANGELOG.md\" target=\"_blank\">here</a>, but I'll talk a bit about the biggest changes. </p>\n<p>For one, every chapter has been rewritten. Every single one. They span from <em>relatively</em> minor changes to complete chapter rewrites. After some rough git diffing, I think I deleted about 11,000 words?<sup id=\"fnref:gross-additions\"><a class=\"footnote-ref\" href=\"#fn:gross-additions\">2</a></sup> The biggest change is probably to the Alloy chapter. After many sleepless nights, I realized the right approach wasn't to teach Alloy as a <em>data modeling</em> tool but to teach it as a <em>domain modeling</em> tool. Which technically means the book no longer covers data modeling.</p>\n<p>There's also a lot more connections between the chapters. The introductory math chapter, for example, foreshadows how each bit of math will be used in the future techniques. I also put more emphasis on the general \"themes\" like the expressiveness-guarantees tradeoff (working title). One theme I'm really excited about is compatibility (extremely working title). It turns out that the <a href=\"https://buttondown.com/hillelwayne/archive/the-liskov-substitution-principle-does-more-than/\" target=\"_blank\">Liskov substitution principle</a>/subtyping in general, <a href=\"https://buttondown.com/hillelwayne/archive/refinement-without-specification/\" target=\"_blank\">database migrations</a>, backwards-compatible API changes, and <a href=\"https://hillelwayne.com/post/refinement/\" target=\"_blank\">specification refinement</a> all follow <em>basically</em> the same general principles. I'm calling this \"compatibility\" for now but prolly need a better name.</p>\n<p>Finally, there's just a lot more new topics in the various chapters. <code>Testing</code> properly covers structural and metamorphic properties. <code>Proofs</code> covers proof by induction and proving recursive functions (in an exercise). <code>Logic Programming</code> now finally has a section on answer set programming. You get the picture.</p>\n<h3>Next Steps</h3>\n<p>There's a lot I still want to add to the book: proper data modeling, data structures, type theory, model-based testing, etc. But I've added new material for two year, and if I keep going it will never get done. So with this release, all the content is in!</p>\n<p>Just like all the content was in <a href=\"https://buttondown.com/hillelwayne/archive/five-unusual-raku-features/\" target=\"_blank\">two Novembers ago</a> and <a href=\"https://buttondown.com/hillelwayne/archive/logic-for-programmers-project-update/\" target=\"_blank\">two Januaries ago</a> and <a href=\"https://buttondown.com/hillelwayne/archive/logic-for-programmers-turns-one/\" target=\"_blank\">last July</a>. To make it absolutely 100% for sure that I won't be tempted to add anything else, I passed the whole manuscript over to a copy editor. So if I write more, it won't get edits. That's a pretty good incentive to stop.</p>\n<p>I also need to find a technical reviewer and proofreader. Once all three phases are done then it's \"just\" a matter of fixing the layout and finding a good printer. I don't know what the timeline looks like but I really want to have something I can hold in my hands before the summer.</p>\n<p>(I also need to get notable-people testimonials. Hampered a little in this because I'm trying real hard not to quid-pro-quo, so I'd like to avoid anybody who helped me or is mentioned in the book. And given I tapped most of my network to help me... I've got some ideas though!)</p>\n<p>There's still a lot of work ahead. Even so, for the first time in two years I don't have research to do or sections to write and it feels so crazy. Maybe I'll update my blog again! Maybe I'll run a workshop! Maybe I'll go outside if Chicago ever gets above 6°F! </p>\n<hr />\n<h2>Conference Season</h2>\n<p>After a pretty slow 2025, the 2026 conference season is looking to be pretty busy! Here's where I'm speaking so far:</p>\n<ul>\n<li><a href=\"https://qconlondon.com/\" target=\"_blank\">QCon London</a>, March 16-19</li>\n<li><a href=\"https://craft-conf.com/2026\" target=\"_blank\">Craft Conference</a>, Budapest, June 4-5</li>\n<li><a href=\"https://softwareshould.work/\" target=\"_blank\">Software Should Work</a>, Missouri, July 16-17</li>\n<li><a href=\"https://hfpug.org/\" target=\"_blank\">Houston Functional Programmers</a>, Virtual, December 3</li>\n</ul>\n<p>For the first three I'm giving variations of my talk \"How to find bugs in systems that don't exist\", which I gave last year at <a href=\"https://systemsdistributed.com/\" target=\"_blank\">Systems Distributed</a>. Last one will ideally be a talk based on LfP. </p>\n<div class=\"footnote\">\n<hr />\n<ol>\n<li id=\"fn:longest\">\n<p>The second longest was my 2003 NaNoWriMo. The third longest was <em>Practical TLA+</em>.&#160;<a class=\"footnote-backref\" href=\"#fnref:longest\" title=\"Jump back to footnote 1 in the text\">&#8617;</a></p>\n</li>\n<li id=\"fn:gross-additions\">\n<p>This means I must have written 20,000 words total. For comparison, the v0.1 release was 19,000 words.&#160;<a class=\"footnote-backref\" href=\"#fnref:gross-additions\" title=\"Jump back to footnote 2 in the text\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
      "content": "<p><img alt=\"cover.jpg\" class=\"newsletter-image\" src=\"https://assets.buttondown.email/images/f821145f-d310-403c-88f4-327758a66606.jpg?w=480&amp;fit=max\" /></p>\n<p>It's taken four months, but the next release of <a href=\"https://logicforprogrammers.com\" target=\"_blank\">Logic for Programmers is now available</a>! v0.13 is over 50,000 words, making it both 20% larger than v0.12 and officially the longest thing I have ever written.<sup id=\"fnref:longest\"><a class=\"footnote-ref\" href=\"#fn:longest\">1</a></sup> Full release notes are <a href=\"https://github.com/logicforprogrammers/book-assets/blob/master/CHANGELOG.md\" target=\"_blank\">here</a>, but I'll talk a bit about the biggest changes. </p>\n<p>For one, every chapter has been rewritten. Every single one. They span from <em>relatively</em> minor changes to complete chapter rewrites. After some rough git diffing, I think I deleted about 11,000 words?<sup id=\"fnref:gross-additions\"><a class=\"footnote-ref\" href=\"#fn:gross-additions\">2</a></sup> The biggest change is probably to the Alloy chapter. After many sleepless nights, I realized the right approach wasn't to teach Alloy as a <em>data modeling</em> tool but to teach it as a <em>domain modeling</em> tool. Which technically means the book no longer covers data modeling.</p>\n<p>There's also a lot more connections between the chapters. The introductory math chapter, for example, foreshadows how each bit of math will be used in the future techniques. I also put more emphasis on the general \"themes\" like the expressiveness-guarantees tradeoff (working title). One theme I'm really excited about is compatibility (extremely working title). It turns out that the <a href=\"https://buttondown.com/hillelwayne/archive/the-liskov-substitution-principle-does-more-than/\" target=\"_blank\">Liskov substitution principle</a>/subtyping in general, <a href=\"https://buttondown.com/hillelwayne/archive/refinement-without-specification/\" target=\"_blank\">database migrations</a>, backwards-compatibl",
      "published": "2026-02-04T14:00:00+00:00",
      "source": "buttondown.com/hillelwayne",
      "source_url": "https://buttondown.com/hillelwayne",
      "word_count": 5750
    },
    {
      "title": "What happened to Conner hard drives",
      "link": "https://dfarq.homeip.net/what-happened-to-conner-hard-drives/?utm_source=rss&utm_medium=rss&utm_campaign=what-happened-to-conner-hard-drives",
      "summary": "<p>Conner Peripherals was founded June 17, 1985 by Seagate Technology co-founder Finis Conner, in San Jose. On Sep. 20, 1995, Conner agreed to merge with Seagate in a deal worth $1 billion. The deal closed February 5, 1996. At the</p>\n<p>The post <a href=\"https://dfarq.homeip.net/what-happened-to-conner-hard-drives/\">What happened to Conner hard drives</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
      "content": "<p>Conner Peripherals was founded June 17, 1985 by Seagate Technology co-founder Finis Conner, in San Jose. On Sep. 20, 1995, Conner agreed to merge with Seagate in a deal worth $1 billion. The deal closed February 5, 1996. At the</p>\n<p>The post <a href=\"https://dfarq.homeip.net/what-happened-to-conner-hard-drives/\">What happened to Conner hard drives</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
      "published": "2026-02-05T12:00:49+00:00",
      "source": "dfarq.homeip.net",
      "source_url": "https://dfarq.homeip.net/",
      "word_count": 444
    },
    {
      "title": "Rewriting pycparser with the help of an LLM",
      "link": "https://eli.thegreenplace.net/2026/rewriting-pycparser-with-the-help-of-an-llm/",
      "summary": "<p><a class=\"reference external\" href=\"https://github.com/eliben/pycparser\">pycparser</a> is my most widely used open\nsource project (with ~20M daily downloads from PyPI <a class=\"footnote-reference\" href=\"#footnote-1\" id=\"footnote-reference-1\">[1]</a>). It's a pure-Python\nparser for the C programming language, producing ASTs inspired by <a class=\"reference external\" href=\"https://docs.python.org/3/library/ast.html\">Python's\nown</a>. Until very recently, it's\nbeen using <a class=\"reference external\" href=\"https://www.dabeaz.com/ply/ply.html\">PLY: Python Lex-Yacc</a> for\nthe core parsing.</p>\n<p>In this post, I'll describe how …</p>",
      "content": "<p><a class=\"reference external\" href=\"https://github.com/eliben/pycparser\">pycparser</a> is my most widely used open\nsource project (with ~20M daily downloads from PyPI <a class=\"footnote-reference\" href=\"#footnote-1\" id=\"footnote-reference-1\">[1]</a>). It's a pure-Python\nparser for the C programming language, producing ASTs inspired by <a class=\"reference external\" href=\"https://docs.python.org/3/library/ast.html\">Python's\nown</a>. Until very recently, it's\nbeen using <a class=\"reference external\" href=\"https://www.dabeaz.com/ply/ply.html\">PLY: Python Lex-Yacc</a> for\nthe core parsing.</p>\n<p>In this post, I'll describe how I collaborated with an LLM coding agent (Codex)\nto help me rewrite pycparser to use a hand-written recursive-descent parser and\nremove the dependency on PLY. This has been an interesting experience and the\npost contains lots of information and is therefore quite long; if you're just\ninterested in the final result, check out the latest code of pycparser - the\n<tt class=\"docutils literal\">main</tt> branch already has the new implementation.</p>\n<img alt=\"meme picture saying &quot;can't come to bed because my AI agent produced something slightly wrong&quot;\" class=\"align-center\" src=\"https://eli.thegreenplace.net/images/2026/cantcometobed.png\" />\n<div class=\"section\" id=\"the-issues-with-the-existing-parser-implementation\">\n<h2>The issues with the existing parser implementation</h2>\n<p>While pycparser has been working well overall, there were a number of nagging\nissues that persisted over years.</p>\n<div class=\"section\" id=\"parsing-strategy-yacc-vs-hand-written-recursive-descent\">\n<h3>Parsing strategy: YACC vs. hand-written recursive descent</h3>\n<p>I began working on pycparser in 2008, and back then using a YACC-based approach\nfor parsing a whole language like C seemed like a no-brainer to me. Isn't this\nwhat everyone does when writing a serious parser? Besides, the K&amp;R2 book\nfamously carries the entire grammar of the C99 language in an append",
      "published": "2026-02-04T19:35:00-08:00",
      "source": "eli.thegreenplace.net",
      "source_url": "https://eli.thegreenplace.net/",
      "word_count": 22332
    }
  ]
}