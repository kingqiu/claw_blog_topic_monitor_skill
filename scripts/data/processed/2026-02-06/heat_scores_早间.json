{
  "timestamp": "2026-02-06 早间",
  "total_topics": 12,
  "topics": [
    {
      "canonical_name": "模型性能优化",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Γ(1/n)",
          "link": "https://www.johndcook.com/blog/2026/02/04/gamma-reciprocal/",
          "source": "johndcook.com",
          "summary": "<p>If n is a positive integer, then rounding Γ(1/n) up to the nearest integer gives n. In symbols, We an illustrate this with the following Python code. &#62;&#62;&#62; from scipy.special import gamma &#62;&#62;&#62; from math import ceil &#62;&#62;&#62; for n in range(1, 101): ... assert(ceil(gamma(1/n)) == n) You can find a full proof in [1]. I&#8217;ll [&#8230;]</p>\nThe post <a href=\"https://www.johndcook.com/blog/2026/02/04/gamma-reciprocal/\">Γ(1/n)</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
          "content": "<p>If <em>n</em> is a positive integer, then rounding Γ(1/<em>n</em>) up to the nearest integer gives <em>n</em>. In symbols,</p>\n<p><img alt=\"\\left\\lceil \\Gamma\\left( \\tfrac{1}{n}\\right) \\right\\rceil = n\" class=\"aligncenter\" height=\"36\" src=\"https://www.johndcook.com/gamma_recip.svg\" style=\"background-color: white;\" width=\"91\" /></p>\n<p>We an illustrate this with the following Python code.</p>\n<pre>&gt;&gt;&gt; from scipy.special import gamma\n&gt;&gt;&gt; from math import ceil\n&gt;&gt;&gt; for n in range(1, 101):\n    ... assert(ceil(gamma(1/n)) == n)\n</pre>\n<p>You can find a full proof in [1]. I&#8217;ll give a partial proof that may be more informative than the full proof.</p>\n<p>The asymptotic expansion of the gamma function near zero is</p>\n<p><img alt=\"\\Gamma(z) = \\frac{1}{z} - \\gamma + {\\cal O}(z^2)\" class=\"aligncenter\" height=\"40\" src=\"https://www.johndcook.com/gamma_recip2.svg\" style=\"background-color: white;\" width=\"172\" /></p>\n<p>where γ is the Euler-Mascheroni constant.</p>\n<p>So when we set <em>z</em> = 1/<em>n</em> we find Γ(1/<em>n</em>) ≈ <em>n</em> − γ + <em>O</em>(1/<em>n</em>²). Since 0 &lt; γ &lt; 1, the theorem above is true for sufficiently large <em>n</em>. And it turns out &#8220;sufficiently large&#8221; can be replaced with <em>n</em> ≥ 1.</p>\n<p>[1] Gamma at reciprocals of integers: 12225. American Mathematical Monthly. October 2022. pp 789–790.</p>The post <a href=\"https://www.johndcook.com/blog/2026/02/04/gamma-reciprocal/\">Γ(1/n)</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
          "depth": 1,
          "published": "2026-02-05T03:42:12+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 1.0,
      "heat_score": 46.0,
      "rank": 1
    },
    {
      "canonical_name": "多元宇宙与法律理论",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Pluralistic: All laws are local (05 Feb 2026)",
          "link": "https://pluralistic.net/2026/02/05/contingency/",
          "source": "pluralistic.net",
          "summary": "Today's links All laws are local: And no law knows how evitable it is. Hey look at this: Delights to delectate. Object permanence: Whisky PC; Anitfeatures; Silicon Roundabout; Steampunk Etch-A-Sketch; MLMs as mirror-world organizers. Upcoming appearances: Where to find me. Recent appearances: Where I've been. Latest books: You keep readin' em, I'll keep writin' 'em. Upcoming books: Like I said, I'll keep writin' 'em. Colophon: All the rest. All laws are local (permalink) About halfway through Thomas Piketty's 2013 barnstorming Capital in the 21st Century, Piketty tosses off a little insight that skewered me on the spot and never let me go: the notion that any societal condition that endures beyond a generation becomes \"eternal\" in the popular consciousness: https://memex.craphound.com/2014/06/24/thomas-pikettys-capital-in-the-21st-century/ Piketty was referring to \"primogeniture,\" the ancient practice of automatically passing the family fortune onto the eldest son (or, if no son was available, the eldest nephew). Primogeniture did important work by keeping dynastic fortunes intact, rather than dividing them up among all children of some baron or lord or other guillotineable monster. Primogeniture persisted until the age of colonization, when Europe's \"great powers\" stole the rest of the world. In that moment, the size of Europe's great fortunes expanded by orders of magnitude. This vast increase in the wealth of Europe's most murderous, remorseless looters made primogeniture obsolete. There was so much blood-soaked money available to the nobility that every son could found a \"great house.\" After a couple generations' worth of this, the colonies were exhausted. There were no more lands to conquer, which meant that every son could no longer expect to found his own fortune. But for these chinless masters of the universe, a world where every son of every rich man wouldn't get his own dynasty was incomprehensible. To do otherwise was literally unimaginable. It was unnatural. For Piketty, this explained World War I: the world's chinless inbred monsters embarking upon an orgy of bloodletting to relieve one another of the lands &#8211; and peoples &#8211; they'd claimed as their property in order to carry on the \"eternal\" tradition of every son starting his own fortune. It's a very important idea, and a provocative explanation for one of the 20th Century's defining events. That's why it struck me so hard when I first read it, but the reason it stuck with me for the decade-plus since I encountered that it is a vital observation about the human condition: as a species, we forget so much. Something that was commonplace a generation ago becomes unimaginable today, and vice versa. Even people who lived through those years forget who they were and what they took for granted in those days. Think, for example, of all those evangelicals who would vote for Satan himself if he promised to hang any woman who obtained an abortion; the same evangelicals who, just a few decades ago, viewed anti-abortionism as a politically suspect form of crypto-papacy: https://pluralistic.net/2021/12/18/schizmogenesis/ Perhaps the reason Piketty's primogeniture-based explanation for WWI struck me so forcefully and durably is that I imbibed a prodigious amount of science fiction as a boy, including the aphorism that \"all laws are local, and no law knows how local it is\": https://locusmag.com/feature/cory-doctorow-a-cosmopolitan-literature-for-the-cosmopolitan-web/ In other words, things that seem eternal and innate to the human condition to you are apt to have been invented ten minutes before you started to notice the world around you and might seem utterly alien to your children. As Douglas Adams put it: Anything that is in the world when you're born is normal and ordinary and is just a natural part of the way the world works. Anything that's invented between when you're fifteen and thirty-five is new and exciting and revolutionary and you can probably get a career in it. Anything invented after you're thirty-five is against the natural order of things. https://en.wikiquote.org/wiki/Douglas_Adams This notion is much on my mind right now because the world is (to me, at least) unassailably in a state of change, and everything is up for grabs. Europe went from 15 years behind on its climate goals to ten years ahead of schedule after the supply of Russian gas dried up and Europeans found themselves shivering in the dark. The massive leap in EU solar means that the (seemingly) all-powerful fossil fuel lobby has absolutely, comprehensively eaten shit, something that was unthinkable just a few years ago: https://pluralistic.net/2025/09/23/our-friend-the-electron/#to-every-man-his-castle Indeed, this happened so fast that many people (including many Europeans) haven't even noticed that it happened. Back in December, when I was at CCC in Hamburg, I talked to a bunch of European activists, close watchers of the Commission and the Parliament, who were completely convinced that Europe would never spurn the fossil fuel sector &#8211; despite the fact that it had already happened. Indeed, it may be that intimate familiarity with European politics is a liability when things change. Spend enough time observing up close how supine European politicians and their Eurocrats are and you may find yourself so reflexively conditioned to view them as spineless corporate lackeys and thus unable to notice when they finally dig up a vertebra or two. Smart financiers are familiar with Stein's Law: \"anything that can't go on forever eventually stops.\" Change happens. Eternal verities might be fifteen minutes older than you. Pink used to be the color of ferocious masculinity, whereas blue was so girly as to be practically titular: https://en.wikipedia.org/wiki/Gendered_associations_of_pink_and_blue Real talk: I have serious, debilitating chronic pain. One of the reasons I'm so prolific is that the only time I stop noticing how much I hurt is when I'm lost in work (compartmentalization is a hell of a drug, and while it's not always healthy, it has its upsides). Ask anyone with chronic pain and they'll tell you that treating pain eventually becomes your hobby, a bottomless well of esoteric dives into various \"modalities\" of pain treatment. Thus it is that I've found myself on one or two psychologists' couches, learning about different mental approaches to living with constant pain. One of the most useful pieces of advice I've gotten was to attend closely to how my pain changes &#8211; how it ebbs and flows. The point is that if pain changes, that means that it can change. It feels eternal, but it comes and goes. Maybe someday it will go altogether. And even if it doesn't, it may improve. It probably will, at least for a while. Things change. Our current crop of cowardly, weak appeasers &#8211; in Congress, in Parliament, in the European Parliament &#8211; have, at various times (and very recently), found their spines. The factions within them that militated for the kind of bold action that might meet this moment have, from time to time, won the day. We have lived through total transformations in our politics before, and that means we might live through them again: https://hypertext.niskanencenter.org/p/the-fragmentation-flywheel Sure, it's easy and tempting to assume that our leaders will always suck as hard as they suck now. But latent in that assumption is that the leaders who presided over big, incredible transformations were exceptional people. Maybe they were and maybe they weren't, but I'm here to tell you, ten minutes' worth of research into the biographies of the \"heroes\" of our history will reveal them to have been every bit as capable of monstrousness, cowardice, cruelty and pig-ignorant bigotry as any of today's rotating cast of fascist goons: https://truthout.org/articles/disrupting-the-myth-of-franklin-d-roosevelt-in-the-age-of-trump-sanders-and-clinton/ The question isn't merely \"How do we elect better leaders?\" It's \"How do we make our leaders follow us?\" Today's Democrats are unserious quislings who keep bringing a squirt-gun to a mass-casualty assault-rifle spree-shooting. How do we terrorize these cowards into rising to the moment? If we want Congressional Democrats to form a Nuremburg Caucus and start holding hearings on who they're going to put in the dock when the Trump regime collapses, we're going to have to drive them to it. And we can! The Democrats who gave us the New Deal weren't braver or more moral than the self-dealing millionaires in Congress today &#8211; they were more afraid of their base. Things change. Some years ago, I gave a speech at Consumer Reports headquarters in Poughkeepsie, trying to get them to refuse to give a passing grade to any product with DRM, on the grounds that the manufacturer could alter how that device worked at any time in the future, meaning that no matter how well a device worked now, it might turn into a pile of shit at any time in the future: https://www.soundguys.com/the-sonos-app-death-spiral-132873/ They didn't take me up on this suggestion, obviously. They made the (seemingly) reasonable point that people bought Consumer Reports to find out what to buy, not to be told that they shouldn't buy anything. Every product in many key categories came with DRM, meaning that their recommendation would have had to be \"just don't buy any of it.\" But today, consumer review sites do sometimes recommend nothing: https://www.mozillafoundation.org/en/blog/privacy-nightmare-on-wheels-every-car-brand-reviewed-by-mozilla-including-ford-volkswagen-and-toyota-flunks-privacy-test/ And of course, there's some precedent here. Somewhere between the emergence of the evidence for seatbelts and the appearance of seatbelts in most makes and models of cars, there would have been a time when the answer to \"which car should I buy?\" was \"don't buy a car, they're all unsafe at any speed.\" Things change. Today, every car has a seatbelt, and they'd continue to do so, even if we did away with regulations requiring seatbelts. Driving a car without a seatbelt would be as weird and terrible as using a radium suppository: https://pluralistic.net/2024/09/19/just-stop-putting-that-up-your-ass/#harm-reduction Things change. The nine-justice Supreme Court isn't an eternal verity. It didn't come down off a mountain on two stone tablets. It's about ten seconds old: https://en.wikipedia.org/wiki/Judiciary_Act_of_1869 Tomorrow, it will be different: https://pluralistic.net/2020/09/20/judicial-equilibria/#pack-the-court Our eternals are all ephemerals. The idea that we should tax capital gains at half the rate of wages? It was practically invented yesterday. You know who thought we should tax all income at the same rate? That noted Bolshevik, Ronald fuckin' Reagan: https://archive.thinkprogress.org/flashback-reagan-raised-capital-gains-taxes-to-the-same-level-as-wage-taxes-for-first-time-444438edf242/ We're living through a time of change. Much of it is calamitous. Some of it wondrous: https://pluralistic.net/2025/06/28/mamdani/#trustbusting It's so easy to slip into the habit of thinking that nothing will change, that our politicians will never fear us more than they love the money and power they get from catering to the Epstein class. I'm not denying that this is how they view the world today, but there was a time in living memory when it wasn't true. If it changed before, it can change again: https://pluralistic.net/2026/01/15/how-the-light-gets-in/#theories-of-change Things change. Hey look at this (permalink) The Scourge of Online Sports Betting https://prospect.org/2026/02/04/feb-2026-magazine-sports-scourge-online-betting-fanduel-draftkings/ ICE has offices in 5 Canadian cities. Here’s what it can — and can’t — do https://www.cbc.ca/lite/story/9.7073273 RIP, Fobazi M Ettarh https://bsky.app/profile/fobettarh.bsky.social/post/3me34k3rtvc2j The Roots of the Youth Sports Gold Rush https://prospect.org/2026/02/05/feb-2026-magazine-youth-sports-private-equity/ Object permanence (permalink) #20yrsago UK nurses want to supply clean blades and cutting advice to self-harmers https://web.archive.org/web/20060206205108/http://www.timesonline.co.uk/article/0,,2087-2025748,00.html #20yrsago PC built into whisky bottle https://web.archive.org/web/20060210043104/https://metku.net/index.html?sect=view&#38;amp;n=1&#38;amp;path=mods/whiskypc/index_eng #15yrsago Startups of London’s “Silicon Roundabout” https://www.theguardian.com/technology/2011/feb/06/tech-startup-internet-entrepreneurs #15yrsago Antifeatures: deliberate, expensive product features that no customer wants https://mako.cc/copyrighteous/antifeatures-at-the-free-technology-academy #15yrsago Steampunk Etch-a-Sketch https://www.reddit.com/r/pics/comments/erbnf/a_steampunk_etchasketch_we_made_for_a_friend_this/ #10yrsago There’s a secret “black site” in New York where terrorism suspects are tortured for years at a time https://web.archive.org/web/20160205143012/https://theintercept.com/2016/02/05/mahdi-hashi-metropolitan-correctional-center-manhattan-guantanamo-pretrial-solitary-confinement/ #10yrsago Error 53: Apple remotely bricks phones to punish customers for getting independent repairs https://www.theguardian.com/money/2016/feb/05/error-53-apple-iphone-software-update-handset-worthless-third-party-repair?CMP=Share_iOSApp_Other #10yrsago Toronto City Council defies mayor, demands open, neutral municipal broadband https://www.michaelgeist.ca/2016/02/toronto-city-council-sides-with-crtc-in-rejecting-mayor-torys-support-of-bell-appeal/ #5yrsago Amazon's brutal warehouse \"megacycle\" https://pluralistic.net/2021/02/05/la-bookseller-royalty/#megacycle #5yrsago AT&#38;T customer complains…via WSJ ad https://pluralistic.net/2021/02/05/la-bookseller-royalty/#go-aaron-go #1yrago MLMs are the mirror-world version of community organizing https://pluralistic.net/2025/02/05/power-of-positive-thinking/#the-socialism-of-fools Upcoming appearances (permalink) Salt Lake City: Enshittification at the Utah Museum of Fine Arts (Tanner Humanities Center), Feb 18 https://tanner.utah.edu/center-events/cory-doctorow/ Montreal (remote): Fedimtl, Feb 24 https://fedimtl.ca/ Victoria: 28th Annual Victoria International Privacy &#38; Security Summit, Mar 3-5 https://www.rebootcommunications.com/event/vipss2026/ Berkeley: Bioneers keynote, Mar 27 https://conference.bioneers.org/ Berlin: Re:publica, May 18-20 https://re-publica.com/de/news/rp26-sprecher-cory-doctorow Berlin: Enshittification at Otherland Books, May 19 https://www.otherland-berlin.de/de/event-details/cory-doctorow.html Hay-on-Wye: HowTheLightGetsIn, May 22-25 https://howthelightgetsin.org/festivals/hay/big-ideas-2 Recent appearances (permalink) How the Internet Got Worse (Masters in Business) https://www.youtube.com/watch?v=auXlkuVhxMo Enshittification (Jon Favreau/Offline): https://crooked.com/podcast/the-enshittification-of-the-internet-with-cory-doctorow/ Why Big Tech is a Trap for Independent Creators (Stripper News) https://www.youtube.com/watch?v=nmYDyz8AMZ0 Enshittification (Creative Nonfiction podcast) https://brendanomeara.com/episode-507-enshittification-author-cory-doctorow-believes-in-a-new-good-internet/ Enshittification with Plutopia https://plutopia.io/cory-doctorow-enshittification/ Latest books (permalink) \"Canny Valley\": A limited edition collection of the collages I create for Pluralistic, self-published, September 2025 \"Enshittification: Why Everything Suddenly Got Worse and What to Do About It,\" Farrar, Straus, Giroux, October 7 2025 https://us.macmillan.com/books/9780374619329/enshittification/ \"Picks and Shovels\": a sequel to \"Red Team Blues,\" about the heroic era of the PC, Tor Books (US), Head of Zeus (UK), February 2025 (https://us.macmillan.com/books/9781250865908/picksandshovels). \"The Bezzle\": a sequel to \"Red Team Blues,\" about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (thebezzle.org). \"The Lost Cause:\" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (http://lost-cause.org). \"The Internet Con\": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (http://seizethemeansofcomputation.org). Signed copies at Book Soup (https://www.booksoup.com/book/9781804291245). \"Red Team Blues\": \"A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before.\" Tor Books http://redteamblues.com. \"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin\", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 https://chokepointcapitalism.com Upcoming books (permalink) \"Unauthorized Bread\": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026 \"Enshittification, Why Everything Suddenly Got Worse and What to Do About It\" (the graphic novel), Firstsecond, 2026 \"The Memex Method,\" Farrar, Straus, Giroux, 2026 \"The Reverse-Centaur's Guide to AI,\" a short book about being a better AI critic, Farrar, Straus and Giroux, June 2026 Colophon (permalink) Today's top sources: Currently writing: \"The Post-American Internet,\" a sequel to \"Enshittification,\" about the better world the rest of us get to have now that Trump has torched America (1005 words today, 22660 total) \"The Reverse Centaur's Guide to AI,\" a short book for Farrar, Straus and Giroux about being an effective AI critic. LEGAL REVIEW AND COPYEDIT COMPLETE. \"The Post-American Internet,\" a short book about internet policy in the age of Trumpism. PLANNING. A Little Brother short story about DIY insulin PLANNING This work &#8211; excluding any serialized fiction &#8211; is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net. https://creativecommons.org/licenses/by/4.0/ Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution. How to get Pluralistic: Blog (no ads, tracking, or data-collection): Pluralistic.net Newsletter (no ads, tracking, or data-collection): https://pluralistic.net/plura-list Mastodon (no ads, tracking, or data-collection): https://mamot.fr/@pluralistic Medium (no ads, paywalled): https://doctorow.medium.com/ Twitter (mass-scale, unrestricted, third-party surveillance and advertising): https://twitter.com/doctorow Tumblr (mass-scale, unrestricted, third-party surveillance and advertising): https://mostlysignssomeportents.tumblr.com/tagged/pluralistic \"When life gives you SARS, you make sarsaparilla\" -Joey \"Accordion Guy\" DeVilla READ CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies (\"BOGUS AGREEMENTS\") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer. ISSN: 3066-764X",
          "content": "<p><!--\nTags:\ntheories of change, science fiction, inevitabalism,\n\nSummary:\nAll laws are local; Hey look at this; Upcoming appearances; Recent appearances; Latest books; Upcoming books\n\nURL:\nhttps://pluralistic.net/2026/02/05/contingency/\n\nTitle:\nPluralistic: All laws are local (05 Feb 2026) contingency\n\nBullet:\n&#x1f6cc;&#x1f3fb;\n\nSeparator:\n->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->\n\nTop Sources:\nNone\n\n--><br />\n<a href=\"https://pluralistic.net/2026/02/05/contingency/\"><img class=\"xmasthead_link\" src=\"https://i0.wp.com/craphound.com/images/05Feb2026.jpg?w=840&#038;ssl=1\" /></a></p>\n<h1 class=\"toch1\">Today's links</h1>\n<ul class=\"toc\">\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#this-too-shall-pass\">All laws are local</a>: And no law knows how evitable it is.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#linkdump\">Hey look at this</a>: Delights to delectate.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#retro\">Object permanence</a>: Whisky PC; Anitfeatures; Silicon Roundabout; Steampunk Etch-A-Sketch; MLMs as mirror-world organizers.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#upcoming\">Upcoming appearances</a>: Where to find me.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#recent\">Recent appearances</a>: Where I've been.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#latest\">Latest books</a>: You keep readin' em, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#upcoming-books\">Upcoming books</a>: Like I said, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/05/contingency/#bragsheet\">Colophon</a>: All the rest.\n</li>\n</ul>\n<p><span id=\"more-12381\"></span></p>\n<hr />\n<p><a name=\"this-too-shall-pass\"></a><br />\n<img alt=\"A pair of broken off statue legs, shod ",
          "depth": 0.8,
          "published": "2026-02-05T12:57:09+00:00",
          "category": "趋势观点"
        },
        {
          "title": "Notes on Space GPUs",
          "link": "https://www.dwarkesh.com/p/notes-on-space-gpus",
          "source": "dwarkesh.com",
          "summary": "Turning my Elon prep into a blog post",
          "content": "<p></p><p>John Collison and I <a href=\"https://www.dwarkesh.com/p/elon-musk\">just interviewed Elon</a>. The interview was recorded before we knew that SpaceX was acquiring xAI, so the fact that our first topic was space GPUs now feels all the more relevant.</p><p>As I was preparing to interview Elon, I put together some notes and a <a href=\"https://docs.google.com/spreadsheets/d/1fa48HAwXaboEXNOrAj-xJF2Vv_xxQZjAtgoTu0FnnlY/edit?usp=sharing\">spreadsheet</a> to help me think through orbital datacenters. I turned those notes into this blog post.</p><p>Even if orbital data centers don&#8217;t make sense yet, in the long run the singularity is clearly moving into space. Earth intercepts about one two-billionth of the sun&#8217;s total output. If AI scaling continues, compute will eventually move to where the energy is. So space GPUs are fun to think about, because they give you a sneak peek at the future. Whether that future arrives in 2030, 2040, or 2050 is another question.</p><p><strong>Please take everything below with grains of salt&#8212;grains so big that you might confuse them for rocks. Assume all the numbers are wrong.</strong> Every paragraph below covers a topic that would take an actual expert a week to properly evaluate. What you&#8217;ll find here is what a professional podcaster has pieced together from conversations with LLMs and some very generous people who talked to me before the interview. Thanks to <a href=\"https://x.com/CJHandmer\">Casey Handmer</a>, <a href=\"https://x.com/PhilipJohnston\">Philip Johnston</a>, <a href=\"https://x.com/ezrafeilden\">Ezra Feilden</a>, <a href=\"https://x.com/andrewmccalip\">Andrew McCalip</a>, <a href=\"https://x.com/vinayramasesh\">Vinay Ramasesh</a> and the team at <a href=\"https://www.kineticpartners.com/\">Kinetic Partnership</a> for all their help.</p><h2><strong>Why orbital data centers?</strong></h2><p>The whole reason to go to space is energy. Yes, panels in space get about 40% more irradiance&#8212;but the real advant",
          "depth": 0.7,
          "published": "2026-02-05T18:26:47+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 2,
      "avg_depth": 0.75,
      "heat_score": 44.5,
      "rank": 2
    },
    {
      "canonical_name": "Git工作流",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Git’s Magic Files",
          "link": "https://nesbitt.io/2026/02/05/git-magic-files.html",
          "source": "nesbitt.io",
          "summary": "Magic files and where to find them: .gitignore, .gitattributes, .mailmap, .git-blame-ignore-revs, .lfsconfig, and more.",
          "content": "<p>A follow-up to my post on <a href=\"https://nesbitt.io/2025/11/26/extending-git-functionality.html\">extending git functionality</a>. Git looks for several special files in your repository that control its behavior. These aren’t configuration files in <code class=\"language-plaintext highlighter-rouge\">.git/</code>, they’re committed files that travel with your code and affect how git treats your files.</p>\n\n<p>If you’re building a tool that works with git repositories, like <a href=\"https://github.com/git-pkgs/git-pkgs\">git-pkgs</a>, you’ll want to ensure you respect these configs.</p>\n\n<h3 id=\"gitignore\">.gitignore</h3>\n\n<p>Patterns of files git should never track. One pattern per line, supports wildcards and directory markers.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>node_modules/\n*.log\n.env\ndist/\n</code></pre></div></div>\n\n<p>Git checks multiple ignore files in order: <code class=\"language-plaintext highlighter-rouge\">.gitignore</code> in each directory, <code class=\"language-plaintext highlighter-rouge\">.git/info/exclude</code> for local-only ignores, and the global ignore file at <code class=\"language-plaintext highlighter-rouge\">~/.config/git/ignore</code> or wherever <code class=\"language-plaintext highlighter-rouge\">core.excludesFile</code> points. Global ignores are good for OS-specific files like <code class=\"language-plaintext highlighter-rouge\">.DS_Store</code> or <code class=\"language-plaintext highlighter-rouge\">Thumbs.db</code> that shouldn’t clutter every project’s <code class=\"language-plaintext highlighter-rouge\">.gitignore</code>.</p>\n\n<p>The pattern matching supports wildcards (<code class=\"language-plaintext highlighter-rouge\">*.log</code>), directory markers (<code class=\"language-plaintext highlighter-rouge\">dist/</code>), negation (<code class=\"language-plaintext highlighter-rouge\">!important.log</code>), and character ranges. The <code class=\"language-plaintext highlighter-ro",
          "depth": 0.9,
          "published": "2026-02-05T10:00:00+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.9,
      "heat_score": 43.0,
      "rank": 3
    },
    {
      "canonical_name": "数据集与测试优化",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Fibonacci number certificates",
          "link": "https://www.johndcook.com/blog/2026/02/05/fibonacci-certificate/",
          "source": "johndcook.com",
          "summary": "<p>Suppose I give you a big number F and claim that F is a Fibonacci number. How could you confirm this? Before I go further, let me say what this post is really about. It&#8217;s not about Fibonacci numbers so much as it is about proofs and certificates. There&#8217;s no market for large Fibonacci numbers, and certainly [&#8230;]</p>\nThe post <a href=\"https://www.johndcook.com/blog/2026/02/05/fibonacci-certificate/\">Fibonacci number certificates</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
          "content": "<p>Suppose I give you a big number <em>F</em> and claim that <em>F</em> is a Fibonacci number. How could you confirm this?</p>\n<p>Before I go further, let me say what this post is really about. It&#8217;s not about Fibonacci numbers so much as it is about proofs and certificates. There&#8217;s no market for large Fibonacci numbers, and certainly no need to quickly verify that a number is a Fibonacci number.</p>\n<p>You could write a program to generate Fibonacci numbers, and run it until it either produces <em>F</em> , in which case you know <em>F</em> is a Fibonacci number, or the program produces a larger number than <em>F</em> without having produced <em>F</em>, in which case you know it&#8217;s not a Fibonacci number. But there&#8217;s a faster way.</p>\n<p>A certificate is data that allows you to confirm a solution to a problem in less time, usually far less time, than it took to generate the solution. For example, <a href=\"https://www.johndcook.com/blog/2023/01/03/pratt-certificate/\">Pratt certificates</a> give you a way to prove that a number is prime. For a large prime, you could verify its Pratt certificate much faster than directly trying to prove the number is prime.</p>\n<p>There is a theorem that says a number <em>f</em> is a Fibonacci number if and only if one of 5<em>f</em><sup>2</sup> ± 4 is a perfect square. So in addition to <em>F</em> another number <em>r</em> that is a certificate that <em>F</em> is a Fibonacci number. You compute</p>\n<p style=\"padding-left: 40px;\"><em>N</em> = 5<em>F</em>² − <em>r</em>²</p>\n<p>and if <em>N</em> is equal to 4 or −4, you know that <em>F</em> is a Fibonacci number. Otherwise it is not.</p>\n<p>Here&#8217;s a small example. Suppose I give you (12586269025, 28143753123) and claim that the first number is a Fibonacci number and the second number is its certificate. You can compute</p>\n<p style=\"padding-left: 40px;\">5 × 12586269025² − 28143753123²</p>\n<p>and get −4, verifying the claim.</p>\n<p>Certificates are all about th",
          "depth": 0.8,
          "published": "2026-02-05T17:14:20+00:00",
          "category": "技术探讨"
        },
        {
          "title": "Writing an LLM from scratch, part 32c -- Interventions: removing dropout",
          "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32c-interventions-removing-dropout",
          "source": "gilesthomas.com",
          "summary": "<p>This is the second in my series of attempts to improve the loss on my test dataset\n-- interventions, as I'm calling them --\nfor a from-scratch GPT-2 small base model, trained on code based on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>Last time around I saw <a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">what gradient clipping can do</a> --\nit improved loss over <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">the baseline</a>\nby 0.014, bringing it down from 3.692 to 3.678.  Not much, but it's something!</p>\n\n<p>This time, I wanted to see what happened if we trained without dropout.  Would removing it make\nthe test loss worse, or better?</p>\n<h3 id=\"background\">Background:</h3>\n\n<p>In a blog post last summer about\n<a href=\"https://magazine.sebastianraschka.com/i/170506328/21-removing-dropout\">architectural advances in LLMs since GPT-2</a>,\nSebastian Raschka wrote:</p>\n\n<blockquote>\n  <p>Dropout (2012) is a traditional technique to prevent overfitting by randomly\n  \"dropping out\" (i.e., setting to zero) a fraction of the layer activations or\n  attention scores (Figure 3) during training. However, dropout is rarely used\n  in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</p>\n  \n  <p>I assume that dropout was originally used in GPT-2 because it was inherited\n  from the original transformer architecture. Researchers likely noticed that\n  it does not really improve LLM performance (I observed the same in my\n  small-scale GPT-2 replication runs). This is likely because LLMs are typically\n  trained for only a single epoch over massive datasets, which is in contrast to\n  the multi-hundred-epoch training regimes for which dropout was first\n  introduced. So, since LLMs see each token only once during training, there is\n  little risk of overfitting.</p>\n</blockquote>\n\n<p>That makes quite a lot of sense.  My own understanding of dropout was that it was\na bit broader than just preventing overfitting -- it seemed to me to be similar\nto the\n<a href=\"/2025/03/dropout-and-mandatory-vacation\">mandatory vacation policies that financial firms user to prevent over-dependence on individuals</a>.\nMy instinct was that having knowledge distributed across different weights in the\nmodel was good in and of itself, even beyond its benefit on multiple-epoch training.</p>\n\n<p>But it is quite a high price to pay.\nWith the training parameters we've been using we're literally discarding 10% of our calculations' results --\nattention weights, feed-forward neuron activations, and so on -- as we do the forward pass.\nIt's easy to see why it would harm training.</p>\n\n<p>Let's give it a go.</p>\n\n<h3 id=\"the-training-run\">The training run</h3>\n\n<p>The nice thing about this one is that, unlike the gradient clipping experiment,\nI didn't have to write any new code.  The dropout level was already controlled by\na setting in the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/main/runs/8xa100m40-remove-dropout/model.json\"><code>model.json</code> file</a>,\nso by setting that to zero for this run, I could just kick it off and let it\ndo its thing while I worked on something else:</p>\n\n<p>Here's what the training run chart looked like (please disregard the stuff about\ngrad norms in the title and the axis -- I'll remove that for the next train):</p>\n\n<p><img alt=\"Training chart for zero-dropout run\" src=\"/post-assets/llm-from-scratch-32c-interventions-removing-dropout/training-chart.png\" title=\"Training chart for zero-dropout run\" /></p>\n\n<p>As you can see, we still have loss spikes, including one just after global step 20,000\nthat lasts for several checkpoint periods of 617 steps.  I imagine gradient clipping\nmight have helped with that, but I'm very deliberately testing each intervention in\nisolation.</p>\n\n<p>At the end of the training run, we got this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 11,376.067 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 286,589 tokens/second</span>\n<span class=\"go\">Final train loss: 3.621</span>\n</code></pre>\n</div>\n\n<p>So, interestingly, it took 967 seconds -- about 16 minutes -- less time than the\ngradient clipping run, and about 15 minutes less than the baseline train.  So\nwhile gradient clipping added on a small amount of time (or maybe that was just noise),\ndropping dropout certainly seems to speed things up!  I guess there's quite a lot of\nwork involved in generating and applying the random masks that drop things out as we're\ndoing the forward pass.</p>\n\n<p>Anyway, with the model trained, it was time to download it,\n<a href=\"https://huggingface.co/gpjt/8xa100m40-remove-dropout\">upload it to Hugging Face Hub</a>, and run the evals.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>Firstly, the smoke test, where it just needs to continue the sequence <code>Every effort moves you</code>,\nit came up with something reasonably coherent:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/model.json<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you to make the world a better place.</span>\n<span class=\"go\">As an international student of the arts in the UK,</span>\n</code></pre>\n</div>\n\n<p>...but it was on the test of the loss on the training set that it was most impressive:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/model.json<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 1086.75it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:54&lt;00:00, 10.87it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.641</span>\n</code></pre>\n</div>\n\n<p>That's a bigger improvement on the baseline train's 3.692 than gradient clipping:\n0.051, which is more than three times the improvement!</p>\n\n<p>Let's start keeping a table of these:</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test set loss</th>\n  <th>Improvement vs baseline</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>8xa100m40-baseline</td>\n  <td>3.692</td>\n  <td>-</td>\n</tr>\n<tr>\n  <td>8xa100m40-gradient-clipping</td>\n  <td>3.678</td>\n  <td>0.014</td>\n</tr>\n<tr>\n  <td>8xa100m40-remove-dropout</td>\n  <td>3.641</td>\n  <td>0.051</td>\n</tr>\n</tbody>\n</table>\n\n<p>Now, of course, we don't know how these different interventions combine together --\nit would be naive to think that if we did both gradient clipping and dropout\nremoval, we'd get a total loss reduction of 0.014 + 0.051 -- but, especially with that\nlong-lived loss spike in our training run -- it does feel like they might play well\ntogether.</p>\n\n<h3 id=\"wrapping-up\">Wrapping up</h3>\n\n<p>So, that's dropout covered.  Which one next?  I think a nice easy one that I should\nbe able to get done on a Friday will be adding bias to the attention weight calculations.\nLet's give that a go and see if it makes things worse or better!</p>\n\n<p>Stay tuned...</p>",
          "content": "<p>This is the second in my series of attempts to improve the loss on my test dataset\n-- interventions, as I'm calling them --\nfor a from-scratch GPT-2 small base model, trained on code based on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>Last time around I saw <a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">what gradient clipping can do</a> --\nit improved loss over <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">the baseline</a>\nby 0.014, bringing it down from 3.692 to 3.678.  Not much, but it's something!</p>\n\n<p>This time, I wanted to see what happened if we trained without dropout.  Would removing it make\nthe test loss worse, or better?</p>\n<h3 id=\"background\">Background:</h3>\n\n<p>In a blog post last summer about\n<a href=\"https://magazine.sebastianraschka.com/i/170506328/21-removing-dropout\">architectural advances in LLMs since GPT-2</a>,\nSebastian Raschka wrote:</p>\n\n<blockquote>\n  <p>Dropout (2012) is a traditional technique to prevent overfitting by randomly\n  \"dropping out\" (i.e., setting to zero) a fraction of the layer activations or\n  attention scores (Figure 3) during training. However, dropout is rarely used\n  in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</p>\n  \n  <p>I assume that dropout was originally used in GPT-2 because it was inherited\n  from the original transformer architecture. Researchers likely noticed that\n  it does not really improve LLM performance (I observed the same in my\n  small-scale GPT-2 replication runs). This is likely because LLMs are typically\n  trained for only a single epoch over massive datasets, which is in contrast to\n  the multi-hundred-epoch training regimes for which dropout was first\n  introduced. So, since LLMs see each token only once during training, there is\n  little risk of o",
          "depth": 0.6,
          "published": "2026-02-05T23:35:00+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 2,
      "avg_depth": 0.7,
      "heat_score": 43.0,
      "rank": 4
    },
    {
      "canonical_name": "人工智能在软件开发中的应用",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Mitchell Hashimoto: My AI Adoption Journey",
          "link": "https://simonwillison.net/2026/Feb/5/ai-adoption-journey/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<p><strong><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey\">Mitchell Hashimoto: My AI Adoption Journey</a></strong></p>\nSome really good and unconventional tips in here for getting to a place with coding agents where they demonstrably improve your workflow and productivity. I particularly liked:</p>\n<ul>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-2-reproduce-your-own-work\">Reproduce your own work</a> - when learning to use coding agents Mitchell went through a period of doing the work manually, then recreating the same solution using agents as an exercise:</p>\n<blockquote>\n<p>I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-3-end-of-day-agents\">End-of-day agents</a> - letting agents step in when your energy runs out:</p>\n<blockquote>\n<p>To try to find some efficiency, I next started up a new pattern: <strong>block out the last 30 minutes of every day to kick off one or more agents.</strong> My hypothesis was that <em>perhaps</em> I could gain some efficiency if the agent can make some <em>positive progress</em> in the times I can't work anyways.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-4-outsource-the-slam-dunks\">Outsource the Slam Dunks</a> - once you know an agent can likely handle a task, have it do that task while you work on something more interesting yourself.</p>\n</li>\n</ul>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46903558\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/mitchell-hashimoto\">mitchell-hashimoto</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a></p>",
          "content": "<p><strong><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey\">Mitchell Hashimoto: My AI Adoption Journey</a></strong></p>\nSome really good and unconventional tips in here for getting to a place with coding agents where they demonstrably improve your workflow and productivity. I particularly liked:</p>\n<ul>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-2-reproduce-your-own-work\">Reproduce your own work</a> - when learning to use coding agents Mitchell went through a period of doing the work manually, then recreating the same solution using agents as an exercise:</p>\n<blockquote>\n<p>I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-3-end-of-day-agents\">End-of-day agents</a> - letting agents step in when your energy runs out:</p>\n<blockquote>\n<p>To try to find some efficiency, I next started up a new pattern: <strong>block out the last 30 minutes of every day to kick off one or more agents.</strong> My hypothesis was that <em>perhaps</em> I could gain some efficiency if the agent can make some <em>positive progress</em> in the times I can't work anyways.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"https://mitchellh.com/writing/my-ai-adoption-journey#step-4-outsource-the-slam-dunks\">Outsource the Slam Dunks</a> - once you know an agent can likely handle a task, have it do that task while you work on something more interesting yourself.</p>\n</li>\n</ul>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46903558\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwil",
          "depth": 0.8,
          "published": "2026-02-05T23:39:07+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 5
    },
    {
      "canonical_name": "代码实验与模型应用",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Quoting Karel D'Oosterlinck",
          "link": "https://simonwillison.net/2026/Feb/6/karel-doosterlinck/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<blockquote cite=\"https://twitter.com/kareldoostrlnck/status/2019477361557926281\"><p>When I want to quickly implement a one-off experiment in a part of the codebase I am unfamiliar with, I get codex to do extensive due diligence. Codex explores relevant slack channels, reads related discussions, fetches experimental branches from those discussions, and cherry picks useful changes for my experiment. All of this gets summarized in an extensive set of notes, with links back to where each piece of information was found. Using these notes, codex wires the experiment and makes a bunch of hyperparameter decisions I couldn’t  possibly make without much more effort.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/kareldoostrlnck/status/2019477361557926281\">Karel D&#x27;Oosterlinck</a>, I spent $10,000 to automate my research at OpenAI with Codex</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/codex-cli\">codex-cli</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
          "content": "<blockquote cite=\"https://twitter.com/kareldoostrlnck/status/2019477361557926281\"><p>When I want to quickly implement a one-off experiment in a part of the codebase I am unfamiliar with, I get codex to do extensive due diligence. Codex explores relevant slack channels, reads related discussions, fetches experimental branches from those discussions, and cherry picks useful changes for my experiment. All of this gets summarized in an extensive set of notes, with links back to where each piece of information was found. Using these notes, codex wires the experiment and makes a bunch of hyperparameter decisions I couldn’t  possibly make without much more effort.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/kareldoostrlnck/status/2019477361557926281\">Karel D&#x27;Oosterlinck</a>, I spent $10,000 to automate my research at OpenAI with Codex</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/codex-cli\">codex-cli</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>",
          "depth": 0.8,
          "published": "2026-02-06T00:42:22+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 6
    },
    {
      "canonical_name": "版本控件编程技巧",
      "category": "技术探讨",
      "articles": [
        {
          "title": "How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?",
          "link": "https://devblogs.microsoft.com/oldnewthing/20260205-00/?p=112042",
          "source": "devblogs.microsoft.com/oldnewthing",
          "summary": "<p>Deny changes to the width.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260205-00/?p=112042\">How can I prevent the user from changing the widths of ListView columns in version 5 of the common controls?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
          "content": "<p>Last time, we saw how to <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037\" title=\"How can I prevent the user from changing the widths of ListView columns?\"> prevent the user from changing the widths of ListView columns</a>, but the technique required version 6 of the common controls. What if you&#8217;re stuck in the dark ages and have to use version 5?</p>\n<p>You can deny the ability to change the width of a header item by listening for <code>HDN_ITEM­CHANGING</code> and returning 1 to deny the change if there is a change to the width.</p>\n<pre>case WM_NOTIFY:\n    {\n        auto hdr = (NMHDR*)lParam;\n        if (hdr-&gt;code == HDN_ITEMCHANGING) {\n            auto header = (NMHEADER*)lParam;\n            if (header-&gt;pitem-&gt;mask &amp; HDI_WIDTH) {\n                return 1;\n            }\n        }\n    }\n    return 0;\n</pre>\n<p>The above code assumes that it is running in a window procedure. If it&#8217;s running in a dialog procedure, then you need to set the dialog message result.</p>\n<pre>case WM_NOTIFY:\n    {\n        auto hdr = (NMHDR*)lParam;\n        if (hdr-&gt;code == HDN_ITEMCHANGING) {\n            auto header = (NMHEADER*)lParam;\n            if (header-&gt;pitem-&gt;mask &amp; HDI_WIDTH) {\n                <span style=\"border-bottom: none;\">SetWindowLongPtr(hDlg, DWLP_MSGRESULT, 1);</span>\n                <span style=\"border-top: none;\">return TRUE;                              </span>\n            }\n        }\n    }\n    return FALSE;\n</pre>\n<p>Note that if somebody tries to change both the width and the text, this will reject the entire change. There is, unfortunately, no way to selectively reject the change: Modifications to <code>header-&gt;pitem-&gt;mask</code> are ignored.¹</p>\n<p>However, all is not lost. Even though changes to the mask are ignored, changes to the <code>pitem-&gt;cxy</code> are still honored, so we can just set the width back to whatever the width is right now.</p>\n<pre>case WM_NOTIFY:\n    {\n        auto hd",
          "depth": 0.8,
          "published": "2026-02-05T15:00:00+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 7
    },
    {
      "canonical_name": "空间计算与数据中心",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Elon Musk - \"In 36 months, the cheapest place to put AI will be space”",
          "link": "https://www.dwarkesh.com/p/elon-musk",
          "source": "dwarkesh.com",
          "summary": "&#8220;Those who live in software land are about to have a hard lesson in hardware.&#8221;",
          "content": "<p>In this episode, John and I got to do a real deep-dive with Elon. We discuss the economics of orbital data centers, the difficulties of scaling power on Earth, what it would take to manufacture humanoids at high-volume in America, xAI&#8217;s business and alignment plans, DOGE, and much more.</p><p>Watch on <a href=\"https://youtu.be/BYXbuik3dgA\">YouTube</a>; listen on <a href=\"https://podcasts.apple.com/us/podcast/elon-musk-in-36-months-the-cheapest-place-to-put-ai/id1516093381?i=1000748400389\">Apple Podcasts</a> or <a href=\"https://open.spotify.com/episode/4nah0x1qQF2hxgJnv8PlmN?si=_U3Ab9A0TOu49wfX6oPQdg\">Spotify</a>.</p><div class=\"youtube-wrap\" id=\"youtube2-BYXbuik3dgA\"><div class=\"youtube-inner\"></div></div><h3>Sponsors</h3><ul><li><p><a href=\"https://mercury.com/personal-banking\">Mercury</a> just started offering personal banking! I&#8217;m already banking with Mercury for business purposes, so getting to bank with them for my personal life makes everything so much simpler. Apply now at <a href=\"https://mercury.com/personal-banking\">mercury.com/personal-banking</a></p></li><li><p><a href=\"https://janestreet.com/dwarkesh\">Jane Street</a> sent me a new puzzle last week: they trained a neural net, shuffled all 96 layers, and asked me to put them back in order. I tried but&#8230; I didn&#8217;t quite nail it. If you&#8217;re curious, or if you think you can do better, you should take a stab at <a href=\"https://janestreet.com/dwarkesh\">janestreet.com/dwarkesh</a></p></li><li><p><a href=\"https://labelbox.com/dwarkesh\">Labelbox</a> can get you robotics and RL data at scale. Labelbox starts by helping you define your ideal data distribution, and then their massive Alignerr network collects frontier-grade data that you can use to train your models. Learn more at <a href=\"https://labelbox.com/dwarkesh\">labelbox.com/dwarkesh</a></p></li></ul><h2>Timestamps</h2><p>00:00:00 - Orbital data centers</p><p>00:36:46 - Grok and alignment</p><p>00:59:56 - xAI&#8217;s business p",
          "depth": 0.8,
          "published": "2026-02-05T16:45:08+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 8
    },
    {
      "canonical_name": "硬盘产品与公司发展",
      "category": "行业动态",
      "articles": [
        {
          "title": "What happened to Conner hard drives",
          "link": "https://dfarq.homeip.net/what-happened-to-conner-hard-drives/?utm_source=rss&utm_medium=rss&utm_campaign=what-happened-to-conner-hard-drives",
          "source": "dfarq.homeip.net",
          "summary": "<p>Conner Peripherals was founded June 17, 1985 by Seagate Technology co-founder Finis Conner, in San Jose. On Sep. 20, 1995, Conner agreed to merge with Seagate in a deal worth $1 billion. The deal closed February 5, 1996. At the</p>\n<p>The post <a href=\"https://dfarq.homeip.net/what-happened-to-conner-hard-drives/\">What happened to Conner hard drives</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
          "content": "<p>Conner Peripherals was founded June 17, 1985 by Seagate Technology co-founder Finis Conner, in San Jose. On Sep. 20, 1995, Conner agreed to merge with Seagate in a deal worth $1 billion. The deal closed February 5, 1996. At the</p>\n<p>The post <a href=\"https://dfarq.homeip.net/what-happened-to-conner-hard-drives/\">What happened to Conner hard drives</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
          "depth": 0.8,
          "published": "2026-02-05T12:00:49+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 9
    },
    {
      "canonical_name": "AI模型发布与更新",
      "category": "行业动态",
      "articles": [
        {
          "title": "Opus 4.6 and Codex 5.3",
          "link": "https://simonwillison.net/2026/Feb/5/two-new-models/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<p>Two major new model releases today, within about 15 minutes of each other.</p>\n<p>Anthropic <a href=\"https://www.anthropic.com/news/claude-opus-4-6\">released Opus 4.6</a>. Here's <a href=\"https://gist.github.com/simonw/a6806ce41b4c721e240a4548ecdbe216\">its pelican</a>:</p>\n<p><img alt=\"Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.\" src=\"https://static.simonwillison.net/static/2026/opus-4.6-pelican.png\" /></p>\n<p>OpenAI <a href=\"https://openai.com/index/introducing-gpt-5-3-codex/\">release GPT-5.3-Codex</a>, albeit only via their Codex app, not yet in their API. Here's <a href=\"https://gist.github.com/simonw/bfc4a83f588ac762c773679c0d1e034b\">its pelican</a>:</p>\n<p><img alt=\"Not nearly as good - the bicycle is a bit mangled, the pelican not nearly as well rendered - it's more of a line drawing.\" src=\"https://static.simonwillison.net/static/2026/codex-5.3-pelican.png\" /></p>\n<p>I've had a bit of preview access to both of these models and to be honest I'm finding it hard to find a good angle to write about them - they're both <em>really good</em>, but so were their predecessors Codex 5.2 and Opus 4.5. I've been having trouble finding tasks that those previous models couldn't handle but the new ones are able to ace.</p>\n<p>The most convincing story about capabilities of the new model so far is Nicholas Carlini from Anthropic talking about Opus 4.6 and <a href=\"https://www.anthropic.com/engineering/building-c-compiler\">Building a C compiler with a team of parallel Claudes</a> - Anthropic's version of Cursor's <a href=\"https://simonwillison.net/2026/Jan/23/fastrender/\">FastRender project</a>.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a>, <a href=\"https://simonwillison.net/tags/c\">c</a>, <a href=\"https://simonwillison.net/tags/nicholas-carlini\">nicholas-carlini</a></p>",
          "content": "<p>Two major new model releases today, within about 15 minutes of each other.</p>\n<p>Anthropic <a href=\"https://www.anthropic.com/news/claude-opus-4-6\">released Opus 4.6</a>. Here's <a href=\"https://gist.github.com/simonw/a6806ce41b4c721e240a4548ecdbe216\">its pelican</a>:</p>\n<p><img alt=\"Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.\" src=\"https://static.simonwillison.net/static/2026/opus-4.6-pelican.png\" /></p>\n<p>OpenAI <a href=\"https://openai.com/index/introducing-gpt-5-3-codex/\">release GPT-5.3-Codex</a>, albeit only via their Codex app, not yet in their API. Here's <a href=\"https://gist.github.com/simonw/bfc4a83f588ac762c773679c0d1e034b\">its pelican</a>:</p>\n<p><img alt=\"Not nearly as good - the bicycle is a bit mangled, the pelican not nearly as well rendered - it's more of a line drawing.\" src=\"https://static.simonwillison.net/static/2026/codex-5.3-pelican.png\" /></p>\n<p>I've had a bit of preview access to both of these models and to be honest I'm finding it hard to find a good angle to write about them - they're both <em>really good</em>, but so were their predecessors Codex 5.2 and Opus 4.5. I've been having trouble finding tasks that those previous models couldn't handle but the new ones are able to ace.</p>\n<p>The most convincing story about capabilities of the new model so far is Nicholas Carlini from Anthropic talking about Opus 4.6 and <a href=\"https://www.anthropic.com/engineering/building-c-compiler\">Building a C compiler with a team of parallel Claudes</a> - Anthropic's version of Cursor's <a href=\"https://simonwillison.net/2026/Jan/23/fastrender/\">FastRender project</a>.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/pelic",
          "depth": 0.8,
          "published": "2026-02-05T20:29:20+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 10
    },
    {
      "canonical_name": "AI行业动态",
      "category": "行业动态",
      "articles": [
        {
          "title": "Is the Great AI meltdown imminent? [NSFW]",
          "link": "https://garymarcus.substack.com/p/is-the-great-ai-meltdown-imminent",
          "source": "garymarcus.substack.com",
          "summary": "A $100 billion dollar deal that was propping up the industry just disappeared",
          "content": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!RzuG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f62dfb5-9c11-4395-b355-fd873644a449_713x965.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"965\" src=\"https://substackcdn.com/image/fetch/$s_!RzuG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f62dfb5-9c11-4395-b355-fd873644a449_713x965.png\" width=\"713\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></",
          "depth": 0.8,
          "published": "2026-02-05T15:11:22+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 11
    },
    {
      "canonical_name": "文化与社会现象",
      "category": "社会研究",
      "articles": [
        {
          "title": "How to stop being boring",
          "link": "https://www.joanwestenberg.com/how-to-stop-being-boring/",
          "source": "joanwestenberg.com",
          "summary": "<p>The most interesting people I know aren&apos;t trying to be interesting. </p><p>Thank God. </p><p>They&apos;re saying what they actually think and wearing what they actually like, pursuing hobbies that genuinely fascinate them, regardless of whether those hobbies are cool. The most mind-numbingly boring people I know are</p>",
          "content": "<img alt=\"How to stop being boring\" src=\"https://images.unsplash.com/photo-1548159417-f283998827c1?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDgxfHxhYnN0cmFjdHxlbnwwfHx8fDE3NzAzMTg2NjN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000\" /><p>The most interesting people I know aren&apos;t trying to be interesting. </p><p>Thank God. </p><p>They&apos;re saying what they actually think and wearing what they actually like, pursuing hobbies that genuinely fascinate them, regardless of whether those hobbies are cool. The most mind-numbingly boring people I know are working overtime to seem interesting: curating their book recommendations, workshopping their opinions to be provocative but not too provocative. </p><p>The effort is palpable. And the effort is exactly what makes them forgettable.</p><p>I&apos;ve come to believe that boring = personality edited down to nothing. Somewhere along the way, too many of us learned to sand off our weird edges, to preemptively remove anything that might make someone uncomfortable or make us seem difficult to be around.</p><p>And the result = boredom.</p><h2 id=\"youve-been-editing-yourself\">You&apos;ve been editing yourself</h2><p>Erving Goffman wrote in 1959 about how we all perform versions of ourselves depending on context. What&apos;s less normal is when the performance becomes the only thing left. When you&apos;ve been editing yourself for so long that you&apos;ve forgotten what the original draft looked like.</p><p>This happens gradually. In middle school, you learn that certain enthusiasms are embarrassing. In high school, you learn which opinions are acceptable in your social group. In college, you refine your persona further. By the time you&apos;re an adult, you&apos;ve become so skilled at reading rooms and ajusting accordingly that you don&apos;t even notice you&apos;re doing it. You&apos;ve automated your own inauthenticity.</p><p>This process feels like maturity, or it feels the way we thi",
          "depth": 0.8,
          "published": "2026-02-05T19:11:57+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 12
    }
  ]
}