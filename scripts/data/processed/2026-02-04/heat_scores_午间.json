{
  "timestamp": "2026-02-04 午间",
  "total_topics": 14,
  "topics": [
    {
      "canonical_name": "SpaceX与xAI合并理论分析",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Four theories about the SpaceX - xAI merger",
          "link": "https://garymarcus.substack.com/p/four-theories-about-the-spacex-xai",
          "source": "garymarcus.substack.com",
          "summary": "Spoiler alert: it&#8217;s probably not really about synergy",
          "content": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!00_I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a90c24c-f299-44d9-941d-dc650855d16e_1109x759.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"759\" src=\"https://substackcdn.com/image/fetch/$s_!00_I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a90c24c-f299-44d9-941d-dc650855d16e_1109x759.jpeg\" width=\"1109\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></d",
          "depth": 0.8,
          "published": "2026-02-03T19:58:00+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 1
    },
    {
      "canonical_name": "OpenAI发布Codex桌面应用",
      "category": "技术探讨",
      "articles": [
        {
          "title": "OpenAI’s Codex",
          "link": "https://simonwillison.net/2026/Feb/2/introducing-the-codex-app/",
          "source": "daringfireball.net",
          "summary": "<p>Simon Willison:</p>\n\n<blockquote>\n  <p>OpenAI just released a new macOS app for their Codex coding agent. I’ve had a few days of preview access — it’s a solid app that provides a nice UI over the capabilities of the Codex CLI agent and adds some interesting new features, most notably first-class support for Skills, and Automations for running scheduled tasks.</p>\n</blockquote>\n\n<p>Interesting, for sure. But <a href=\"https://daringfireball.net/linked/2026/02/03/xcode-ai-agentic-coding\">super-duper</a> interesting? I don’t know.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2026/02/03/openai-codex\" title=\"Permanent link to ‘OpenAI’s Codex’\">&nbsp;★&nbsp;</a>\n</div>",
          "content": "<p>Simon Willison:</p>\n\n<blockquote>\n  <p>OpenAI just released a new macOS app for their Codex coding agent. I’ve had a few days of preview access — it’s a solid app that provides a nice UI over the capabilities of the Codex CLI agent and adds some interesting new features, most notably first-class support for Skills, and Automations for running scheduled tasks.</p>\n</blockquote>\n\n<p>Interesting, for sure. But <a href=\"https://daringfireball.net/linked/2026/02/03/xcode-ai-agentic-coding\">super-duper</a> interesting? I don’t know.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2026/02/03/openai-codex\" title=\"Permanent link to ‘OpenAI’s Codex’\">&nbsp;★&nbsp;</a>\n</div>",
          "depth": 0.8,
          "published": "2026-02-04T01:40:24+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 2
    },
    {
      "canonical_name": "Deno团队发布SaaS平台沙箱功能",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Introducing Deno Sandbox",
          "link": "https://simonwillison.net/2026/Feb/3/introducing-deno-sandbox/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<p><strong><a href=\"https://deno.com/blog/introducing-deno-sandbox\">Introducing Deno Sandbox</a></strong></p>\nHere's a new hosted sandbox product from the Deno team. It's actually unrelated to Deno itself - this is part of their Deno Deploy SaaS platform. As such, you don't even need to use JavaScript to access it - you can create and execute code in a hosted sandbox using their <a href=\"https://pypi.org/project/deno-sandbox/\">deno-sandbox</a> Python library like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> DENO_DEPLOY_TOKEN=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>... API token ...<span class=\"pl-pds\">\"</span></span>\nuv run --with deno-sandbox python</pre></div>\n<p>Then:</p>\n<pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">deno_sandbox</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">DenoDeploy</span>\n\n<span class=\"pl-s1\">sdk</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">DenoDeploy</span>()\n\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">sdk</span>.<span class=\"pl-c1\">sandbox</span>.<span class=\"pl-c1\">create</span>() <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sb</span>:\n    <span class=\"pl-c\"># Run a shell command</span>\n    <span class=\"pl-s1\">process</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">spawn</span>(\n        <span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s1\">args</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"Hello from the sandbox!\"</span>]\n    )\n    <span class=\"pl-s1\">process</span>.<span class=\"pl-c1\">wait</span>()\n    <span class=\"pl-c\"># Write and read files</span>\n    <span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">fs</span>.<span class=\"pl-c1\">write_text_file</span>(\n        <span class=\"pl-s\">\"/tmp/example.txt\"</span>, <span class=\"pl-s\">\"Hello, World!\"</span>\n    )\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">fs</span>.<span class=\"pl-c1\">read_text_file</span>(\n        <span class=\"pl-s\">\"/tmp/example.txt\"</span>\n    ))</pre>\n<p>There’s a JavaScript client library as well. The underlying API isn’t documented yet but appears <a href=\"https://tools.simonwillison.net/zip-wheel-explorer?package=deno-sandbox#deno_sandbox/sandbox.py--L187\">to use WebSockets</a>.</p>\n<p>There’s a lot to like about this system. Sandboxe instances can have up to 4GB of RAM, get 2 vCPUs, 10GB of ephemeral storage, can mount persistent volumes and can use snapshots to boot pre-configured custom images quickly. Sessions can last up to 30 minutes and are billed by CPU time, GB-h of memory and volume storage usage.</p>\n<p>When you create a sandbox you can configure network domains it’s allowed to access.</p>\n<p>My favorite feature is the way it handles API secrets.</p>\n<pre><span class=\"pl-k\">with</span> <span class=\"pl-s1\">sdk</span>.<span class=\"pl-c1\">sandboxes</span>.<span class=\"pl-c1\">create</span>(\n    <span class=\"pl-s1\">allowNet</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"api.openai.com\"</span>],\n    <span class=\"pl-s1\">secrets</span><span class=\"pl-c1\">=</span>{\n        <span class=\"pl-s\">\"OPENAI_API_KEY\"</span>: {\n            <span class=\"pl-s\">\"hosts\"</span>: [<span class=\"pl-s\">\"api.openai.com\"</span>],\n            <span class=\"pl-s\">\"value\"</span>: <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">environ</span>.<span class=\"pl-c1\">get</span>(<span class=\"pl-s\">\"OPENAI_API_KEY\"</span>),\n        }\n    },\n) <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sandbox</span>:\n    <span class=\"pl-c\"># ... $OPENAI_API_KEY is available</span></pre>\n<p>Within the container that <code>$OPENAI_API_KEY</code> value is set to something like this:</p>\n<pre><code>DENO_SECRET_PLACEHOLDER_b14043a2f578cba...\n</code></pre>\n<p>Outbound API calls to <code>api.openai.com</code> run through a proxy which is aware of those placeholders and replaces them with the original secret.</p>\n<p>In this way the secret itself is not available to code within the sandbox, which limits the ability for malicious code (e.g. from a prompt injection) to exfiltrate those secrets.</p>\n<p>From <a href=\"https://news.ycombinator.com/item?id=46874097#46874959\">a comment on Hacker News</a> I learned that Fly have a project called <a href=\"https://github.com/superfly/tokenizer\">tokenizer</a> that implements the same pattern. Adding this to my list of tricks to use with sandoxed environments!\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46874097\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/security\">security</a>, <a href=\"https://simonwillison.net/tags/deno\">deno</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a></p>",
          "content": "<p><strong><a href=\"https://deno.com/blog/introducing-deno-sandbox\">Introducing Deno Sandbox</a></strong></p>\nHere's a new hosted sandbox product from the Deno team. It's actually unrelated to Deno itself - this is part of their Deno Deploy SaaS platform. As such, you don't even need to use JavaScript to access it - you can create and execute code in a hosted sandbox using their <a href=\"https://pypi.org/project/deno-sandbox/\">deno-sandbox</a> Python library like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> DENO_DEPLOY_TOKEN=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>... API token ...<span class=\"pl-pds\">\"</span></span>\nuv run --with deno-sandbox python</pre></div>\n<p>Then:</p>\n<pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">deno_sandbox</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">DenoDeploy</span>\n\n<span class=\"pl-s1\">sdk</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">DenoDeploy</span>()\n\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">sdk</span>.<span class=\"pl-c1\">sandbox</span>.<span class=\"pl-c1\">create</span>() <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sb</span>:\n    <span class=\"pl-c\"># Run a shell command</span>\n    <span class=\"pl-s1\">process</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">spawn</span>(\n        <span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s1\">args</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"Hello from the sandbox!\"</span>]\n    )\n    <span class=\"pl-s1\">process</span>.<span class=\"pl-c1\">wait</span>()\n    <span class=\"pl-c\"># Write and read files</span>\n    <span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">fs</span>.<span class=\"pl-c1\">write_text_file</span>(\n        <span class=\"pl-s\">\"/tmp/example.txt\"</span>, <span class=\"pl-s\">\"Hello, World!\"</span>\n    )\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">fs</span>.<span class=\"pl-c1\">read_text_file</span",
          "depth": 0.8,
          "published": "2026-02-03T22:44:50+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 3
    },
    {
      "canonical_name": "从头开始训练大型语言模型",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Writing an LLM from scratch, part 32a -- Interventions: training a baseline model",
          "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32a-interventions-baseline-model",
          "source": "gilesthomas.com",
          "summary": "<p>I'm rounding out my series of posts on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\"\nby seeing how I could train the <em>best</em> base model I can from scratch on my own hardware.\nI started by <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">training one in two days on my RTX 3090</a>,\nand found that while it was a decent little model, it wasn't as good as the original\nGPT-2 small, either in terms of the loss it got on my test dataset, or\nin terms of how good it was at following instruction prompts after fine-tuning on them.  I decided\nthat I wanted to see what levers I could pull -- dropout, attention weight biases, and so\non -- to make it better.</p>\n\n<p>For that, I didn't want to have my PC tied up for days at a time with multiple long training runs,\nso I <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">learned how to train faster in the cloud</a>.\nThat led to some <a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">refinements in the prompt-following test I was using</a>,\nand I also spent a bit of time on a side quest getting the various models I'd trained <a href=\"/2026/01/llm-from-scratch-31-models-on-hugging-face\">onto Hugging Face Hub</a>.</p>\n\n<p>Now it's time to try the various \"interventions\", as I'll call them -- the levers to\npull to see if I can make the model better.  This post is\nto recap what they are, and to describe what I did to establish a baseline model to\ncompare to.</p>\n<h3 id=\"the-interventions\">The interventions</h3>\n\n<p>I listed a number of possible interventions at the end of the RTX 3090 post; I'm not going\nto do them all, but for completeness, here's the full list:</p>\n\n<ul>\n<li>The amount of training data.  I'm not going to dig into this one; it looks like it\ndoes help, but the returns diminish rapidly, so I think that in order to get any serious\nimprovement we'd need to train for much more than two days locally.  In the one\n\"extended training\" test I did, I managed to get the loss down from 4.167 to 4.135, which\nwas... less-than-inspiring.</li>\n<li>The number of epochs.  I'm going to stick to single-epoch training -- that is, I'll\ntrain on a single pass through an amount of non-repeating data chosen to take 48 hours to handle\non my local machine.</li>\n<li>The bias on the <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math> matrices.  This one definitely sounds worth looking\ninto -- easy, as it's just a change to a config flag, and makes the model more like the\noriginal GPT-2.  I'll give that a go.</li>\n<li>Dropout.  I've read that for single-epoch training, dropout doesn't help (which\ndoesn't quite work with <a href=\"/2025/03/dropout-and-mandatory-vacation\">my mental model</a> of what it's for, but\ndoes sound plausible).  Worth a look!</li>\n<li>The learning rate, and weight decay.  The values I've used for these are basically copypasta\nfrom the book.  I think I should learn to understand these and try to optimise them a bit.</li>\n<li>The precision.  I'm using <a href=\"https://docs.pytorch.org/docs/stable/amp.html\">AMP</a>, which means that\nsome calculations are done in 16-bit rather than 32-bit, and calling\n<a href=\"https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\"><code>set_float32_matmul_precision</code></a>\nwith <code>\"high\"</code> to let PyTorch choose to use the GPU's tensor cores, which use TF32, a kind of \"32-bit float lite\" (see the\n<a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">post on the local train</a> for details).\nThose both (at least potentially) reduce the precision of\nthe train below what you'd get if you trained with full-fat <code>float32</code>.  Would reverting\nthat be worth the longer train time?  I should probably at least poke at that.</li>\n<li>The batch size.  I've already, in effect, tried playing with that.  The different\ncloud machines I played with had different amounts of per-GPU VRAM, so supported\ndifferent per-GPU micro-batch sizes.  So I wound up trying batch sizes from\n512 (the same as the original GPT-2 was trained with) down to 104 in the cloud,\nplus my local trains with a batch size of 6.  I did a rough-and-ready calculation\n<a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud#the-results\">at the end of the cloud training post</a>\nwhere I estimated that the ideal batch size might be something like 97.  So, probably\nnot worth much more investigation.</li>\n<li>Exploding gradients.  In one of my local trains, and in three out of the four cloud\ntrains, I had sudden spikes in both training and validation loss.  It generally took\nquite a bit of training -- maybe 10-15% of training time -- to get back on track after\nsome of these, so we had what could be seen as wasted time in the training runs.\nExploding gradients can be fixed by gradient clipping, which is relatively easy to\ndo.  Definitely worth investigating!</li>\n</ul>\n\n<p>I'm going to work through each of those apart from the first two and the batch size (and will retrospectively add links to\nthe posts when I do), trying a train with just that intervention and nothing else, on\na cloud machine.  Once that's done, I'll bake all of the things that helped into\nthe training loop, and do another local train -- with\n<a href=\"https://www.hopsworks.ai/dictionary/gradient-accumulation\">gradient accumulation</a> to make\nthe batch size match the cloud instances'.</p>\n\n<p>The cloud machine size that I decided to use for this was the one that came out the\nmost cost-effective (and due to its VRAM size, had the best loss) in my earlier\ncloud training test: an 8x A100 machine with 40 GiB VRAM per GPU.</p>\n\n<p>But first, we need a baseline model.</p>\n\n<h3 id=\"why-a-new-baseline\">Why a new baseline?</h3>\n\n<p>I've already done a train on an 8x A100 40 GiB machine -- why do we need a new one?</p>\n\n<p>In my cloud training post, I came to the conclusion that the cost in terms of training\ntime of running a periodic validation loop as we trained was not really worth it, at\nleast in this case.  Two of the biggest reasons to have validation during training\nare to work out when you're overfitting on a multi-epoch train, and to see how your model\ncan handle datasets that it has not been trained on.</p>\n\n<p>In a single-epoch train like this,\nyou're not going to overfit -- every sample it sees will be new to it -- and the training\nloss itself is over samples it's not been trained on at the time it was calculated, for the\nsame reason (though of course it will be trained on them as soon as we do the backward\npass starting with that loss).</p>\n\n<p>Of course, it's not perfect --\na big benefit of the validation loss is that it's over the <em>same</em> held-back dataset\non every run -- and there are arguments for keeping it (albeit, perhaps doing full\nruns less frequently than I was).  But for these experiments, I decided that I'd\nsimply drop it.</p>\n\n<p>I also wanted to introduce a consistent random seed at the start of the training\nloop.  I didn't have that in my cloud trains, and of course if we want to have solid\nresults on whether each intervention really does improve matters, then we need one\nso that we can be sure they're all starting from the same point.</p>\n\n<p>Both of those meant that I couldn't use the earlier train on the 8x A100 40 GiB machine\nas a baseline; I'd need a new one, introducing those two changes: no validation during the\ntraining run (using training loss as a proxy), and setting a random seed at the start\nfor reproducibility.</p>\n\n<p>So: what was the baseline train going to look like?</p>\n\n<h3 id=\"creating-the-baseline\">Creating the baseline</h3>\n\n<p>The first step was to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/5175b84262a95b0c0353d784ed9bb6161e9ee882\">strip out the validation code</a>\nand to replace it with code that just took periodic checkpoints, keeping track of\nwhich one had the best average training loss over the period since the previous one.  Next, I\ndecided to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/9ed8cfd5ff40cde47c8c49a730f0571285469cf8\">plot on the training chart</a> that is generated during the run not just\nthe training loss, but also an indicator of the maximum and minimum training loss\nover all of the steps in that period.  Then I <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/6a7e5867f86882050fee058cdf9c5880a0f3fcb1\">added the random seed</a>,\nwhich I set to 42.</p>\n\n<p>A couple of bugfixes, and we were left with <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/tree/dc341a246c13190409d0158765e340d8292a29ae\">this version of the code</a>.</p>\n\n<p>One thing to highlight: in the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/dc341a246c13190409d0158765e340d8292a29ae/runs/8xa100m40-baseline/train.json\"><code>train.json</code></a>\nfile that specifies the various training parameters, I set the per-GPU micro-batch\nsize to 12 rather than the 13 I'd used on this size of machine earlier.\nTwo reasons for that:</p>\n\n<p>Firstly, I'm going to want to do a local run with gradient\naccumulation later, using all of the helpful interventions.\nWith gradient accumulation, you do a number of steps with batches that you can fit\ninto your memory, but you don't update the gradients each time.  After a number of those,\nyou do one big update based on the accumulated gradients -- hence the name.  The full\nbatch is all of those smaller batches taken together.</p>\n\n<p>If I want that to closely match the cloud train, I'll want the accumulated batches to\nbe the same size as each global batch in the cloud.</p>\n\n<p>Now, on my local machine, I can fit a batch of 6 into VRAM.  So that means that the\nfull batch needs to be divisible by 6 <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>.  On the cloud train, with a micro-batch of 13 and 8 GPUs, we had an overall batch\nsize of 104 in the previous train.  104 is not divisible by 6: no joy.  But with a micro-batch size of\n12, we have an overall batch of <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>12</mn><mi>&#x000d7;</mi><mn>8</mn><mo>&#x0003d;</mo><mn>96</mn></mrow></math>, which means we'd be able to do gradient\naccumulation and do a parameter update every <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>96</mn><mi>&#x000f7;</mi><mn>6</mn><mo>&#x0003d;</mo><mn>16</mn></mrow></math> steps.</p>\n\n<p>Secondly, while my estimate of the ideal overall batch size was based on a rather arbitrary\nbit of curve-fitting, it did say that 97 was the ideal size.  So it could be interesting\nto see whether it did help!</p>\n\n<p>So, having coded that up and set up the configuration, it was time to run it.</p>\n\n<p>Here's the training chart it came up with:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>Note the loss spikes at around global steps 4,200, 13,000 and 23,000.  Those are important, I'll explain why later.</p>\n\n<p>The training run reported this at the end:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,243.523 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 266,284 tokens/second</span>\n<span class=\"go\">Final train loss: 3.743</span>\n</code></pre>\n</div>\n\n<p>So it took about 3h24m to train, even less than we expected from the previous\ncloud experiments' estimates of how long it would take excluding validation.  About\nUS$35 in cost.</p>\n\n<p><a href=\"https://huggingface.co/gpjt/8xa100m40-baseline\">Here is the model on Hugging Face Hub</a>.</p>\n\n<p>Let's see how it looks.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>For these intervention posts, I won't run the instruction-following tests, as they\ncan only be run against a batch of models in one go to get results that\n<a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">are consistent with each other</a>.</p>\n\n<p>But the smoke test -- how does it complete the sequence <code>Every effort moves you</code> is worthwhile:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-baseline/model.json<span class=\"w\"> </span>runs/8xa100m40-baseline/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you in on a good cause.</span>\n<span class=\"go\">If it doesn’t work you would like to join the</span>\n</code></pre>\n</div>\n\n<p>Looks good!  Reasonably coherent.</p>\n\n<p>Now we can find the loss on our held-back test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-baseline/model.json<span class=\"w\"> </span>runs/8xa100m40-baseline/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 990.57it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:53&lt;00:00, 10.91it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.692</span>\n</code></pre>\n</div>\n\n<p>That's a bit worse than the 3.674 we got for the original cloud train.  Either\nthe calculations of the optimal batch size I did were not quite right (entirely likely,\nthey were very ad-hoc) or the model weights we started with, given the random seed\nwe're using, just happened to lead us in a slightly worse direction (also plausible).  Either way, it's\nin line with what we expected, and is still better than the test loss of 3.725 that we\ngot with the second-best machine in the cloud comparison post (the 8x H100 80 GiB with a global batch size of 216).</p>\n\n<p>So: we have a solid baseline model -- before we wrap up, let's consider those spikes in\nthe loss that I called out in the training chart.</p>\n\n<h3 id=\"the-loss-spikes\">The loss spikes</h3>\n\n<p>Random spikes in the loss are a Bad Thing, right?  Certainly they're a bad thing for\na train in general, especially if you don't know for sure what's causing them.  But my\nworking assumption has been that they're caused by exploding gradients -- for some\nspecific sample in the dataset, the gradients have gone up to some insanely high value,\nand we've had a bad update to our parameters as a result.  It hasn't completely knocked the model back to its starting\npoint, but it does take some time to recover, so we lose the benefit of some of our\ntraining.</p>\n\n<p>If that is the case -- and it's not just something like a batch happening to have stuff\nthat's wildly different to the rest of the training data, or something weird in the optimiser\n-- then gradient clipping is the solution.  I wanted to see if it\nwould help the model quality in general, but of course if we hadn't had any loss spikes\nin this baseline train it would have been hard to see if that was the case!</p>\n\n<p>So I\nwas very glad to see them here, as if there had been none I would either have had\nto do a gradient clipping experiment with no real expectation of it helping -- or do\nanother baseline train with a different random seed in the hope that that caused some\nspikes, which would have cost another US$35.</p>\n\n<p>All in all, it was good to see them there, as it sets us up well for that experiment.</p>\n\n<h3 id=\"wrapping-up\">Wrapping up</h3>\n\n<p>So, we've trained a baseline model that we can make changes to -- the interventions\nI listed at the start -- and get a pretty reliable understanding of whether or not\nthey help the quality of the final model.  With that in place, we're in a good position\nto start running those intervention tests!</p>\n\n<p>Given the loss spike situation in that chart, I think that a solid first one to go\nfor -- even though it was the last in that list at the top of this post -- is gradient\nclipping.  Where are those loss spikes coming from, and if it's exploding gradients, what happens if we limit the\ndamage they do with gradient clipping?</p>\n\n<p>Stay tuned!  I've already done the training run for that (while I wrote this one up),\nso I should be able to post about it tomorrow.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>Well, you could potentially do something with batches of different sizes, but\nthat would be fiddly.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
          "content": "<p>I'm rounding out my series of posts on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\"\nby seeing how I could train the <em>best</em> base model I can from scratch on my own hardware.\nI started by <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">training one in two days on my RTX 3090</a>,\nand found that while it was a decent little model, it wasn't as good as the original\nGPT-2 small, either in terms of the loss it got on my test dataset, or\nin terms of how good it was at following instruction prompts after fine-tuning on them.  I decided\nthat I wanted to see what levers I could pull -- dropout, attention weight biases, and so\non -- to make it better.</p>\n\n<p>For that, I didn't want to have my PC tied up for days at a time with multiple long training runs,\nso I <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">learned how to train faster in the cloud</a>.\nThat led to some <a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">refinements in the prompt-following test I was using</a>,\nand I also spent a bit of time on a side quest getting the various models I'd trained <a href=\"/2026/01/llm-from-scratch-31-models-on-hugging-face\">onto Hugging Face Hub</a>.</p>\n\n<p>Now it's time to try the various \"interventions\", as I'll call them -- the levers to\npull to see if I can make the model better.  This post is\nto recap what they are, and to describe what I did to establish a baseline model to\ncompare to.</p>\n<h3 id=\"the-interventions\">The interventions</h3>\n\n<p>I listed a number of possible interventions at the end of the RTX 3090 post; I'm not going\nto do them all, but for completeness, here's the full list:</p>\n\n<ul>\n<li>The amount of training data.  I'm not going to dig into this one; it looks like it\ndoes help, but the returns diminish rapidly, so I thi",
          "depth": 0.8,
          "published": "2026-02-04T01:45:00+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 4
    },
    {
      "canonical_name": "人工智能时代决策与创造性的转变",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Saying “No” In an Age of Abundance",
          "link": "https://blog.jim-nielsen.com/2026/saying-no/",
          "source": "blog.jim-nielsen.com",
          "summary": "<p>You’ve probably heard this famous quote from Steve Jobs about saying ‘no’:</p>\n<blockquote>\n<p>People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I’m actually as proud of the things we haven’t done as the things I have done. Innovation is saying no to 1,000 things.</p>\n</blockquote>\n<p>But wait, we have AI now. We don’t have to say no to 1,000 things. We can say yes to all the things — generate them all, simultaneously!</p>\n<p>Do you really have to “pick carefully” when AI can materialize everything you previously would’ve been too constrained to do?</p>\n<p>Generative technology paired with being “data-driven” means it’s easy to build every idea, ship it, measure it, and see what sticks.</p>\n<p>Humans, money, time — these all used to be constraints which required budgets, trade-offs, and decision making.</p>\n<p>Organizations had an incentive to say “no” when development was constrained — “We can only do so much, so let’s make sure we do the most impactful things.”</p>\n<p>But maybe the scarcity of organizational resources was the wrong focus all along?</p>\n<p>It’s never been a good idea to ship everything you think of. Every addition accretes complexity and comes with a cognitive cost.</p>\n<p>Maybe we need to reframe the concept of scarcity from <em>us</em>, the makers of software, to <em>them</em>, the users of software. Their resources are what matter most:</p>\n<ul>\n<li>Attention (too many features and they can’t all be used, or even tried)</li>\n<li>Stability (too much frequent change is an impediment to learning a product)</li>\n<li>Clarity (too many options creates confusion and paralysis)</li>\n<li>Coherence (too many plots and subplots cannot tell a unified story)</li>\n</ul>\n<p>So maybe the way you argue for saying “no” isn’t because it helps <em>you</em> as a business, but because it helps your customers. It helps them make sense of what you’ve made.</p>\n<p>And yet: arguing for customer clarity has always been harder than arguing for internal efficiency or some bottom line.</p>\n<p>In an age of abundance, restraint becomes the only scarce thing left, which means saying “no” is more valuable than ever.</p>\n<p>I’m as proud of the things I haven’t generated as the things I have.</p>\n\n    <hr />\n    \n\n    <p>\n      Reply via:\n      \n\n      <a href=\"mailto:jimniels%2Bblog@gmail.com?subject=Re:%20blog.jim-nielsen.com/2026/saying-no/\">Email</a>\n      · <a href=\"https://mastodon.social/@jimniels\">Mastodon</a> ·\n\n      <a href=\"https://bsky.app/profile/jim-nielsen.com\">Bluesky</a>\n    </p>",
          "content": "<p>You’ve probably heard this famous quote from Steve Jobs about saying ‘no’:</p>\n<blockquote>\n<p>People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I’m actually as proud of the things we haven’t done as the things I have done. Innovation is saying no to 1,000 things.</p>\n</blockquote>\n<p>But wait, we have AI now. We don’t have to say no to 1,000 things. We can say yes to all the things — generate them all, simultaneously!</p>\n<p>Do you really have to “pick carefully” when AI can materialize everything you previously would’ve been too constrained to do?</p>\n<p>Generative technology paired with being “data-driven” means it’s easy to build every idea, ship it, measure it, and see what sticks.</p>\n<p>Humans, money, time — these all used to be constraints which required budgets, trade-offs, and decision making.</p>\n<p>Organizations had an incentive to say “no” when development was constrained — “We can only do so much, so let’s make sure we do the most impactful things.”</p>\n<p>But maybe the scarcity of organizational resources was the wrong focus all along?</p>\n<p>It’s never been a good idea to ship everything you think of. Every addition accretes complexity and comes with a cognitive cost.</p>\n<p>Maybe we need to reframe the concept of scarcity from <em>us</em>, the makers of software, to <em>them</em>, the users of software. Their resources are what matter most:</p>\n<ul>\n<li>Attention (too many features and they can’t all be used, or even tried)</li>\n<li>Stability (too much frequent change is an impediment to learning a product)</li>\n<li>Clarity (too many options creates confusion and paralysis)</li>\n<li>Coherence (too many plots and subplots cannot tell a unified story)</li>\n</ul>\n<p>So maybe the way you argue for saying “no” isn’t because it helps <em>you</em> as a business, but because it helps your customers. It",
          "depth": 0.7,
          "published": "2026-02-03T19:00:00+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 37.0,
      "rank": 5
    },
    {
      "canonical_name": "Anthropic Claude Agent与OpenAI Codex在Xcode中的应用",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Xcode 26.3 ‘Unlocks the Power of Agentic Coding’",
          "link": "https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/",
          "source": "daringfireball.net",
          "summary": "<p>Apple Newsroom:</p>\n\n<blockquote>\n  <p>Xcode 26.3 introduces support for agentic coding, a new way in Xcode for developers to build apps using coding agents such as Anthropic’s Claude Agent and OpenAI’s Codex. With agentic coding, Xcode can work with greater autonomy toward a developer’s goals — from breaking down tasks to making decisions based on the project architecture and using built-in tools.</p>\n</blockquote>\n\n<p>I don’t know if this is super-duper interesting news, but I think it’s super-duper interesting that Apple saw the need to release this now, not at WWDC in June.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2026/02/03/xcode-ai-agentic-coding\" title=\"Permanent link to ‘Xcode 26.3 ‘Unlocks the Power of Agentic Coding’’\">&nbsp;★&nbsp;</a>\n</div>",
          "content": "<p>Apple Newsroom:</p>\n\n<blockquote>\n  <p>Xcode 26.3 introduces support for agentic coding, a new way in Xcode for developers to build apps using coding agents such as Anthropic’s Claude Agent and OpenAI’s Codex. With agentic coding, Xcode can work with greater autonomy toward a developer’s goals — from breaking down tasks to making decisions based on the project architecture and using built-in tools.</p>\n</blockquote>\n\n<p>I don’t know if this is super-duper interesting news, but I think it’s super-duper interesting that Apple saw the need to release this now, not at WWDC in June.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2026/02/03/xcode-ai-agentic-coding\" title=\"Permanent link to ‘Xcode 26.3 ‘Unlocks the Power of Agentic Coding’’\">&nbsp;★&nbsp;</a>\n</div>",
          "depth": 0.7,
          "published": "2026-02-04T01:34:54+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 37.0,
      "rank": 6
    },
    {
      "canonical_name": "纽约科技社区30周年庆祝活动",
      "category": "行业动态",
      "articles": [
        {
          "title": "New York Tech at 30: the Crossroads",
          "link": "https://anildash.com/2026/02/03/nye-tech-30/",
          "source": "anildash.com",
          "summary": "<p>This past week, over a series of events, the New York tech community celebrated the 30th anniversary of a nebulous idea described as “Silicon Alley”, the catch-all term for our greater collective of creators and collaborators, founders and funders, inventors and investors, educators and entrepreneurs and electeds, activists and architects and artists. Some of the parties or mixers have been typical industry affairs, the usual glad-handing about deal-making and pleasantries. But a lot have been deeper, reflecting on what’s special and meaningful about the community we’ve built in New York. <a href=\"https://www.mediapost.com/publications/article/412470/\">Steven Rosenbaum’s reflection</a> on the anniversary captures this well from someone who’s been there, and <a href=\"https://finance.yahoo.com/news/silicon-alley-turns-30-york-114752768.html\">Leo Schwartz’s piece for Fortune</a> covers the more conventional business angle.</p>\n<p>Beyond the celebrations, though, I wanted to reflect on a number of the deeper conversations I’ve had over these last few days. These are conversations grounded in the reality of where our country and city are today, far beyond spaces where wealthy techies are going to parties and celebrating each other. The hard questions raised in these conversations are the ones that determine where this community goes in the future, and they’re the ones that <em>every</em> tech community is going to face in the current moment.</p>\n<p>I know what the New York City tech community has been; there was a time when I was one of its most prominent voices. The question now is what it will be in the future. Because we are at a profound crossroads.</p>\n\n<h1>What community can be</h1>\n<p>Nobody better exemplifies the best of what New York tech has been than Aaron Swartz. As I’d <a href=\"https://www.anildash.com/2026/01/09/how-markdown-took-over-the-world/\">written about</a> recently, he was brilliant and delightfully impossible. At an incredibly young age, <a href=\"https://www.eff.org/deeplinks/2017/01/everyone-made-themselves-hero-remembering-aaron-swartz\">he led our community</a> in the battle to push back against a pair of ill-considered bills that threatened free expression on the Internet. (These bills would have done to the web what the current administration has done to broadcast television, having a chilling effect on free speech and putting large swaths of content under government control.) As we stood outside Chuck Schumer’s office and demanded that big business take their hands off our internet, we got our first glimpse of the immense power that our community could wield. And <a href=\"https://www.eff.org/deeplinks/2017/01/5-years-later-victory-over-sopa-means-more-ever\">we won</a>, at least for a while.</p>\n<p>My own path within the New York tech community was nowhere near as dramatic, but I was just as motivated in wanting to serve the community. When I became the first person <a href=\"https://www.anildash.com/2010/12/13/im-running-for-the-new-york-tech-meetup-board/\">elected to the board of the New York Tech Meetup</a> (later the New York Tech Alliance), it was the largest member-led organization of tech industry workers in the country. By the time it reached its peak, we were over 100,000 members strong, and could sell out one of our monthly events (at a venue of over 1000 attendees) in minutes. The collective power and impact of that cohort was immense. So, when I say “community”, I mean <em>community</em>. I’m not talking about the contemporary usage of the word, when people call their TikTok followers a “community”. I mean people who care about each other and show up for each other so that they can achieve meaningful things.</p>\n<p>New York tech demonstrated its values time and again, and not just in organizing around policy that served its self-interest. When the city was still reeling from 9/11, these were people who not only chose to stay in the city, or who simply talked about how New York ought to rebuild, but actually took the risk and rebuilt the economy of the city — the <em>majority</em> of the economic regrowth and new jobs in New York City in the quarter-century since the attacks of 9/11 have happened thanks to the technology sector.</p>\n<p>When Hurricane Sandy hit, these were people who <a href=\"https://www.nbcnews.com/id/wbna49663102\">were amongst the first to step up</a> to help their neighbors dig out. When our city began to <a href=\"https://www.anildash.com/2011/03/05/nyc-mta-ftw/\">open up its data</a>, the community responded in kind by building an entire ecosystem of new tools that laid the groundwork for the tech we now take for granted when navigating around our neighborhoods. There was no reluctance to talk about the importance of diversity and inclusion, and no apology in saying that tech was failing to do its job in hiring and promoting equitably, because we know how much talent is available in our city. Hackers would come to meetups to show off their startups, sure, but just as often to show off how they’d built cool new technology to <a href=\"https://www.wbur.org/hereandnow/2021/12/28/heat-seek-tool-tenants\">help make sure our neighbors in public housing had heat in the winter</a>. This was <a href=\"https://www.anildash.com/2016/07/15/new-york-style-tech/\">New York-style tech</a>.</p>\n<p>What’s more, the work of this community happened with remarkable solidarity; the SOPA/PIPA protests that Aaron Swartz spoke at had him standing next to some of the most powerful venture capitalists in the city. When it was time to take action, a number of the most influential tech CEOs in New York took Amtrak down to Washington, D.C. to talk to elected officials and their staffers about the importance of defending free expression online, advocating for the same issue that had been so important to the broke college kids who’d been at the rally just a few days earlier. People had actually gathered around <em>principles</em>. I don’t say this as a Pollyanna who thinks everything was perfect, or that things would have always stayed so idealistically aligned, but simply to point out that <em>this did happen</em>. I don’t have to assert that it is theoretically possible, because I have already seen a community which functions in this way.</p>\n<h2>From bottoms-up to big business</h2>\n<p>But things have changed in recent years for New York’s tech community. What used to often be about extending a hand to neighbors has, much of the time, become about simply focusing on who’s getting funded to chase the trends defined by Silicon Valley. The vibrancy of the New York Tech Meetup took a huge hit from covid, preventing the ability for the community to gather in person, and the organization’s evolution from a Meetup to an Alliance to being part of Civic Hall shifted its focus in recent years, though there has been a recent push to revitalize its signature events. In its place, much of the public narrative for the community is led by Tech:NYC, which has active and able leadership, but is a far more conventional trade group. There's a focus on pragmatic tools like job listings (their <a href=\"https://technycdigest.beehiiv.com/subscribe?ref=kdPsdXErYd\">email newsletter</a> is excellent), but they're unlikely to lead a rally in front of a Senator's office. An organization whose founding members include Google and Meta is necessarily going to be different than one with 100,000 individual members.</p>\n<p>When I <a href=\"https://web.archive.org/web/20150601041007/https://www.wsj.com/articles/SB10001424127887324624404578255752537705008\">spoke to the Wall Street Journal</a> back in 2013 about the political and social power of our community, at a far different time, I called out the breadth of who our community includes:</p>\n<blockquote>\n<p>The tech constituency encompasses a range of potential voters who remain unlikely to behave as a traditional bloc. &quot;It's venture capitalists and 23-year-old graphic designers in Bushwick,&quot; Mr. Dash said. &quot;It's labor and management. It's not traditional allies.&quot;</p>\n</blockquote>\n<p>I wanted to make sure people understood that tech in New York is much broader than just, well, what the bosses and the big companies want. It is important to understand that New York is about <a href=\"https://www.anildash.com/2025/10/24/founders-over-funders/\">founders, not just funders</a>.</p>\n<p>The distinction between these groups and their goals was never clearer to me than in the 2017 battle around Amazon’s proposed <a href=\"https://en.wikipedia.org/wiki/Amazon_HQ2\">HQ2 headquarters</a>. The public narrative was that Amazon was trying to make a few cities jump through hoops to make the best possible set of bribes to the company so that they would build a new headquarters complex in the host city. The reality was, New York City offered $1.5 billion dollars to the richest man in the world in order to open up an office in a city where the company was inevitably going to do business regardless, and the contract that Amazon would have to sign in exchange only obligated them to hire 500 new workers in the city — <strong>fewer</strong> people than their typical hiring plan would expect in that timeframe. In addition, the proposed plan would have taken over land intended for 6,000 homes, including 1500 affordable units, would have defunded the mass transit system through years of tax breaks for the company while putting massive additional burden on the transit system, and raised housing prices. (Amazon has since signed a lease for 335,000 square feet and hired over 1000 employees, without any subsidies.)</p>\n<p>At the time, I was CEO of a company that two entrepreneurs had founded in 2000 and bootstrapped to success, leading to them spinning out multiple companies which would go on to exit for over $2.2 billion, providing over 500 jobs and creating dozens of millionaires out of the workers who joined the companies over the years. Several of the people who had worked at those companies went on to form their own companies, and <em>those</em> companies are now collectively worth over $5 billion. All of these companies, combined, have gotten a total of <em>zero billion dollars</em> from the state and city of New York. In addition, none of those companies have ever had working conditions anywhere close to <a href=\"https://en.wikipedia.org/wiki/Criticism_of_Amazon#Treatment_of_workers\">those Amazon has been criticized for</a>.</p>\n<p>But the <em>story</em> of the time was that “New York tech wants HQ2!” Media like newspapers and TV were firmly convinced that techies were in support of Amazon getting a massive unnecessary handout, and I had genuinely struggled to figure out why for a long time. After a while, it became obvious. Everyone that they had spoken to, and all the voices that were considered canonical and credible when talking about “New York tech”, were investors or giant publicly-traded companies.</p>\n<p>People who actually <em>built</em> things were no longer the voice of the community. Those who showed up when the power was out, or when the community was hurting, or when there was an issue that called for someone to bravely stand up and lead the crowd even if there was some social or political risk — they were not considered valid. People liked the <em>myth</em> of Aaron Swartz by then, but they would have ignored the fact that he almost certainly would have objected to corporate subsidy for the company.</p>\n<h2>New York tech today, and tomorrow</h2>\n<p>I am still proud of the New York tech community. But that’s because I get to see what happens in person. Last week, I was reminded at every one of the in-person commemorations of the community that there are so many generous, kind-hearted, thoughtful people who will fight to do the right thing. The challenge today, though, is that those are no longer the people who define the story of the community. That’s not who a <em>new</em> person thinks of when they’re introduced to our community.</p>\n<p>When I talk to young people who are new to the industry, or people who are changing careers who are curious about tech, they have heard of things like Tech Week, or they read trade press. In those venues, a big name is generally not our home-grown founders, or even the “big” success stories of New York tech. That’s especially true as once high-flying New York tech companies like Tumblr and Foursquare and Kickstarter and Etsy and Buzzfeed either faded or got acquired, and newer successful startups are more prosaic and less attention-grabbing. Who’s left to tell them a story of what “tech” means in New York? Where will they find community?</p>\n<p>One possible future is that they try to build a startup, doing everything you’re “supposed” to do. They pitch the VC firms in town, and the big name firms that they’ve heard of. If they’re looking for community, they go to the events that get the most promotion, which might be Tech Week events. And all of these paths lead the same way — the most prominent VC firm is Andreessen Horowitz, and they run Tech Week too, even though they’re not from NYC.</p>\n<p>On that path, New York tech puts you across the table from <a href=\"https://fortune.com/2025/02/05/daniel-penny-andreessen-horowitz-a16z-investing-david-ulevitch/\">the man who strangled my neighbor to death</a>.</p>\n<p>Another possible future is that we rebuild the kind of community that we used to have. We start to get together the people who actually <em>make</em> things, and show off what we’ve built for one another. It’s going to require re-centering the hundreds of thousands of people who create and invent, rather than the dozens of people who write checks. It’s going to mean that the stories start with New York City (and maybe even… <em>in the outer boroughs</em>!), rather than taking dictation from those in Silicon Valley who hate our city. And it’s going to require understanding that technology is a set of tools and tactics we can use in service of goals — ideally positive social goals — and not just an economic opportunity to be extracted from.</p>\n<p>We would never talk about education by only talking to those who invest in making pencils. We’d never consider a story about a new movie to be complete if we only talked to those who funded the film. And certainly our policymakers would balk if we skipped speaking with them and instead aimed our policy questions directly at their financial backers, though that might result in more accurate responses. Yet somehow, with technology, we’ve given over the narrative entirely to the money men.</p>\n<p>In New York, we’ve borne the brunt of that error. A tech community with heart and soul is in danger of being snuffed out by those who will only let its most base instincts survive. Even our <em>investors</em> here are more thoughtful than these stories would make it seem! But we can change it, and maybe even change the larger tech story, if we’re diligent in never letting the bad actors control the narrative of what tech is in the world.</p>\n<p>Like so many good things, it can all start with New York City.</p>",
          "content": "<p>This past week, over a series of events, the New York tech community celebrated the 30th anniversary of a nebulous idea described as “Silicon Alley”, the catch-all term for our greater collective of creators and collaborators, founders and funders, inventors and investors, educators and entrepreneurs and electeds, activists and architects and artists. Some of the parties or mixers have been typical industry affairs, the usual glad-handing about deal-making and pleasantries. But a lot have been deeper, reflecting on what’s special and meaningful about the community we’ve built in New York. <a href=\"https://www.mediapost.com/publications/article/412470/\">Steven Rosenbaum’s reflection</a> on the anniversary captures this well from someone who’s been there, and <a href=\"https://finance.yahoo.com/news/silicon-alley-turns-30-york-114752768.html\">Leo Schwartz’s piece for Fortune</a> covers the more conventional business angle.</p>\n<p>Beyond the celebrations, though, I wanted to reflect on a number of the deeper conversations I’ve had over these last few days. These are conversations grounded in the reality of where our country and city are today, far beyond spaces where wealthy techies are going to parties and celebrating each other. The hard questions raised in these conversations are the ones that determine where this community goes in the future, and they’re the ones that <em>every</em> tech community is going to face in the current moment.</p>\n<p>I know what the New York City tech community has been; there was a time when I was one of its most prominent voices. The question now is what it will be in the future. Because we are at a profound crossroads.</p>\n\n<h1>What community can be</h1>\n<p>Nobody better exemplifies the best of what New York tech has been than Aaron Swartz. As I’d <a href=\"https://www.anildash.com/2026/01/09/how-markdown-took-over-the-world/\">written about</a> recently, he was brilliant and delightfully impossible. At an incredibly young age, <a href",
          "depth": 0.8,
          "published": "2026-02-04T00:00:00+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 7
    },
    {
      "canonical_name": "互联网销售洋葱的创新实践",
      "category": "行业动态",
      "articles": [
        {
          "title": "Underrated ways to change the world, vol. II",
          "link": "https://www.experimental-history.com/p/underrated-ways-to-change-the-world-b64",
          "source": "experimental-history.com",
          "summary": "OR: why you should sell onions on the internet",
          "content": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!0MoL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b5af13-f1c7-4c7c-9c9d-7716b2616a8a_1663x1218.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1066\" src=\"https://substackcdn.com/image/fetch/$s_!0MoL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b5af13-f1c7-4c7c-9c9d-7716b2616a8a_1663x1218.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div>",
          "depth": 0.8,
          "published": "2026-02-03T16:28:19+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 8
    },
    {
      "canonical_name": "微软红mond园区卫星天线历史",
      "category": "行业动态",
      "articles": [
        {
          "title": "Some small stories about the giant satellite dish antenna that was behind Microsoft Building 11",
          "link": "https://devblogs.microsoft.com/oldnewthing/20260203-00/?p=112035",
          "source": "devblogs.microsoft.com/oldnewthing",
          "summary": "<p>A little trivia.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260203-00/?p=112035\">Some small stories about the giant satellite dish antenna that was behind Microsoft Building 11</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
          "content": "<p>Back in the day, if you wandered into the parking area behind Building 11 on the original Redmond Microsoft campus, you would find a very large satellite dish antenna. This antenna was used for receiving video signals, such as cable television feeds for distribution to the Redmond campus. One purpose was to provide cable TV service for internal development and testing to teams like the Windows Media Center team and later the Xbox One team.</p>\n<p>The satellite dish antenna was a Simulsat-5 which was capable of gathering signals from 35 satellites simultaneously. (The record during this particular antenna&#8217;s lifetime was 26 simultaneous satellites.) It was a stationary antenna, not capable of changing its orientation. It went into service in 1997, was upgraded a few times, until it was finally decommissioned in 2017 when all of its tasks had been subsumed by a satellite dish antenna at the Studio C building.</p>\n<p>Fun trivial about the satellite dish antenna:</p>\n<p>In the summer, bees would nest in the feedbox (the thingie at focus of the satellite dish antenna that collected the signal), so you had to be careful when doing work there to avoid getting stung.</p>\n<p>It wasn&#8217;t fun in the winter either, because the enclosure for the electronic equipment (known as the &#8220;doghouse&#8221;) would get filled with spiders who enjoyed the warmth from the equipment.</p>\n<p>Snow had to be kept off the antenna for it to continue receiving signals, so whether or not Microsoft formally declared a snow closure, somebody had to remain on site to clear off the snow.</p>\n<p>In 2007, there was a mystery to be solved: Occasionally, there would be interference that disrupted the signal. After some investigation, it was discovered that the source was electromagnetic interference generated by the pressure washers that were used to clean the parking lot. The water connection port was at the rear of Building 11, right near the satellite dish antenna. The solution was to do",
          "depth": 0.8,
          "published": "2026-02-03T15:00:00+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 9
    },
    {
      "canonical_name": "青少年网络安全教育",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Weekly Update 489",
          "link": "https://www.troyhunt.com/weekly-update-489/",
          "source": "troyhunt.com",
          "summary": "<p>This week I&apos;m in Hong Kong, and the day after recording, I gave the talk shown in the image above at INTERPOL&apos;s Cybercrime Expert Group. I posted a little about this on Facebook and LinkedIn, but thought I&apos;d expand on what really stuck with</p>",
          "content": "<img alt=\"Weekly Update 489\" src=\"https://www.troyhunt.com/content/images/2026/02/Splash-Template@1x_1.jpg\" /><p>This week I&apos;m in Hong Kong, and the day after recording, I gave the talk shown in the image above at INTERPOL&apos;s Cybercrime Expert Group. I posted a little about this on Facebook and LinkedIn, but thought I&apos;d expand on what really stuck with me after watching other speakers: the effort agencies are putting into cybercrime prevention. It&apos;s very easy for folks to judge law enforcement solely on what they see from the outside, and that&apos;s mostly going after offenders and taking down criminal infrastructure. But the bit I&apos;m increasingly seeing behind the scenes is a push to help kids (the sorts of hackers I usually interact with are teenagers or young adults at most) make better choices when they&apos;re faced with a pathway into cybercrime. The transition from minor offences (game cheats and DDoS&apos;ing) to full-on cybercriminals (hacking and extortion) is very well-known, and intervening at the right time can not only make a difference to the impact of data breaches on all of us, but it can also make a massive difference to these kids&apos; lives. These agencies are underfunded and understaffed compared to the scale of the problem, so making the time to come visit and find some ways to help in our little corner of the data breach world is a no-brainer &#x1f60a;</p>\n<!--kg-card-begin: html-->\n<div><div style=\"width: 170px; display: inline-block; margin-right: 3px;\"><a href=\"https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699?ref=troy-hunt\"><img alt=\"Weekly Update 489\" src=\"https://www.troyhunt.com/content/images/2018/05/Listen-on-Apple-Podcasts.svg\" /></a></div><div style=\"width: 175px; display: inline-block; margin-right: 3px;\"><a href=\"https://www.youtube.com/playlist?list=PL7LAAxaabizMAXnJe0s3xjQ30q12EVmjt&amp;ref=troyhunt.com\"><img alt=\"Weekly Update 489\" src=\"https://www.troyhunt.com/content/im",
          "depth": 0.6,
          "published": "2026-02-04T02:31:18+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.6,
      "heat_score": 34.0,
      "rank": 10
    },
    {
      "canonical_name": "微软初创阶段",
      "category": "行业动态",
      "articles": [
        {
          "title": "When Bill Gates claimed to work for $2 an hour",
          "link": "https://dfarq.homeip.net/when-bill-gates-claimed-to-work-for-2-an-hour/?utm_source=rss&utm_medium=rss&utm_campaign=when-bill-gates-claimed-to-work-for-2-an-hour",
          "source": "dfarq.homeip.net",
          "summary": "<p>The most popular software product for the MITS Altair 8800 computer was Altair Basic, the first Microsoft product. But there was a problem. Only about 10 percent of Altair owners paid for Altair Basic. On February 3, 1976, Bill Gates</p>\n<p>The post <a href=\"https://dfarq.homeip.net/when-bill-gates-claimed-to-work-for-2-an-hour/\">When Bill Gates claimed to work for $2 an hour</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
          "content": "<p>The most popular software product for the MITS Altair 8800 computer was Altair Basic, the first Microsoft product. But there was a problem. Only about 10 percent of Altair owners paid for Altair Basic. On February 3, 1976, Bill Gates</p>\n<p>The post <a href=\"https://dfarq.homeip.net/when-bill-gates-claimed-to-work-for-2-an-hour/\">When Bill Gates claimed to work for $2 an hour</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
          "depth": 0.6,
          "published": "2026-02-03T12:00:34+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.6,
      "heat_score": 29.0,
      "rank": 11
    },
    {
      "canonical_name": "国防部高层领导变动",
      "category": "行业动态",
      "articles": [
        {
          "title": "Making the Wrong Things Go Faster at The Department of War",
          "link": "https://steveblank.com/2026/02/03/making-the-wrong-things-go-faster-at-the-department-of-war/",
          "source": "steveblank.com",
          "summary": "This article previously appeared in Defense Scoop The Department of War (DoW) senior Acquisition leadership (the people who decide what and how the DoW buys equipment and services) now is headed by people from private capital (venture capital and private equity.)  Deputy Secretary of War Steven Feinberg ran Cerebus Capital Secretary of the Army Daniel [&#8230;]",
          "content": "<p><a href=\"https://defensescoop.com/2026/01/30/making-the-wrong-things-go-faster-at-the-department-of-war-steve-blank-op-ed/\"><img alt=\"\" class=\"alignleft wp-image-33492 size-thumbnail\" height=\"21\" src=\"https://i0.wp.com/steveblank.com/wp-content/uploads/2026/02/defense-scoop.jpg?resize=150%2C21&#038;ssl=1\" width=\"150\" /></a></p>\n<p><em>This article previously appeared in <a href=\"https://defensescoop.com/2026/01/30/making-the-wrong-things-go-faster-at-the-department-of-war-steve-blank-op-ed/\" rel=\"noopener\" target=\"_blank\">Defense Scoop</a></em></p>\n<p><span style=\"font-weight: 400;\">The Department of War (DoW) senior Acquisition leadership (the people who decide what and how the DoW buys equipment and services) now is headed by people from private capital (venture capital and private equity.) </span></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Deputy Secretary of War </span><a href=\"https://history.defense.gov/DOD-History/Deputy-Secretaries-of-Defense/Article-View/Article/4244309/stephen-a-feinberg/\"><span style=\"font-weight: 400;\">Steven Feinberg</span></a><span style=\"font-weight: 400;\"> ran Cerebus Capital</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Secretary of the Army </span><a href=\"https://www.army.mil/leaders/sa#org-bio\"><span style=\"font-weight: 400;\">Daniel Driscoll</span></a><span style=\"font-weight: 400;\"> was a former VC and investment banker</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Secretary of the Navy </span><a href=\"https://www.navy.mil/Leadership/Flag-Officer-Biographies/BioDisplay/Article/4136042/the-honorable-john-phelan/\"><span style=\"font-weight: 400;\">John Phelan</span></a><span style=\"font-weight: 400;\"> ran MSD capital.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Deputy Secretary of the Army </span><a href=\"https://www.linkedin.com/in/mike-obadal-136495126/\"><span style=\"font-weight: 400;\">Michael Obadal</span></a><span",
          "depth": 0.6,
          "published": "2026-02-03T14:00:08+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.6,
      "heat_score": 29.0,
      "rank": 12
    },
    {
      "canonical_name": "波兰谚语‘Nie mój cyrk, nie moje małpy’解读",
      "category": "文化研究",
      "articles": [
        {
          "title": "Polish serenity",
          "link": "https://www.johndcook.com/blog/2026/02/03/polish-serenity/",
          "source": "johndcook.com",
          "summary": "<p>Yesterday I ran across the following mashup by Amy Swearer of a Polish proverb and the Serenity Prayer. Lord, grant me the serenity to accept when it&#8217;s no longer my circus, the courage to control the monkeys that are still mine, and the wisdom to know the difference. The proverb is &#8220;Nie mój cyrk, nie [&#8230;]</p>\nThe post <a href=\"https://www.johndcook.com/blog/2026/02/03/polish-serenity/\">Polish serenity</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
          "content": "<p>Yesterday I ran across the following mashup by <a href=\"https://x.com/AmySwearer/status/2018363836336607464\">Amy Swearer</a> of a Polish proverb and the Serenity Prayer.</p>\n<blockquote><p>Lord, grant me the serenity to accept when it&#8217;s no longer my circus,<br />\nthe courage to control the monkeys that are still mine,<br />\nand the wisdom to know the difference.</p></blockquote>\n<p>The proverb is &#8220;<span lang=\"pl\">Nie mój cyrk, nie moje małpy</span>,&#8221; literally &#8220;Not my circus, not my monkeys&#8221;.</p>The post <a href=\"https://www.johndcook.com/blog/2026/02/03/polish-serenity/\">Polish serenity</a> first appeared on <a href=\"https://www.johndcook.com/blog\">John D. Cook</a>.",
          "depth": 0.6,
          "published": "2026-02-03T12:56:58+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.6,
      "heat_score": 29.0,
      "rank": 13
    },
    {
      "canonical_name": "作家近期活动与作品发布",
      "category": "行业动态",
      "articles": [
        {
          "title": "Pluralistic: Michael Swanwick's \"The Universe Box\" (03 Feb 2026)",
          "link": "https://pluralistic.net/2026/02/03/the-last-days-of-old-night/",
          "source": "pluralistic.net",
          "summary": "Today's links Michael Swanwick's \"The Universe Box\": Short stories from a science fiction master at the top of his form. Hey look at this: Delights to delectate. Object permanence: DRM lobotomizes “human memory”; Crayola hex values; Tattoo artists copyright customers' bodies. Upcoming appearances: Where to find me. Recent appearances: Where I've been. Latest books: You keep readin' em, I'll keep writin' 'em. Upcoming books: Like I said, I'll keep writin' 'em. Colophon: All the rest. Michael Swanwick's \"The Universe Box\" (permalink) No one writes short stories like Michael Swanwick, the five-time Hugo-winning master of science fiction. To prove it, you need only pick up The Universe Box, Swanwick's just-published short story collection, a book representing one of the field's greatest writers at the absolute pinnacle of his game: https://tachyonpublications.com/product/the-universe-box/ Science fiction has a long and honorable history with the short story. Sf is a pulp literature that was born in the pages of magazines specializing in short fiction and serials, and long after other genres had given up the ghost, sf remained steadfastly rooted in short form fiction. There are still, to this day, multiple sf magazines that publish short stories every month, on paper, and pay for it. I started my career as a short story writer, and continue to dabble in the form, but I have mostly moved onto novels. That's a pretty common trajectory in sf, where &#8211; notwithstanding the field's status as a haven for the short story &#8211; the reach (and money) come from novels. But sf has always had a cohort of short fiction writers who are staunchly committed to the form: Harlan Ellison, Martha Soukup, Martha Wells, Ray Bradbury, Ted Chiang, James Tiptree Jr, Theodore Sturgeon, and, of course, Michael Swanwick. It's a little weird, how sf serves as a powerful redoubt for short fiction. After all, sf is a genre in which everything is up for grabs: the reader can't assume anything about the story's setting, its era, the species of its characters. Time can run forwards, backwards, or in a loop. There can be gods and teleporters, faster-than-light drives and superintelligent machines. There can be aliens and space colonies. All of that has to be established in the story. The most straightforward way to do this is, of course, through exposition. There's a commonplace (and wrong) notion that exposition is bad (\"show, don't tell\"). It's fairer to say that exposition is hard &#8211; dramatization is, well, dramatic, which makes it easier to engage the reader's attention. But great exposition is great and sf is a genre that celebrates exposition, done well: https://maryrobinettekowal.com/journal/my-favorite-bit/my-favorite-bit-cory-doctorow-talks-about-the-bezzle/ The opposite of exposition is what Jo Walton calls \"incluing,\" \"the process of scattering information seamlessly through the text, as opposed to stopping the story to impart the information\": https://web.archive.org/web/20111119145140/http:/papersky.livejournal.com/324603.html Incluing is a beautiful prose technique, but it makes the reader work. You have to pay close attention to all these subtle clues and build a web of inferences about the kind of world you've been plunged into. Incluing turns a story into a (wonderful and engaging) puzzle. It makes the aesthetic affect of short sf into something that's not so much a reverie as a high-engagement activity, a mystery whose solution is totally unbounded. This is a terrific experience, but it is also work. Doing that kind of work as part of the process of consuming a 300-page novel is one thing, but trying to get the reader up to speed in a 7,000 word story and still have room left over for the story part is a big lift, and even the best writers end up asking a lot of the reader in their short stories. Sf shorts can be the \"difficult jazz\" of literature, a form and genre that requires &#8211; and rewards &#8211; very active attention. (Incidentally, my favorite incluing example is Mark Twain's classic comedic short, \"The Petrified Man\":) https://americanliterature.com/author/mark-twain/short-story/the-petrified-man/ But here's the thing. None of this applies to Swanwick. His stories use a mix of (impeccable) exposition and (subtle) incluing, and yet, there's never a moment in reading a Swanwick story where it feels like work. It's not merely that he's a gorgeous prose-smith whose sentences are each more surpassingly lovely than the last (though he is). Nor does he lack ambition: each of these stories has a more embroidered and outlandish premise than the last. Somehow, though, he just slides these stories into your brain. And what stories they are! They are, by turns, individually and in combination, slapstick, grave, horny, hilarious, surreal, disturbing and heartwarming. They have surprise endings and surprise middles and sometimes surprise beginnings (Swanwick does an opening paragraph like no one else). This is what it means to read a short story collection from an absolute master at the absolute peak of his powers. He can slide you frictionlessly between Icelandic troll tragedies to lethal drone-leopard romantic agonies to battles of the gods and the cigar box that has the universe inside of it. All with the lyricism of Bradbury, the madcap wit of Sturgeon, the unrelenting weirdness of Dick, the heart of Tiptree and the precision of Chiang. This is a book of worlds that each exist for just a handful of pages but occupy more space than those pages could possibly contain. It's a series of cigar boxes, each with the universe inside of it. Hey look at this (permalink) U.S. Envoys Refused to Report \"Apocalyptic\" Conditions in Gaza. Exclusive Photos Show the Reality They Suppressed https://www.dropsitenews.com/p/northern-gaza-apocalyptic-wasteland-jack-lew-israeli-war-supressed To Avoid a Tax Hike, Billionaires Decide to Take Over California https://prospect.org/2026/02/02/billionaires-california-tax-hike/ Mentioned in Hell’s Dispatches https://ftrain.com/mentioned-in-satans-dispatches MAGA's \"People's Capitalism\" https://www.unpopularfront.news/p/magas-peoples-capitalism The Onion’s Exclusive Interview With Pete Hegseth https://theonion.com/the-onions-exclusive-interview-with-pete-hegseth/ Object permanence (permalink) #20yrsago Sony CD spyware vendor caves to EFF demands https://web.archive.org/web/20060208033113/https://www.eff.org/news/archives/2006_02.php#004378 #20yrsago British Library: DRM lobotomizes “human memory” http://news.bbc.co.uk/2/hi/technology/4675280.stm #15yrsago Hex values for Crayola colors https://en.wikipedia.org/wiki/List_of_Crayola_crayon_colors #15yrsago Michael Lewis explains the Irish econopocalypse https://www.vanityfair.com/news/2011/03/michael-lewis-ireland-201103?currentPage=all #15yrsago Canada’s Internet rescued from weak and pathetic regulator https://web.archive.org/web/20110203054651/http://www.thestar.com/news/canada/article/932571&#8211;ottawa-threatens-to-reverse-crtc-decision-on-internet-billing #10yrsago Tattoo artist asserts copyright over customers’ bodies https://www.hollywoodreporter.com/business/business-news/nba-2k-videogame-maker-sued-861131/ #10yrsago EU plans to class volunteers who rescue drowning Syrian refugees as “traffickers” https://www.statewatch.org/news/2016/january/refugee-crisis-council-proposals-on-migrant-smuggling-would-criminalise-humanitarian-assistance-by-civil-society-local-people-and-volunteers-greece-ngos-and-volunteers-have-to-register-with-the-police-and-be-vetted/ Upcoming appearances (permalink) Salt Lake City: Enshittification at the Utah Museum of Fine Arts (Tanner Humanities Center), Feb 18 https://tanner.utah.edu/center-events/cory-doctorow/ Montreal (remote): Fedimtl, Feb 24 https://fedimtl.ca/ Victoria: 28th Annual Victoria International Privacy &#38; Security Summit, Mar 3-5 https://www.rebootcommunications.com/event/vipss2026/ Berkeley: Bioneers keynote, Mar 27 https://conference.bioneers.org/ Berlin: Re:publica, May 18-20 https://re-publica.com/de/news/rp26-sprecher-cory-doctorow Berlin: Enshittification at Otherland Books, May 19 https://www.otherland-berlin.de/de/event-details/cory-doctorow.html Hay-on-Wye: HowTheLightGetsIn, May 22-25 https://howthelightgetsin.org/festivals/hay/big-ideas-2 Recent appearances (permalink) How the Internet Got Worse (Masters in Business) https://www.youtube.com/watch?v=auXlkuVhxMo Enshittification (Jon Favreau/Offline): https://crooked.com/podcast/the-enshittification-of-the-internet-with-cory-doctorow/ Why Big Tech is a Trap for Independent Creators (Stripper News) https://www.youtube.com/watch?v=nmYDyz8AMZ0 Enshittification (Creative Nonfiction podcast) https://brendanomeara.com/episode-507-enshittification-author-cory-doctorow-believes-in-a-new-good-internet/ Enshittification with Plutopia https://plutopia.io/cory-doctorow-enshittification/ Latest books (permalink) \"Canny Valley\": A limited edition collection of the collages I create for Pluralistic, self-published, September 2025 \"Enshittification: Why Everything Suddenly Got Worse and What to Do About It,\" Farrar, Straus, Giroux, October 7 2025 https://us.macmillan.com/books/9780374619329/enshittification/ \"Picks and Shovels\": a sequel to \"Red Team Blues,\" about the heroic era of the PC, Tor Books (US), Head of Zeus (UK), February 2025 (https://us.macmillan.com/books/9781250865908/picksandshovels). \"The Bezzle\": a sequel to \"Red Team Blues,\" about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (thebezzle.org). \"The Lost Cause:\" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (http://lost-cause.org). \"The Internet Con\": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (http://seizethemeansofcomputation.org). Signed copies at Book Soup (https://www.booksoup.com/book/9781804291245). \"Red Team Blues\": \"A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before.\" Tor Books http://redteamblues.com. \"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin\", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 https://chokepointcapitalism.com Upcoming books (permalink) \"Unauthorized Bread\": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026 \"Enshittification, Why Everything Suddenly Got Worse and What to Do About It\" (the graphic novel), Firstsecond, 2026 \"The Memex Method,\" Farrar, Straus, Giroux, 2026 \"The Reverse-Centaur's Guide to AI,\" a short book about being a better AI critic, Farrar, Straus and Giroux, June 2026 Colophon (permalink) Today's top sources: Currently writing: \"The Post-American Internet,\" a sequel to \"Enshittification,\" about the better world the rest of us get to have now that Trump has torched America (1053 words today, 20644 total) \"The Reverse Centaur's Guide to AI,\" a short book for Farrar, Straus and Giroux about being an effective AI critic. LEGAL REVIEW AND COPYEDIT COMPLETE. \"The Post-American Internet,\" a short book about internet policy in the age of Trumpism. PLANNING. A Little Brother short story about DIY insulin PLANNING This work &#8211; excluding any serialized fiction &#8211; is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net. https://creativecommons.org/licenses/by/4.0/ Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution. How to get Pluralistic: Blog (no ads, tracking, or data-collection): Pluralistic.net Newsletter (no ads, tracking, or data-collection): https://pluralistic.net/plura-list Mastodon (no ads, tracking, or data-collection): https://mamot.fr/@pluralistic Medium (no ads, paywalled): https://doctorow.medium.com/ Twitter (mass-scale, unrestricted, third-party surveillance and advertising): https://twitter.com/doctorow Tumblr (mass-scale, unrestricted, third-party surveillance and advertising): https://mostlysignssomeportents.tumblr.com/tagged/pluralistic \"When life gives you SARS, you make sarsaparilla\" -Joey \"Accordion Guy\" DeVilla READ CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies (\"BOGUS AGREEMENTS\") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer. ISSN: 3066-764X",
          "content": "<p><!--\nTags:\nbooks, reviews, science fiction, short fiction, short stories, michael swanwick, gift guide\n\nSummary:\nMichael Swanwick's \"The Universe Box\"; Hey look at this; Upcoming appearances; Recent appearances; Latest books; Upcoming books\n\nURL:\nhttps://pluralistic.net/2026/02/23/the-last-days-of-old-night/\n\nTitle:\nPluralistic: Michael Swanwick's \"The Universe Box\" (03 Feb 2026) the-last-days-of-old-night\n\nBullet:\n&#x1f420;\n\nSeparator:\n->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->\n\nTop Sources:\nNone\n\n--><br />\n<a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/\"><img class=\"xmasthead_link\" src=\"https://i0.wp.com/craphound.com/images/03Feb2026.jpg?w=840&#038;ssl=1\" /></a></p>\n<h1 class=\"toch1\">Today's links</h1>\n<ul class=\"toc\">\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#nirvana-or-bust\">Michael Swanwick's \"The Universe Box\"</a>: Short stories from a science fiction master at the top of his form.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#linkdump\">Hey look at this</a>: Delights to delectate.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#retro\">Object permanence</a>: DRM lobotomizes “human memory”; Crayola hex values; Tattoo artists copyright customers' bodies.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#upcoming\">Upcoming appearances</a>: Where to find me.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#recent\">Recent appearances</a>: Where I've been.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#latest\">Latest books</a>: You keep readin' em, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/23/the-last-days-of-old-night/#upcoming-books\">Upcoming books</a>: Like I said, I'll keep writin' 'em.\n</li>\n<li ",
          "depth": 0.5,
          "published": "2026-02-03T14:16:29+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.5,
      "heat_score": 26.0,
      "rank": 14
    }
  ]
}