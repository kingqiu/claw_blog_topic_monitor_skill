{
  "timestamp": "2026-02-05 早间",
  "total_topics": 14,
  "topics": [
    {
      "canonical_name": "从零开始构建大型语言模型的学习过程",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Writing an LLM from scratch, part 32a -- Interventions: training a baseline model",
          "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32a-interventions-baseline-model",
          "source": "gilesthomas.com",
          "summary": "<p>I'm rounding out my series of posts on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\"\nby seeing how I could train the <em>best</em> base model I can from scratch on my own hardware.\nI started by <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">training one in two days on my RTX 3090</a>,\nand found that while it was a decent little model, it wasn't as good as the original\nGPT-2 small, either in terms of the loss it got on my test dataset, or\nin terms of how good it was at following instruction prompts after fine-tuning on them.  I decided\nthat I wanted to see what levers I could pull -- dropout, attention weight biases, and so\non -- to make it better.</p>\n\n<p>For that, I didn't want to have my PC tied up for days at a time with multiple long training runs,\nso I <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">learned how to train faster in the cloud</a>.\nThat led to some <a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">refinements in the prompt-following test I was using</a>,\nand I also spent a bit of time on a side quest getting the various models I'd trained <a href=\"/2026/01/llm-from-scratch-31-models-on-hugging-face\">onto Hugging Face Hub</a>.</p>\n\n<p>Now it's time to try the various \"interventions\", as I'll call them -- the levers to\npull to see if I can make the model better.  This post is\nto recap what they are, and to describe what I did to establish a baseline model to\ncompare to.</p>\n<h3 id=\"the-interventions\">The interventions</h3>\n\n<p>I listed a number of possible interventions at the end of the RTX 3090 post; I'm not going\nto do them all, but for completeness, here's the full list:</p>\n\n<ul>\n<li>The amount of training data.  I'm not going to dig into this one; it looks like it\ndoes help, but the returns diminish rapidly, so I think that in order to get any serious\nimprovement we'd need to train for much more than two days locally.  In the one\n\"extended training\" test I did, I managed to get the loss down from 4.167 to 4.135, which\nwas... less-than-inspiring.</li>\n<li>The number of epochs.  I'm going to stick to single-epoch training -- that is, I'll\ntrain on a single pass through an amount of non-repeating data chosen to take 48 hours to handle\non my local machine.</li>\n<li>The bias on the <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math> matrices.  This one definitely sounds worth looking\ninto -- easy, as it's just a change to a config flag, and makes the model more like the\noriginal GPT-2.  I'll give that a go.</li>\n<li>Dropout.  I've read that for single-epoch training, dropout doesn't help (which\ndoesn't quite work with <a href=\"/2025/03/dropout-and-mandatory-vacation\">my mental model</a> of what it's for, but\ndoes sound plausible).  Worth a look!</li>\n<li>The learning rate, and weight decay.  The values I've used for these are basically copypasta\nfrom the book.  I think I should learn to understand these and try to optimise them a bit.</li>\n<li>The precision.  I'm using <a href=\"https://docs.pytorch.org/docs/stable/amp.html\">AMP</a>, which means that\nsome calculations are done in 16-bit rather than 32-bit, and calling\n<a href=\"https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\"><code>set_float32_matmul_precision</code></a>\nwith <code>\"high\"</code> to let PyTorch choose to use the GPU's tensor cores, which use TF32, a kind of \"32-bit float lite\" (see the\n<a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">post on the local train</a> for details).\nThose both (at least potentially) reduce the precision of\nthe train below what you'd get if you trained with full-fat <code>float32</code>.  Would reverting\nthat be worth the longer train time?  I should probably at least poke at that.</li>\n<li>The batch size.  I've already, in effect, tried playing with that.  The different\ncloud machines I played with had different amounts of per-GPU VRAM, so supported\ndifferent per-GPU micro-batch sizes.  So I wound up trying batch sizes from\n512 (the same as the original GPT-2 was trained with) down to 104 in the cloud,\nplus my local trains with a batch size of 6.  I did a rough-and-ready calculation\n<a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud#the-results\">at the end of the cloud training post</a>\nwhere I estimated that the ideal batch size might be something like 97.  So, probably\nnot worth much more investigation.</li>\n<li>Exploding gradients.  In one of my local trains, and in three out of the four cloud\ntrains, I had sudden spikes in both training and validation loss.  It generally took\nquite a bit of training -- maybe 10-15% of training time -- to get back on track after\nsome of these, so we had what could be seen as wasted time in the training runs.\nExploding gradients can be fixed by gradient clipping, which is relatively easy to\ndo.  Definitely worth investigating!  <a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">Here's the post on that</a>.</li>\n</ul>\n\n<p>I'm going to work through each of those apart from the first two and the batch size (and will retrospectively add links to\nthe posts when I do), trying a train with just that intervention and nothing else, on\na cloud machine.  Once that's done, I'll bake all of the things that helped into\nthe training loop, and do another local train -- with\n<a href=\"https://www.hopsworks.ai/dictionary/gradient-accumulation\">gradient accumulation</a> to make\nthe batch size match the cloud instances'.</p>\n\n<p>The cloud machine size that I decided to use for this was the one that came out the\nmost cost-effective (and due to its VRAM size, had the best loss) in my earlier\ncloud training test: an 8x A100 machine with 40 GiB VRAM per GPU.</p>\n\n<p>But first, we need a baseline model.</p>\n\n<h3 id=\"why-a-new-baseline\">Why a new baseline?</h3>\n\n<p>I've already done a train on an 8x A100 40 GiB machine -- why do we need a new one?</p>\n\n<p>In my cloud training post, I came to the conclusion that the cost in terms of training\ntime of running a periodic validation loop as we trained was not really worth it, at\nleast in this case.  Two of the biggest reasons to have validation during training\nare to work out when you're overfitting on a multi-epoch train, and to see how your model\ncan handle datasets that it has not been trained on.</p>\n\n<p>In a single-epoch train like this,\nyou're not going to overfit -- every sample it sees will be new to it -- and the training\nloss itself is over samples it's not been trained on at the time it was calculated, for the\nsame reason (though of course it will be trained on them as soon as we do the backward\npass starting with that loss).</p>\n\n<p>Of course, it's not perfect --\na big benefit of the validation loss is that it's over the <em>same</em> held-back dataset\non every run -- and there are arguments for keeping it (albeit, perhaps doing full\nruns less frequently than I was).  But for these experiments, I decided that I'd\nsimply drop it.</p>\n\n<p>I also wanted to introduce a consistent random seed at the start of the training\nloop.  I didn't have that in my cloud trains, and of course if we want to have solid\nresults on whether each intervention really does improve matters, then we need one\nso that we can be sure they're all starting from the same point.</p>\n\n<p>Both of those meant that I couldn't use the earlier train on the 8x A100 40 GiB machine\nas a baseline; I'd need a new one, introducing those two changes: no validation during the\ntraining run (using training loss as a proxy), and setting a random seed at the start\nfor reproducibility.</p>\n\n<p>So: what was the baseline train going to look like?</p>\n\n<h3 id=\"creating-the-baseline\">Creating the baseline</h3>\n\n<p>The first step was to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/5175b84262a95b0c0353d784ed9bb6161e9ee882\">strip out the validation code</a>\nand to replace it with code that just took periodic checkpoints, keeping track of\nwhich one had the best average training loss over the period since the previous one.  Next, I\ndecided to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/9ed8cfd5ff40cde47c8c49a730f0571285469cf8\">plot on the training chart</a> that is generated during the run not just\nthe training loss, but also an indicator of the maximum and minimum training loss\nover all of the steps in that period.  Then I <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/6a7e5867f86882050fee058cdf9c5880a0f3fcb1\">added the random seed</a>,\nwhich I set to 42.</p>\n\n<p>A couple of bugfixes, and we were left with <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/tree/dc341a246c13190409d0158765e340d8292a29ae\">this version of the code</a>.</p>\n\n<p>One thing to highlight: in the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/dc341a246c13190409d0158765e340d8292a29ae/runs/8xa100m40-baseline/train.json\"><code>train.json</code></a>\nfile that specifies the various training parameters, I set the per-GPU micro-batch\nsize to 12 rather than the 13 I'd used on this size of machine earlier.\nTwo reasons for that:</p>\n\n<p>Firstly, I'm going to want to do a local run with gradient\naccumulation later, using all of the helpful interventions.\nWith gradient accumulation, you do a number of steps with batches that you can fit\ninto your memory, but you don't update the gradients each time.  After a number of those,\nyou do one big update based on the accumulated gradients -- hence the name.  The full\nbatch is all of those smaller batches taken together.</p>\n\n<p>If I want that to closely match the cloud train, I'll want the accumulated batches to\nbe the same size as each global batch in the cloud.</p>\n\n<p>Now, on my local machine, I can fit a batch of 6 into VRAM.  So that means that the\nfull batch needs to be divisible by 6 <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>.  On the cloud train, with a micro-batch of 13 and 8 GPUs, we had an overall batch\nsize of 104 in the previous train.  104 is not divisible by 6: no joy.  But with a micro-batch size of\n12, we have an overall batch of <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>12</mn><mi>&#x000d7;</mi><mn>8</mn><mo>&#x0003d;</mo><mn>96</mn></mrow></math>, which means we'd be able to do gradient\naccumulation and do a parameter update every <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>96</mn><mi>&#x000f7;</mi><mn>6</mn><mo>&#x0003d;</mo><mn>16</mn></mrow></math> steps.</p>\n\n<p>Secondly, while my estimate of the ideal overall batch size was based on a rather arbitrary\nbit of curve-fitting, it did say that 97 was the ideal size.  So it could be interesting\nto see whether it did help!</p>\n\n<p>So, having coded that up and set up the configuration, it was time to run it.</p>\n\n<p>Here's the training chart it came up with:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>Note the loss spikes at around global steps 4,200, 13,000 and 23,000.  Those are important, I'll explain why later.</p>\n\n<p>The training run reported this at the end:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,243.523 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 266,284 tokens/second</span>\n<span class=\"go\">Final train loss: 3.743</span>\n</code></pre>\n</div>\n\n<p>So it took about 3h24m to train, even less than we expected from the previous\ncloud experiments' estimates of how long it would take excluding validation.  About\nUS$35 in cost.</p>\n\n<p><a href=\"https://huggingface.co/gpjt/8xa100m40-baseline\">Here is the model on Hugging Face Hub</a>.</p>\n\n<p>Let's see how it looks.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>For these intervention posts, I won't run the instruction-following tests, as they\ncan only be run against a batch of models in one go to get results that\n<a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">are consistent with each other</a>.</p>\n\n<p>But the smoke test -- how does it complete the sequence <code>Every effort moves you</code> is worthwhile:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-baseline/model.json<span class=\"w\"> </span>runs/8xa100m40-baseline/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you in on a good cause.</span>\n<span class=\"go\">If it doesn’t work you would like to join the</span>\n</code></pre>\n</div>\n\n<p>Looks good!  Reasonably coherent.</p>\n\n<p>Now we can find the loss on our held-back test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-baseline/model.json<span class=\"w\"> </span>runs/8xa100m40-baseline/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 990.57it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:53&lt;00:00, 10.91it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.692</span>\n</code></pre>\n</div>\n\n<p>That's a bit worse than the 3.674 we got for the original cloud train.  Either\nthe calculations of the optimal batch size I did were not quite right (entirely likely,\nthey were very ad-hoc) or the model weights we started with, given the random seed\nwe're using, just happened to lead us in a slightly worse direction (also plausible).  Either way, it's\nin line with what we expected, and is still better than the test loss of 3.725 that we\ngot with the second-best machine in the cloud comparison post (the 8x H100 80 GiB with a global batch size of 216).</p>\n\n<p>So: we have a solid baseline model -- before we wrap up, let's consider those spikes in\nthe loss that I called out in the training chart.</p>\n\n<h3 id=\"the-loss-spikes\">The loss spikes</h3>\n\n<p>Random spikes in the loss are a Bad Thing, right?  Certainly they're a bad thing for\na train in general, especially if you don't know for sure what's causing them.  But my\nworking assumption has been that they're caused by exploding gradients -- for some\nspecific sample in the dataset, the gradients have gone up to some insanely high value,\nand we've had a bad update to our parameters as a result.  It hasn't completely knocked the model back to its starting\npoint, but it does take some time to recover, so we lose the benefit of some of our\ntraining.</p>\n\n<p>If that is the case -- and it's not just something like a batch happening to have stuff\nthat's wildly different to the rest of the training data, or something weird in the optimiser\n-- then gradient clipping is the solution.  I wanted to see if it\nwould help the model quality in general, but of course if we hadn't had any loss spikes\nin this baseline train it would have been hard to see if that was the case!</p>\n\n<p>So I\nwas very glad to see them here, as if there had been none I would either have had\nto do a gradient clipping experiment with no real expectation of it helping -- or do\nanother baseline train with a different random seed in the hope that that caused some\nspikes, which would have cost another US$35.</p>\n\n<p>All in all, it was good to see them there, as it sets us up well for that experiment.</p>\n\n<h3 id=\"wrapping-up\">Wrapping up</h3>\n\n<p>So, we've trained a baseline model that we can make changes to -- the interventions\nI listed at the start -- and get a pretty reliable understanding of whether or not\nthey help the quality of the final model.  With that in place, we're in a good position\nto start running those intervention tests!</p>\n\n<p>Given the loss spike situation in that chart, I think that a solid first one to go\nfor -- even though it was the last in that list at the top of this post -- is gradient\nclipping.  Where are those loss spikes coming from, and if it's exploding gradients, what happens if we limit the\ndamage they do with gradient clipping?</p>\n\n<p>Stay tuned!  I've already done the training run for that (while I wrote this one up),\nso I should be able to post about it tomorrow.</p>\n\n<p><a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">Here's a link to the next post in this series</a>.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>Well, you could potentially do something with batches of different sizes, but\nthat would be fiddly.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
          "content": "<p>I'm rounding out my series of posts on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\"\nby seeing how I could train the <em>best</em> base model I can from scratch on my own hardware.\nI started by <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">training one in two days on my RTX 3090</a>,\nand found that while it was a decent little model, it wasn't as good as the original\nGPT-2 small, either in terms of the loss it got on my test dataset, or\nin terms of how good it was at following instruction prompts after fine-tuning on them.  I decided\nthat I wanted to see what levers I could pull -- dropout, attention weight biases, and so\non -- to make it better.</p>\n\n<p>For that, I didn't want to have my PC tied up for days at a time with multiple long training runs,\nso I <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">learned how to train faster in the cloud</a>.\nThat led to some <a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">refinements in the prompt-following test I was using</a>,\nand I also spent a bit of time on a side quest getting the various models I'd trained <a href=\"/2026/01/llm-from-scratch-31-models-on-hugging-face\">onto Hugging Face Hub</a>.</p>\n\n<p>Now it's time to try the various \"interventions\", as I'll call them -- the levers to\npull to see if I can make the model better.  This post is\nto recap what they are, and to describe what I did to establish a baseline model to\ncompare to.</p>\n<h3 id=\"the-interventions\">The interventions</h3>\n\n<p>I listed a number of possible interventions at the end of the RTX 3090 post; I'm not going\nto do them all, but for completeness, here's the full list:</p>\n\n<ul>\n<li>The amount of training data.  I'm not going to dig into this one; it looks like it\ndoes help, but the returns diminish rapidly, so I thi",
          "depth": 0.8,
          "published": "2026-02-04T01:45:00+00:00",
          "category": "实战技巧"
        },
        {
          "title": "Writing an LLM from scratch, part 32b -- Interventions: gradient clipping",
          "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32b-interventions-gradient-clipping",
          "source": "gilesthomas.com",
          "summary": "<p>I'm still working on training the best GPT-2 small sized base model that I can\nwith a number of FLOPs roughly equal to two days on my own machine -- my \"extra credit\"\nexercise after having worked through\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>In the <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">last post</a> I trained\na baseline model -- one with the same architecture and almost the same training code as in\nthe minimal training run in the book, just modified to run using DDP on an 8x A100 40 GiB/GPU\nmachine in the cloud.\nThere are a bunch of \"interventions\" I want to try to see if they'll make it better,\nas measured by the loss they get on a test set.  I'll do a post for each intervention,\nand this is the first: gradient clipping.</p>\n<h3 id=\"why\">Why?</h3>\n\n<p>In the training chart for the baseline model, you can see that there are three\nplaces where the loss suddenly spiked up, at around global steps 4,200, 13,000,\nand 23,000:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>There are a number of things that could cause loss spikes like that:</p>\n\n<ul>\n<li>A \"bad batch\" -- that is, one batch, or even one sequence in a batch, was\nmassively different in structure to the others that the model had seen, so it just\nhad much worse loss.  That doesn't seem likely in this case, though: the numbers\non the chart are averages over 617 global steps each, and it would take a truly pathological\nsequence to move the needle that much.</li>\n<li>Something weird in the optimiser.  That's not something I understand well, but\naccording to the various LLMs I'm working with, it's a possibility.</li>\n<li>Exploding gradients.  This is my working hypothesis, and so in this post I'll\ntry out gradient clipping, the normal solution to that problem.</li>\n</ul>\n\n<h3 id=\"what\">What?</h3>\n\n<p>Exploding gradients are common in RNNs, and also happen in LLMs like this one.  I spent a bit\nof time reading around to find out how they happen, and the ah-ha moment came when\nI came across <a href=\"https://medium.com/data-science/what-is-gradient-clipping-b8e815cdfb48\">this post from Wanshun Wong</a>.\nNot only is the post itself a good intro in terms of how it affects RNNs, but in the\n\"further reading\" at the end, there's some gold:</p>\n\n<blockquote>\n  <p>Chapter 10.11 of [1] has a good overview of how gradient clipping works.</p>\n  \n  <p>...</p>\n  \n  <p>References</p>\n  \n  <ol>\n  <li>I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning (2016), MIT Press.</li>\n  </ol>\n</blockquote>\n\n<p>Now, I bought a copy of \"<a href=\"https://www.deeplearningbook.org/\">Deep Learning</a>\" at the\nsame time as I bought Raschka's book, but I'd only glanced through it.  Now was the\ntime to get it down from the shelf -- and, indeed, section 10.11.1 is all about clipping\nto handle exploding gradients.  I'll put the explanation of how they happen into my\nown words, to see if I can clarify things (at least in my mind).</p>\n\n<p>Normally, when we learn about gradient descent, it's illustrated with nice smooth\nloss charts like this imaginary one for a single-parameter model:</p>\n\n<p><img alt=\"A simple loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/simple-loss-chart.png\" title=\"A simple loss chart\" /></p>\n\n<p>We're told that we might start at point A.  The gradient is quite high and negative,\nso we multiply it by our learning rate and subtract it from our parameter.  That\ngets us to point B.  This time around, the gradient is smaller as the curve is flatter\nthere, so when we do the same -- multiply by LR and subtract -- we take a smaller step, and\nwind up at C.  Rinse and repeat and we'll wind up near the minimum.</p>\n\n<p>The problem is, what if the loss curve actually looks like this:</p>\n\n<p><img alt=\"A more complex loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/more-complex-loss-chart.png\" title=\"A more complex loss chart\" /></p>\n\n<p>...?</p>\n\n<p>We start at A, with a small gradient, move a little to the right, and now we're at\nB halfway down a cliff!  The gradient is massive, and when we subtract it, even scaled\nby the learning rate, we can zoom off somewhere to the right -- maybe not even on the\nchart.  Indeed, you can imagine a cliff that is so steep that\nit would have vertical portions -- negative infinite gradients in this case -- and no matter what your learning\nrate is, you'll wind up with an infinite parameter update and everything will break.\nIt's hard to see how a model can continue training in a case like that.</p>\n\n<p>Now, what can cause steep cliffs like that?  The book says \"strongly nonlinear functions,\nsuch as those computed by a recurrent neural net over many time steps\".</p>\n\n<p>If you know about RNNs (I <a href=\"/2025/10/revisiting-karpathy-unreasonable-effectiveness-rnns\">wrote about them</a>\nif you'd like a summary), you'll remember that a single RNN might be quite\nshallow -- maybe three or four layers -- but when you're doing backpropagation,\nyou run a number of inputs through, one after the other, work out the overall loss, and then \"unroll\" it to\nsomething similar to a \"vanilla\" neural net to do the backward pass.  To put that in\nconcrete terms, a 3-layer neural network trained with a 100-element sequence would\nunroll to a 300-layer deep network.  Every one of those layers has several operations, including\n(in the implementation I was looking at in my post above), a <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow></math>.  It's not surprising that there are cliffs in the loss landscape -- it's\nmore surprising that there are any smooth bits!</p>\n\n<p>Now in LLMs, we don't have that unrolling through time -- but our network is deep enough\nas it is.  For the GPT-2 small model, disregarding the embeddings and the final output head, we have 12 Transformer layers,\neach of which is multiple matrix multiplications for attention, then a softmax, then another layer, and\nthen a feed-forward... mapping precisely to the equivalent vanilla NN is hard, but I think\nyou can treat each one as at least four layers, so we've got 48. And there are GELUs and logs and\nexps <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup> dotted around, so again -- we should expect cliffs.</p>\n\n<p>So if sometimes we'll get crazy gradients, what can we do about them?  We clip them.</p>\n\n<h3 id=\"how\">How?</h3>\n\n<p>Clipping gradients simply means that if they get larger than a particular number -- <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>,\nwhich we define -- we reduce them to that number.  In other words, we have a cap on how\nbig they can get.</p>\n\n<p>\"Deep Learning\" (\"DL\" from now on) suggests two ways to do it.  Remember that while in the\nexample above, we only had one parameter -- on the X axis -- for the GPT-2 small\nLLM we're training, we have 163 million of them.  So the gradients, instead of\nbeing one number, will be a 163M-long vector, one per parameter.  The two ways to clip are:</p>\n\n<ul>\n<li>We clip element-wise.  If any one of the gradients in the vector is larger than <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>,\nwe reduce it to <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>.</li>\n<li>We clip based on the norm: the length of the gradient vector in -- in our\ncase -- 163M-dimensional space.  That sounds harder than it is -- it's really\njust an extension of the Pythagorean equation that <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&#x0002b;</mo><msup><mi>b</mi><mn>2</mn></msup><mo>&#x0003d;</mo><msup><mi>c</mi><mn>2</mn></msup></mrow></math> to multiple\ndimensions.  If you want to work out the length of a vector <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo stretchy=\"false\">&#x00028;</mo><mi>a</mi><mo>&#x0002c;</mo><mi>b</mi><mo stretchy=\"false\">&#x00029;</mo></mrow></math> then you\ncan use Pythagoras to work out <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>c</mi><mo>&#x0003d;</mo><msqrt><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&#x0002b;</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></msqrt></mrow></math>, and that generalises\nto any number of dimensions.  So for our model we'd just square all 163M\nelements of the vector, sum those, and take the square root of the result, and that's the norm. <sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"#fn-2\">2</a></sup>\nIf the norm is greater than <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, we just divide every element of the gradient vector by the norm\nand multiply the result by <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, to produce\na new gradient vector whose norm is <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>.</li>\n</ul>\n\n<p>The second feels more elegant -- we're scaling all of the elements of the gradient\nvector by the same amount, so it still points in the same direction.  Interestingly, though,\nDL says that the two methods \"work similarly\", which I'll read as \"are pretty much\nthe same in practice\".</p>\n\n<p>DL then goes on to say how infinite or not-a-number gradients should be handled.\nWith the first way, clearly doing it naively would set every element in the gradient\nvector to <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, which would make the total size (norm) of the update very large.  With the\nsecond, it be even worse -- we'd still wind up with completely junk gradients, because\nthe norm would be infinite, and in Python <code>math.inf / math.inf</code> is <code>math.nan</code>, so\nwe'd be applying gradients with NaNs in them at best.  That would be likely to\nknock our model into unrecoverable territory, as any parameter that had that applied\nto it would be NaN forever.</p>\n\n<p>Their suggested solution is that if you get garbage gradients like that, you can take\na random step -- that is, create a new gradient to apply that has the norm <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>\nbut just points in a random direction. The idea is that this will move you away from\nthe cliff-ridden part of the loss landscape where you've found yourself (more about that later), and things will\ncontinue nicely.</p>\n\n<p>So, anyway, how to do this in practice?</p>\n\n<p>PyTorch has a function, <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a>,\nand that's what's referenced in almost every bit of writing I've found about how\nto clip gradients.  So I decided to use that, assuming it would do what was described\nin DL's second option and that it would do the random updates they suggest for non-finite\ngradients.  (I was half-correct -- see later.)</p>\n\n<p>As to how to use it -- if we had a normal training loop, where we were just using a normal optimiser, we would go from:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">y_logits</span><span class=\"p\">,</span> <span class=\"n\">target_y_ids</span><span class=\"p\">)</span>\n    <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...to something like</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">y_logits</span><span class=\"p\">,</span> <span class=\"n\">target_y_ids</span><span class=\"p\">)</span>\n    <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...where <code>clipping_max_norm</code> is the max value <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math> from above.</p>\n\n<p>However, for our training code using Automatic Mixed Precision (AMP),\nit's a little more complicated -- but luckily, the AMP explainer we've been using\n<a href=\"https://docs.pytorch.org/docs/stable/notes/amp_examples.html#gradient-clipping\">has a section explaining what to do</a>.</p>\n\n<p>Right now we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Per that explainer, we need to move to this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">unscale_</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>That looks a bit weird; we're \"unscaling\" the gradients,\nthen clipping them, then using the scaler to step the\noptimiser.  You'd think that you'd need to \"re-scale\" the scaler after clipping the gradients --\nto get back to where you started from before the optimiser step.\nFrom the help page I gather it keeps track of whether or not the gradients it has right now are\ncurrently scaled and handles them appropriately based on that state in <code>scaler.step</code>.</p>\n\n<p>Anyway, given that we know what the code looks like now, we need to implement it\nin a way that can be easily switched on for this experiment (and potentially in\nthe future), but which also allows us to not use it if we don't want to.</p>\n\n<p>The best way with our setup is to make it a training option, so we can do it this way:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">clipping_max_norm</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">unscale_</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...with <code>clipping_max_norm</code> extracted from the <code>train.json</code> file where we call it in\n<code>load_datasets_and_train</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train</span><span class=\"p\">(</span>\n        <span class=\"n\">run_dir</span><span class=\"p\">,</span>\n        <span class=\"n\">ddp_model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n        <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;clipping_max_norm&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">train_ds</span><span class=\"p\">,</span>\n        <span class=\"n\">global_step</span><span class=\"p\">,</span> <span class=\"n\">best_loss</span><span class=\"p\">,</span>\n        <span class=\"n\">checkpoint_interval</span><span class=\"o\">=</span><span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;checkpoint_interval&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">do_checkpoints</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and we can just pass in <code>None</code> for it in our <code>check_batch_size_works</code> function that\nwe use to find the maximum micro-batch size for our current hardware, as all we're\ntesting for there is memory usage -- we don't care if we're doing good updates.</p>\n\n<p>Here's <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/4abebdd2b81dbc7d0a3113fc1c5daf943361357e\">the code delta for that</a>,\nplus a <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/202a79a1d6ede40aab7a8adb19bc0b17bb5c9f5f\">bugfix</a> to allow\nfor <code>train.json</code> files without a <code>clipping_max_norm</code> in them.</p>\n\n<p>But it would also be useful to be able to track when it \"fired\" -- that is, when we\nhad to clip our gradients.  Then we can see two things:</p>\n\n<ol>\n<li>Whether we actually did wind up clipping them and fixing those loss spikes</li>\n<li>Whether we were clipping at other times -- we don't want to be doing it unnecessarily.</li>\n</ol>\n\n<p>Now, the docs for <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\"><code>clip_grad_norm_</code></a>\nsay that it returns the \"[t]otal norm of the parameter gradients (viewed as a single vector)\".\nIt doesn't say whether that's before or after the clipping, but given that the return value would\nalways be <code>clipping_max_norm</code> if it was after, I'm going to guess that it returns\nthe pre-clipping norm (ChatGPT agrees).</p>\n\n<p>So we can chart that; changes in these diffs: <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/6d5d0cd96b553b420cbf2c8d6c2d2af0f5a36582\">1</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/ac0ef28ac8879f3a6351da166344d918d38cfc76\">2</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/4d476239d9ed3aa1e0864536ea4c61632898b3bd\">3</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/326ccd2ef490aea57371394a949969a687c904ad\">4</a>.</p>\n\n<h3 id=\"how-much\">How much?</h3>\n\n<p>So we now have code to clip gradients to a given norm size and to chart the gradient\nnorms so that we know what they were before clipping.  The question is, what\nshould that clipping norm be?  Some googling around suggested that there was no standard way\nof saying \"for such-and-such a kind of model, gradients should be clipped at around\n<math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>x</mi></mrow></math>\".  For example, on <a href=\"https://www.reddit.com/r/MachineLearning/comments/kqgne3/choosing_gradient_norm_clip_value_d/\">this Reddit thread</a>,\n<code>GLVic</code> says \"Common values are 1, 3, 5, 8, 10\", and likewise sample code in\n<a href=\"https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem\">this tutorial</a>.\nhas 1, as does <a href=\"https://www.geeksforgeeks.org/deep-learning/understanding-gradient-clipping/\">this one</a>.</p>\n\n<p>So my initial thought was, let's just use 1.  But then I wondered, what actually are\nthe gradient norms that we're getting in normal training?  I decided to run a local short\ntrain on 3m tokens (a thousandth of the full training set, taking just less than four minutes) with very frequent checkpointing, and\ngradient clipping set to 1, and\nsee what happened.</p>\n\n<p><img alt=\"Small local train, gradient clipping at 1\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/gradient-clipping-1-locally.png\" title=\"Small local train, gradient clipping at 1\" /></p>\n\n<p>You can see that the \"grad max\" line is almost always above the \"grad clip\" -- we're\nalmost always clipping.   This doesn't sound right.  It looked like the range of the grad max\nwas generally beween 1.1 and a little above 3, so I set the <code>clipping_max_norm</code> to 3.5 and\ndid another train:</p>\n\n<p><img alt=\"Small local train, gradient clipping at 3.5\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/gradient-clipping-3.5-locally.png\" title=\"Small local train, gradient clipping at 3.5\" /></p>\n\n<p>Our loss is about the same, but we're no longer clipping -- and that's what we want;\nthere was no evidence of exploding gradients for that short run -- just big updates\nnear the start, as you'd expect.</p>\n\n<p>I then ran the same with no gradient clipping at all, and got exactly the same shape\nfor the loss chart as I did with gradient clipping at 3.5, and the same final loss -- that's a good signal that clipping is\nnot affecting the train when we stay inside the limit, which is exactly what we want.</p>\n\n<p>So, it was time to train our model!</p>\n\n<h3 id=\"running-the-train\">Running the train</h3>\n\n<p>I kicked off the train, and after a little while, I looked at the training chart,\nwhich is updated dynamically as the model trains:</p>\n\n<p><img alt=\"First run of cloud train, with missing max gradients\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/cloud-run-missing-gradient-maxes.png\" title=\"First run of cloud train, with missing max gradients\" /></p>\n\n<p>You can see the dotted green lines, both the light one and the dark one -- that is,\nthe \"grad max\" and the \"grad avg\" -- disappear starting just before global step\n4,000, only coming back at about 5,500 -- that is, these were not plotted for\nglobal steps 4,319 and 4,936, even though the loss was.  What was going on?</p>\n\n<p>I took a look at the checkpoint meta file for the first of those to see what the actual numbers\nwere, and saw this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;min_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">3.7176883220672607</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;max_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">5.877607822418213</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;avg_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.3170230991450085</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;max_grad_norms&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">I</span><span class=\"kc\">nf</span><span class=\"err\">i</span><span class=\"kc\">n</span><span class=\"err\">i</span><span class=\"kc\">t</span><span class=\"err\">y</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;avg_grad_norms&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">I</span><span class=\"kc\">nf</span><span class=\"err\">i</span><span class=\"kc\">n</span><span class=\"err\">i</span><span class=\"kc\">t</span><span class=\"err\">y</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;frac_clipped&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.0016207455429497568</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;global_step&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">4319</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;is_best&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>Aha!  The PyPlot code I was using could not handle infinite values, which is entirely\nreasonable.  That was easy enough to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/78a1e794a84c4e01cd7f812755c17ded07f35b93\">fix</a>,\nthough -- I just replaced positive infinity by 1,000,000 and negative infinity by -1,000,000,\nand then (in the interest of getting a proper from-scratch run) kicked everything\noff from the beginning.</p>\n\n<p>That training run completed with this chart:</p>\n\n<p><img alt=\"Second cloud run, showing clipping periods\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/second-cloud-run-showing-clipping.png\" title=\"Second cloud run, showing clipping periods\" /></p>\n\n<p>That's a little hard to read, but if you look closely at the green lines, you\ncan see that there are seven periods where gradients were either very large or\ninfinite.  Weirdly, though, out of the seven, two of them were two checkpoint periods long\n(that is, two periods of 617 global steps).  That felt weird, though of course\nwe're looking at the maximum gradient norm and the average gradient norm -- so\ntwo single infinite/high-gradient steps in successive 617-step periods would lead to that effect.</p>\n\n<p>What was even stranger, though,\nwas that if you look at the training chart for the run with no gradient clipping,\nwe have only three loss spikes rather than seven:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>...though it's also very noticeable that the gradient-clipped run had only two small loss\nspikes, unlike the three larger ones in the unclipped run.</p>\n\n<p>The training loss the gradient-clipped run reported at the end was better, too:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,343.442 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 264,128 tokens/second</span>\n<span class=\"go\">Final train loss: 3.728</span>\n</code></pre>\n</div>\n\n<p>...versus 3.743 at the end of the baseline train.</p>\n\n<p>So it was time to download it, and run the sequence-completion smoke test:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/model.json<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/checkpoints/best/model.safetensors\nEvery<span class=\"w\"> </span>effort<span class=\"w\"> </span>moves<span class=\"w\"> </span>you<span class=\"w\"> </span>further<span class=\"w\"> </span>afield<span class=\"w\"> </span>the<span class=\"w\"> </span>most<span class=\"w\"> </span>to<span class=\"w\"> </span>get<span class=\"w\"> </span>more<span class=\"w\"> </span><span class=\"nb\">time</span><span class=\"w\"> </span>and<span class=\"w\"> </span>space<span class=\"w\"> </span>out<span class=\"w\"> </span>of<span class=\"w\"> </span>your<span class=\"w\"> </span>pocket.<span class=\"w\"> </span>With<span class=\"w\"> </span>more<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>abundance\n</code></pre>\n</div>\n\n<p>Coherent enough!</p>\n\n<p>Next, we evaluate it against our held-back test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/model.json<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/checkpoints/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">1471</span>.81it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:58&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.72it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.678\n</code></pre>\n</div>\n\n<p>So, the loss had gone down -- but only from 3.743 to 3.678, a reduction of 0.065,\nor about 1.7%.</p>\n\n<p>That's not actually all that bad!\nAfter all, in my initial experiments on my local machine, training for a Chinchilla-optimal\nnumber of tokens from FineWeb-Edu (rather than the regular FineWeb I'm using now)\ngot a loss of 4.167 on the same dataset (weirdly worse with the more-curated training set),\nand training for a further Chinchilla-optimal number of tokens only brought that down to\n4.135, for a difference of 0.032, or 0.7%.</p>\n\n<p>It's not strictly comparable due to the different training sets, but speaking <em>very</em>\nloosely, we could say that gradient clipping for this train had more effect than doubling the training time for\nthe other one.  That's pretty nifty.</p>\n\n<p>But the question remained: why those long periods of high gradients, even with gradient\nclipping?  And why were there still loss spikes -- in particular the one just before\nglobal step 12,000, which lasted for two checkpoint periods?</p>\n\n<h3 id=\"chasing-infinity\">Chasing infinity</h3>\n\n<p>Remember that when I started the first run of this train, and got the chart with\nthe missing bits, it was because the logged <code>max_grad_norms</code> and <code>avg_grad_norms</code>\nwere infinite.</p>\n\n<p>What happens when <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a> gets an infinite gradient -- either one that has\nan infinity as one of its components, or one that (due to numerical overflow) winds up\nwith a norm of infinity anyway?  I'd been kind of assuming that it did what the authors\ndescribed in \"Deep Learning\" -- a random update of norm <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math> -- given that the book\nstated pretty confidently that you \"can\" do it but then appeared to consider the topic closed.</p>\n\n<p>But it doesn't!  If you check that link to the docs, you'll see that it has a parameter\n<code>error_if_nonfinite</code>, which is <code>False</code> by default.  If it's set to <code>True</code>, that will\nraise an exception if the norm is positive or negative infinity, or if it's not a number\n-- which catches both the infinite component and the norm overflow cases above.  But if\nit's not set -- and we weren't setting it -- and the norm or the gradients are non-finite, then <code>clip_grad_norm_</code> will essentially\nreturn garbage gradients.  Depending on the exact cause, elements will either be infinities\nof one sign or another, or NaNs.  And if these are added to parameters, then those\nparameters will become garbage too.</p>\n\n<p>Now that leads to the question, given that we know that somewhere in the period\nbetween the checkpoint at global step 4,319 and the previous one at 3,702 there was\nan infinite norm at some point, how on earth did the model manage to continue training\nafter that?  Loss went up at around the same time, but it wasn't completely broken as\nit would have been with NaNs or infinities in its parameters.</p>\n\n<p>Obscurely enough, the answer turned out to be in the <a href=\"https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-gradscaler\">AMP explainer</a>,\nin a comment in one of the bits of example code.  Regarding the <code>GradScaler</code> class we're using:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"c1\"># ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.</span>\n        <span class=\"c1\"># If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,</span>\n        <span class=\"c1\"># otherwise, optimizer.step() is skipped.</span>\n</code></pre>\n</div>\n\n<p>So what was happening was that the scaler -- something we introduced into our code\nto get a speedup by using 16-bit floats instead of 32-bit whenever PyTorch thought\nit would make sense -- was protecting us against infinite and NaN gradients as a\nside-effect.  It was skipping updates that would have polluted our weights with\nbad values from non-finite gradients.</p>\n\n<p>Hmph.</p>\n\n<h3 id=\"grumble\">Grumble</h3>\n\n<p>If the above comes across as a little frustrated, then it's because I am a bit!\nFrom a software engineering viewpoint, this situation really does feel a bit like a\nrather messy part of the API.</p>\n\n<p>There are three things that it's reasonable for a library to do with infinite/NaN\ngradients:</p>\n\n<ol>\n<li>Blindly apply them and expect the developer to sanitise their inputs.</li>\n<li>Raise an error.</li>\n<li>Take some kind of default sane action, like skipping the update.</li>\n</ol>\n\n<p>Now, if we look at that <code>error_if_nonfinite</code>, we can see that the first two of those\ncases are handled there; and the developer can choose which option to follow.\nIt's not where I'd personally put it (the <code>step</code> function on the optimiser seems more\nnatural) and I think I'd probably set the default to <code>True</code> too, but I can also imagine\ngood reasons for it being the way it is -- backward compatibility for one.</p>\n\n<p>But the \"skip non-finite gradients\" being a (not even optional!) behaviour that is\non a class designed for handling mixed-precision training just seems outright bonkers.\nI would be surprised if there weren't people out there who've spent days trying\nto work out why their training runs failed catastrophically when they decided to\nswitch from mixed-precision to \"full fat\" 32-bit floats, not realising that a\nhardly-even-documented feature of the scaler <sup class=\"footnote-ref\" id=\"fnref-3\"><a href=\"#fn-3\">3</a></sup> had been saving them from gradient issues\npreviously.</p>\n\n<p>Anyway, rant over.  What does this all mean?</p>\n\n<h3 id=\"so\">So...?</h3>\n\n<p>There are three ways a gradient can explode:</p>\n\n<ol>\n<li>It can get very large, still be finite, and have a finite norm.</li>\n<li>It can get very large, still be finite, but have an infinite norm (eg. due to numerical overflow)</li>\n<li>It can become infinite -- that is, at least one of the parameters' gradients is infinite (which\nof course means an infinite norm regardless of any numerical stuff).</li>\n</ol>\n\n<p>With both the baseline code and our new code, the <code>GradScaler</code> was saving us from\nthe last two of those, by skipping the optimiser steps with non-finite gradients.</p>\n\n<p>However, the baseline run was not protected against the first kind -- large but finite\ngradients with a finite norm -- while this run was protected.</p>\n\n<p>What I'm almost certain is happening here is that in all of my training runs so\nfar, there have been all three kinds of issues with exploding gradients.  The\n<code>GradScaler</code>, which again, we introduced for faster training, happened to be saving\nus from the infinite gradients/norms.  But we were still being bitten by the finite\nbut excessively large ones.</p>\n\n<p>And that, I think, is why this training run had a positive -- not huge, but certainly worthwhile\n-- effect on the test set loss.</p>\n\n<p>If I had more time, I think I'd do another run, logging all three of those categories\nof error to see how frequent they are, and charting the result.  That might go some way to\nexplaining the final question I had here: why is it that the renowned \"Deep Learning\"\nsuggests a random update to get away from the cliff where you've found yourself,\nwhile we seem to be getting away with just skipping the update, which is much simpler?\nWell, the book was written in 2016, and I guess rather a lot has changed in the last 10 years :-)</p>\n\n<p>My guess is that their solution might have been\na solid default in the age of RNNs, but might not make so much sense with the kind of models\nwe're training these days.</p>\n\n<p>I think I can see a way in which that makes sense.  Think of the illustration of a loss \"cliff\"\nin a one-parameter world that we had at the start of this post:</p>\n\n<p><img alt=\"A more complex loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/more-complex-loss-chart.png\" title=\"A more complex loss chart\" /></p>\n\n<p>If you happen to wind up on that cliff, you're in trouble.</p>\n\n<p>But imagine a two-parameter model -- the line of the loss function becomes a surface.\nJust as in the real world you might be able to walk along the edge at the top of a cliff and\nfind a nice easy slope down next to it, you can imagine that the cliff in the two-parameter\ncase might be less of a problem because you don't need to be lucky enough to jump down it --\nyou can walk around it.</p>\n\n<p>Extrapolating examples like this to higher dimensions is\nrisky, but I think it should hold that the more dimensions you're working with,\nthe less likely it is that a cliff is an issue -- you're more likely to be able to find\na way around it.  I've heard a very similar argument made for why local minima are\nless of an issue with lots of parameters.  It's certainly worth saying that this is\nfar from a mathematical proof, but I think it's a decent grounding for intuition.</p>\n\n<p>Now think about an RNN.  Although you're doing back-propagation through time over\nwhat amounts to a very deep network, there aren't actually all that many parameters,\ncertainly compared to an LLM like this.  Each parameter is involved in the back-propagation\nmultiple times.</p>\n\n<p>So, thinking of it that way, the gradient vector for the RNNs they were dealing with\nwas of much lower dimensionality than the ones we're dealing with, even for this\ntiny model.</p>\n\n<p>They say that the random step \"will typically move away from the numerically unstable\nconfiguration\".  I'm probably playing fast and loose here, but I'll take that as something\nlike: if you wound up on a cliff, you were likely in a very \"cliffy\"\narea of the loss landscape.  \"Teleporting\" randomly to somewhere some distance away\nwas a sensible way to handle that.</p>\n\n<p>In our situation, even if the area is \"cliffy\" in the direction that one particular\nbatch might push us, we have so many extra dimensions that it may well be that it\nwon't be so bad with the next one.  So just skipping the problematic update -- under\nall of those assumptions -- seems a perfectly reasonable way to handle it.</p>\n\n<h3 id=\"validation\">Validation</h3>\n\n<p>All of this, BTW, made me think back to validation loss.  In our previous training runs,\nwhere we were measuring it just before each checkpoint, its spikes were in general correlated\nwith but not identical to spikes in training loss:</p>\n\n<p><img alt=\"Loss in a run with validation\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xh100m80.png\" title=\"Loss in a run with validation\" /></p>\n\n<p>Now, of course, exploding gradients don't have to be related to high training loss --\nthere's enough non-linearity in there that we can treat them as being completely uncorrelated,\nI think.  But you definitely would expect them to have an effect on validation\nloss if applied.  Disregarding the infinite ones (which were being filtered out anyway),\nthe very high ones that we are now clipping would, in the unclipped baseline\ntrain, seem very likely to have caused validation loss spikes.</p>\n\n<p>So: if I hadn't stripped that out, we would likely have been able to see a clear\ndifference in the validation loss line between clipped and unclipped.  That would have\nbeen useful!</p>\n\n<p>I'm not going to re-introduce it, though.  Best to keep the number of code changes\nto a minimum if I'm trying to compare like with like over the course of these intervention\ntests.</p>\n\n<h3 id=\"anyway\">Anyway.</h3>\n\n<p>I think that's enough for gradient clipping.  I may come back and do the experiment\nanother time to see what the relative ratios of the different kinds of problematic\ngradients are.  Are there parts of the train where we get lots of them as a percentage (ie.\nwe're somewhere \"cliffy\" in the loss landscape)?  How many infinite gradient vs infinite norm\nvs big-but-not-infinite instances do we have relative to each other, and to normal\ngradient updates?  What do we see if we have validation loss?  And so on.</p>\n\n<p>But for now: gradient clipping definitely helps, and goes on the positive interventions list!</p>\n\n<p>I'm thinking I'll see what happens with switching off dropout next.  That should at\nleast be a bit easier...</p>\n\n<p>Stay tuned!</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p><a href=\"https://www.youtube.com/watch?v=DdRnMjfVQi0\">Oh my</a>.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-2\">\n<p>Technically the L2 norm -- if you used cubes/cube root it would be L3,\nand likewise for the power of four and L4 and so on.  But the L2 is the\none used for gradient clipping.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-2\" title=\"Jump back to footnote 2 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-3\">\n<p>Shades of <a href=\"https://www.goodreads.com/quotes/40705-but-the-plans-were-on-display-on-display-i-eventually\">Douglas Adams</a>, really:</p>\n\n<p>\"But the plans were on display...\"</p>\n\n<p>\"On display? I eventually had to go down to the cellar to find them.\"</p>\n\n<p>“That’s the display department.\"</p>\n\n<p>“With a flashlight.\"</p>\n\n<p>“Ah, well, the lights had probably gone.\"</p>\n\n<p>“So had the stairs.\"</p>\n\n<p>“But look, you found the notice, didn’t you?\"</p>\n\n<p>“Yes,\" said Arthur, “yes I did. It was on display in the bottom of a locked filing cabinet stuck in a disused lavatory with a sign on the door saying ‘Beware of the Leopard.\"&#160;<a class=\"footnoteBackLink\" href=\"#fnref-3\" title=\"Jump back to footnote 3 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
          "content": "<p>I'm still working on training the best GPT-2 small sized base model that I can\nwith a number of FLOPs roughly equal to two days on my own machine -- my \"extra credit\"\nexercise after having worked through\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>In the <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">last post</a> I trained\na baseline model -- one with the same architecture and almost the same training code as in\nthe minimal training run in the book, just modified to run using DDP on an 8x A100 40 GiB/GPU\nmachine in the cloud.\nThere are a bunch of \"interventions\" I want to try to see if they'll make it better,\nas measured by the loss they get on a test set.  I'll do a post for each intervention,\nand this is the first: gradient clipping.</p>\n<h3 id=\"why\">Why?</h3>\n\n<p>In the training chart for the baseline model, you can see that there are three\nplaces where the loss suddenly spiked up, at around global steps 4,200, 13,000,\nand 23,000:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>There are a number of things that could cause loss spikes like that:</p>\n\n<ul>\n<li>A \"bad batch\" -- that is, one batch, or even one sequence in a batch, was\nmassively different in structure to the others that the model had seen, so it just\nhad much worse loss.  That doesn't seem likely in this case, though: the numbers\non the chart are averages over 617 global steps each, and it would take a truly pathological\nsequence to move the needle that much.</li>\n<li>Something weird in the optimiser.  That's not something I understand well, but\naccording to the various LLMs I'm working with, it's a possibility.</li>\n<li",
          "depth": 0.7,
          "published": "2026-02-05T01:20:00+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 2,
      "avg_depth": 0.75,
      "heat_score": 44.5,
      "rank": 1
    },
    {
      "canonical_name": "员工日常安全体验与门禁系统",
      "category": "技术探讨",
      "articles": [
        {
          "title": "We installed a single turnstile to feel secure",
          "link": "https://idiallo.com/blog/installed-single-turnstile-for-security-theater?src=feed",
          "source": "idiallo.com",
          "summary": "<p>After the acquisition by a much larger company, security became a top priority. Our company occupied three tall buildings, each at least 13 stories high. Key card readers were installed next to every entrance, every elevator car, and even at the parking lot entrance, which itself was eight stories tall.</p>\n\t\t\t<p>The parking lot system was activated first. If you wanted to park your car, you needed to scan your pass. It didn't take long for lines to start forming, but they were still manageable.</p>\n\n<p>Then the doors were activated. I would often forget my key card on my desk and get stuck in the stairwell. After lunch, I'd climb the stairs all the way to the 11th floor, only to find myself locked out at the door. Fortunately, the buildings were full of people, and there was always someone to open the door for me. I'd slip in suspiciously while they contemplated the email that clearly said not to let anyone in with your own card.</p>\n\n<p>While we were battling to get used to the key cards, the company was installing turnstiles on the ground floor of every building. They looked futuristic, but I was already anticipating a problem the designers hadn't considered. Each building had 13 floors. Each floor was full of employees. Hundreds of employees per building would each have to scan their card to get in.</p>\n\n<p>I'm a software engineer. I understand that security isn't an optional feature you build on top of your application. Instead, you need to implement safeguards at the foundation. In fact, one of the most important applications I was working on was a tool to manage how different teams retrieved their tasks from Jira. If you've read this blog before, you know I always complain about Jira.</p>\n\n<p>Anyway, the original designer of this application must have been pressed for time. Each action in the app required a call to the Jira endpoint, which needed authentication. He never saved the auth token returned by the API. Instead, each call had to re-authenticate and then perform its task.</p>\n\n<p>Did he ask the user to reenter the password every single time? No, he was smarter than that. Did he save the credentials in the database in plain text? He might have <a href=\"https://idiallo.com/blog/hiring-angular-experts-not\">been an intern</a>, but he wasn't crazy. No! Instead, he saved the username and password in the cookies. But for good measures, it was base64 encoded.</p>\n\n<p>Eventually, we received the email. All turnstiles were going to be activated. The following Monday, they would run in mock mode, where the turnstiles would remain open, but we'd have to scan and wait for the beep and green light before entering.</p>\n\n<p>I arrived at 8:30am. I met my colleagues and hundreds of other employees in the lobby. When the first person scanned their card, the machine beeped and turned green. We all clapped in celebration. We took turns making our way to the machine. Beep, turn green, next. But it grumbled for some employees and turned red. That was fine though, it was mock day. We all went about our day.</p>\n\n<p>The next day, when I came to work, I remained in my car, stuck in line for the parking lot for at least 10 minutes. Looking outside, I saw long lines of people circling each building.</p>\n\n<p>I managed to park my car and discovered that the line of people extended all the way down to the parking level. I waited in line for at least 30 minutes just to make it to the lobby. I texted my manager that I'd be late for the daily standup because I was stuck in line. She didn't text back. Instead, she waved at me from the front of the line. Scanning was already slow, you had to wait to be approved. But once you passed the turnstile, there was another line for the elevators. The elevator key card readers were also active.</p>\n\n<p>Imagine a couple dozen people all trying to squeeze into crowded elevators, each going to a different floor, and each trying to scan their key card to access their floor because someone who wasn't authorized for that floor couldn't scan it for them. Some elevator doors opened with a few people already inside because they couldn't scan their cards in the crowd, so they'd gone back down for a second attempt. In other words, it was complete chaos.</p>\n\n<p>It took more than an hour to go from the parking lot to my desk on the 11th floor.</p>\n\n<p>The next day, I decided to save time and take an Uber to work. Those were the days when an <a href=\"https://idiallo.com/blog/paying-for-my-8-years-old-ride\">Uber ride cost only $3</a>. I thought I was being smart, but another hundred people or so had the same idea. We had a pile of Uber rides lining up outside, each trying to drop off their riders and blocking the way to the parking lot, causing yet another traffic jam. Inside the building, it was still the same chaos. I only saved a few minutes.</p>\n\n<p>On the third day, they shut down the turnstiles. They clearly weren't working. They also disabled the key card readers in the elevators. It was a relief.</p>\n\n<p>Security was supposedly a priority, yet nobody ever talked about the Jira credentials saved in cookies. I received significant pushback when I requested we install a Redis service to store the generated auth tokens. I had to write entire documents to justify using it and request enterprise support from a vendor. After a month, the security issue was fixed to no fanfare.</p>\n\n<p>We did, however, receive an email celebrating the installation of three new turnstiles in the lobby. They never turned the elevator key card readers back on. They remained dormant, a reminder of the mess we'd gone through.</p>\n\n<hr />\n\n<p>The turnstiles were visible. They were expensive. They disrupted everyone's day and made headlines in company-wide emails. Management could point to them and say that we're taking security seriously. Meanwhile, thousands of employees had their Jira credentials stored in cookies. A vulnerability that could expose our entire project management system. But that fix required documentation, vendor approval, a month of convincing people it mattered. A whole lot of begging.</p>\n\n<p>Security theater checks a box. It makes people feel like something is being done. Real security is invisible. It's reviewing code, implementing proper authentication, storing tokens correctly. It doesn't come with a ribbon-cutting ceremony or a celebratory email. It's just good engineering that nobody notices when it's done right. But security theater is impossible to miss.</p>",
          "content": "<p>After the acquisition by a much larger company, security became a top priority. Our company occupied three tall buildings, each at least 13 stories high. Key card readers were installed next to every entrance, every elevator car, and even at the parking lot entrance, which itself was eight stories tall.</p>\n\t\t\t<p>The parking lot system was activated first. If you wanted to park your car, you needed to scan your pass. It didn't take long for lines to start forming, but they were still manageable.</p>\n\n<p>Then the doors were activated. I would often forget my key card on my desk and get stuck in the stairwell. After lunch, I'd climb the stairs all the way to the 11th floor, only to find myself locked out at the door. Fortunately, the buildings were full of people, and there was always someone to open the door for me. I'd slip in suspiciously while they contemplated the email that clearly said not to let anyone in with your own card.</p>\n\n<p>While we were battling to get used to the key cards, the company was installing turnstiles on the ground floor of every building. They looked futuristic, but I was already anticipating a problem the designers hadn't considered. Each building had 13 floors. Each floor was full of employees. Hundreds of employees per building would each have to scan their card to get in.</p>\n\n<p>I'm a software engineer. I understand that security isn't an optional feature you build on top of your application. Instead, you need to implement safeguards at the foundation. In fact, one of the most important applications I was working on was a tool to manage how different teams retrieved their tasks from Jira. If you've read this blog before, you know I always complain about Jira.</p>\n\n<p>Anyway, the original designer of this application must have been pressed for time. Each action in the app required a call to the Jira endpoint, which needed authentication. He never saved the auth token returned by the API. Instead, each call had to re-authenticate an",
          "depth": 0.8,
          "published": "2026-02-04T12:00:00+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 2
    },
    {
      "canonical_name": "ListView列宽调整防止策略",
      "category": "技术探讨",
      "articles": [
        {
          "title": "How can I prevent the user from changing the widths of ListView columns?",
          "link": "https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037",
          "source": "devblogs.microsoft.com/oldnewthing",
          "summary": "<p>You can ask the header to be non-resizing.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037\">How can I prevent the user from changing the widths of ListView columns?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
          "content": "<p>Suppose you are using a Win32 ListView control in report mode, and you&#8217;ve got all your columns set up perfectly, and you don&#8217;t want the user to resize them. How do you do that?</p>\n<p>There is no ListView style for preventing column resize, but there <i>is</i> a header control style to prevent sizing: <code>HDS_NOSIZING</code>. This style requires Common Controls version 6, but I&#8217;m sure you&#8217;re all using that version already, right?</p>\n<pre>auto hdr = ListView_GetHeader(hwndLV);\nSetWindowLong(hdr, GWL_STYLE,\n              GetWindowLong(hdr, GWL_STYLE) | HDS_NOSIZING);\n</pre>\n<p>Whether the columns can be resized is independent of whether the columns can be rearranged, which you specify by setting the <code>LVS_EX_HEADER­DRAG­DROP</code> ListView extended style.</p>\n<pre>ListView_SetExtendedListViewStyleEx(hwndLV,\n                                    LVS_EX_HEADERDRAGDROP,\n                                    LVS_EX_HEADERDRAGDROP);\n</pre>\n<p>Okay, but what if you&#8217;re stuck in the dark ages with version 5 of the Common Controls? We&#8217;ll look at that next time.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-00/?p=112037\">How can I prevent the user from changing the widths of ListView columns?</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
          "depth": 0.8,
          "published": "2026-02-04T15:00:00+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 3
    },
    {
      "canonical_name": "Bash脚本中的日期计算技巧",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Date Arithmetic in Bash",
          "link": "https://blog.miguelgrinberg.com/post/date-arithmetic-in-bash",
          "source": "miguelgrinberg.com",
          "summary": "<p>Date and time management libraries in many programming languages are famously bad. <a href=\"https://dev.arie.bovenberg.net/blog/python-datetime-pitfalls/\">Python's datetime module</a> comes to mind as one of the best (worst?) examples, and so does <a href=\"https://fjolt.com/article/javascript-date-is-weird/\">JavaScript's Date class</a>. It feels like these libraries could not have been made worse on purpose, or so I thought until today, when I needed to implement some date calculations in a backup rotation script written in bash.</p>\n<p>So, if you wanted to learn how to perform date and time arithmetic in your bash scripts, you've come to the right place. Just don't blame me for the nightmares.</p>",
          "content": "<p>Date and time management libraries in many programming languages are famously bad. <a href=\"https://dev.arie.bovenberg.net/blog/python-datetime-pitfalls/\">Python's datetime module</a> comes to mind as one of the best (worst?) examples, and so does <a href=\"https://fjolt.com/article/javascript-date-is-weird/\">JavaScript's Date class</a>. It feels like these libraries could not have been made worse on purpose, or so I thought until today, when I needed to implement some date calculations in a backup rotation script written in bash.</p>\n<p>So, if you wanted to learn how to perform date and time arithmetic in your bash scripts, you've come to the right place. Just don't blame me for the nightmares.</p>",
          "depth": 0.8,
          "published": "2026-02-04T11:09:06+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 40.0,
      "rank": 4
    },
    {
      "canonical_name": "sqlite-scanner工具介绍",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Distributing Go binaries like sqlite-scanner through PyPI using go-to-wheel",
          "link": "https://simonwillison.net/2026/Feb/4/distributing-go-binaries/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<p>I've been exploring Go for building small, fast and self-contained binary applications recently. I'm enjoying how there's generally one obvious way to do things and the resulting code is boring and readable - and something that LLMs are very competent at writing. The one catch is distribution, but it turns out publishing Go binaries to PyPI means any Go binary can be just a <code>uvx package-name</code> call away.</p>\n<h4 id=\"sqlite-scanner\">sqlite-scanner</h4>\n<p><a href=\"https://github.com/simonw/sqlite-scanner\">sqlite-scanner</a> is my new Go CLI tool for scanning a filesystem for SQLite database files.</p>\n<p>It works by checking if the first 16 bytes of the file exactly match the SQLite magic number sequence <code>SQLite format 3\\x00</code>. It can search one or more folders recursively, spinning up concurrent goroutines to accelerate the scan. It streams out results as it finds them in plain text, JSON or newline-delimited JSON. It can optionally display the file sizes as well.</p>\n<p>To try it out you can download a release from the <a href=\"https://github.com/simonw/sqlite-scanner/releases\">GitHub releases</a> - and then <a href=\"https://support.apple.com/en-us/102445\">jump through macOS hoops</a> to execute an \"unsafe\" binary. Or you can clone the repo and compile it with Go. Or... you can run the binary like this:</p>\n<pre><code>uvx sqlite-scanner\n</code></pre>\n<p>By default this will search your current directory for SQLite databases. You can pass one or more directories as arguments:</p>\n<pre><code>uvx sqlite-scanner ~ /tmp\n</code></pre>\n<p>Add <code>--json</code> for JSON output, <code>--size</code> to include file sizes or <code>--jsonl</code> for newline-delimited JSON. Here's a demo:</p>\n<pre><code>uvx sqlite-scanner ~ --jsonl --size\n</code></pre>\n<p><img alt=\"running that command produces a sequence of JSON objects, each with a path and a size key\" src=\"https://static.simonwillison.net/static/2025/sqlite-scanner-demo.gif\" /></p>\n<p>If you haven't been uv-pilled yet you can instead install <code>sqlite-scanner</code> using <code>pip install sqlite-scanner</code> and then run <code>sqlite-scanner</code>.</p>\n<p>To get a permanent copy with <code>uv</code> use <code>uv tool install sqlite-scanner</code>.</p>\n<h4 id=\"how-the-python-package-works\">How the Python package works</h4>\n<p>The reason this is worth doing is that <code>pip</code>, <code>uv</code> and <a href=\"https://pypi.org/\">PyPI</a> will work together to identify the correct compiled binary for your operating system and architecture.</p>\n<p>This is driven by file names. If you visit <a href=\"https://pypi.org/project/sqlite-scanner/#files\">the PyPI downloads for sqlite-scanner</a> you'll see the following files:</p>\n<ul>\n<li><code>sqlite_scanner-0.1.1-py3-none-win_arm64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-win_amd64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-musllinux_1_2_x86_64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-musllinux_1_2_aarch64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-manylinux_2_17_x86_64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-manylinux_2_17_aarch64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl</code></li>\n<li><code>sqlite_scanner-0.1.1-py3-none-macosx_10_9_x86_64.whl</code></li>\n</ul>\n<p>When I run <code>pip install sqlite-scanner</code> or <code>uvx sqlite-scanner</code> on my Apple Silicon Mac laptop Python's packaging magic ensures I get that <code>macosx_11_0_arm64.whl</code> variant.</p>\n<p>Here's <a href=\"https://tools.simonwillison.net/zip-wheel-explorer?url=https%3A%2F%2Ffiles.pythonhosted.org%2Fpackages%2F88%2Fb1%2F17a716635d2733fec53ba0a8267f85bd6b6cf882c6b29301bc711fba212c%2Fsqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl#sqlite_scanner/__init__.py\">what's in the wheel</a>, which is a zip file with a <code>.whl</code> extension.</p>\n<p>In addition to the <code>bin/sqlite-scanner</code> the most important file is <code>sqlite_scanner/__init__.py</code> which includes the following:</p>\n<pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">get_binary_path</span>():\n    <span class=\"pl-s\">\"\"\"Return the path to the bundled binary.\"\"\"</span>\n    <span class=\"pl-s1\">binary</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">path</span>.<span class=\"pl-c1\">join</span>(<span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">path</span>.<span class=\"pl-c1\">dirname</span>(<span class=\"pl-s1\">__file__</span>), <span class=\"pl-s\">\"bin\"</span>, <span class=\"pl-s\">\"sqlite-scanner\"</span>)\n \n    <span class=\"pl-c\"># Ensure binary is executable on Unix</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">platform</span> <span class=\"pl-c1\">!=</span> <span class=\"pl-s\">\"win32\"</span>:\n        <span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">stat</span>(<span class=\"pl-s1\">binary</span>).<span class=\"pl-c1\">st_mode</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">not</span> (<span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">&amp;</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXUSR</span>):\n            <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">chmod</span>(<span class=\"pl-s1\">binary</span>, <span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXUSR</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXGRP</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXOTH</span>)\n \n    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">binary</span>\n \n \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-s\">\"\"\"Execute the bundled binary.\"\"\"</span>\n    <span class=\"pl-s1\">binary</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">get_binary_path</span>()\n \n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">platform</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s\">\"win32\"</span>:\n        <span class=\"pl-c\"># On Windows, use subprocess to properly handle signals</span>\n        <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">exit</span>(<span class=\"pl-s1\">subprocess</span>.<span class=\"pl-c1\">call</span>([<span class=\"pl-s1\">binary</span>] <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">argv</span>[<span class=\"pl-c1\">1</span>:]))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c\"># On Unix, exec replaces the process</span>\n        <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">execvp</span>(<span class=\"pl-s1\">binary</span>, [<span class=\"pl-s1\">binary</span>] <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">argv</span>[<span class=\"pl-c1\">1</span>:])</pre>\n<p>That <code>main()</code> method - also called from <code>sqlite_scanner/__main__.py</code> - locates the binary and executes it when the Python package itself is executed, using the <code>sqlite-scanner = sqlite_scanner:main</code> entry point defined in the wheel.</p>\n<h4 id=\"which-means-we-can-use-it-as-a-dependency\">Which means we can use it as a dependency</h4>\n<p>Using PyPI as a distribution platform for Go binaries feels a tiny bit abusive, albeit <a href=\"https://simonwillison.net/2022/May/23/bundling-binary-tools-in-python-wheels/\">there is plenty of precedent</a>.</p>\n<p>I’ll justify it by pointing out that this means <strong>we can use Go binaries as dependencies</strong> for other Python packages now.</p>\n<p>That's genuinely useful! It means that any functionality which is available in a cross-platform Go binary can now be subsumed into a Python package. Python is really good at running subprocesses so this opens up a whole world of useful tricks that we can bake into our Python tools.</p>\n<p>To demonstrate this, I built <a href=\"https://github.com/simonw/datasette-scan\">datasette-scan</a> - a new Datasette plugin which depends on <code>sqlite-scanner</code> and then uses that Go binary to scan a folder for SQLite databases and attach them to a Datasette instance.</p>\n<p>Here's how to use that (without even installing anything first, thanks <code>uv</code>) to explore any SQLite databases in your Downloads folder:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uv run --with datasette-scan datasette scan <span class=\"pl-k\">~</span>/Downloads</pre></div>\n<p>If you peek at the code you'll see it <a href=\"https://github.com/simonw/datasette-scan/blob/1a2b6d1e6b04c8cd05f5676ff7daa877efd99f08/pyproject.toml#L14\">depends on sqlite-scanner</a> in <code>pyproject.toml</code> and calls it using <code>subprocess.run()</code> against <code>sqlite_scanner.get_binary_path()</code> in its own <a href=\"https://github.com/simonw/datasette-scan/blob/1a2b6d1e6b04c8cd05f5676ff7daa877efd99f08/datasette_scan/__init__.py#L38-L58\">scan_directories() function</a>.</p>\n<p>I've been exploring this pattern for other, non-Go binaries recently - here's <a href=\"https://github.com/simonw/tools/blob/main/python/livestream-gif.py\">a recent script</a> that depends on <a href=\"https://pypi.org/project/static-ffmpeg/\">static-ffmpeg</a> to ensure that <code>ffmpeg</code> is available for the script to use.</p>\n<h4 id=\"building-python-wheels-from-go-packages-with-go-to-wheel\">Building Python wheels from Go packages with go-to-wheel</h4>\n<p>After trying this pattern myself a couple of times I realized it would be useful to have a tool to automate the process.</p>\n<p>I first <a href=\"https://claude.ai/share/2d9ced56-b3e8-4651-83cc-860b9b419187\">brainstormed with Claude</a> to check that there was no existing tool to do this. It pointed me to <a href=\"https://www.maturin.rs/bindings.html#bin\">maturin bin</a> which helps distribute Rust projects using Python wheels, and <a href=\"https://github.com/Bing-su/pip-binary-factory\">pip-binary-factory</a> which bundles all sorts of other projects, but did not identify anything that addressed the exact problem I was looking to solve.</p>\n<p>So I <a href=\"https://gisthost.github.io/?41f04e4eb823b1ceb888d9a28c2280dd/index.html\">had Claude Code for web build the first version</a>, then refined the code locally on my laptop with the help of more Claude Code and a little bit of OpenAI Codex too, just to mix things up.</p>\n<p>The full documentation is in the <a href=\"https://github.com/simonw/go-to-wheel\">simonw/go-to-wheel</a> repository. I've published that tool to PyPI so now you can run it using:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx go-to-wheel --help</pre></div>\n<p>The <code>sqlite-scanner</code> package you can <a href=\"https://pypi.org/project/sqlite-scanner/\">see on PyPI</a> was built using <code>go-to-wheel</code> like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx go-to-wheel <span class=\"pl-k\">~</span>/dev/sqlite-scanner \\\n  --set-version-var main.version \\\n  --version 0.1.1 \\\n  --readme README.md \\\n  --author <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Simon Willison<span class=\"pl-pds\">'</span></span> \\\n  --url https://github.com/simonw/sqlite-scanner \\\n  --description <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Scan directories for SQLite databases<span class=\"pl-pds\">'</span></span></pre></div>\n<p>This created a set of wheels in the <code>dist/</code> folder. I tested one of them like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uv run --with dist/sqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl \\\n  sqlite-scanner --version</pre></div>\n<p>When that spat out the correct version number I was confident everything had worked as planned, so I pushed the whole set of wheels to PyPI using <code>twine upload</code> like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>uvx twine upload dist/<span class=\"pl-k\">*</span></pre></div>\n<p>I had to paste in a PyPI API token I had saved previously and that was all it took.</p>\n<h4 id=\"i-expect-to-use-this-pattern-a-lot\">I expect to use this pattern a lot</h4>\n<p><code>sqlite-scanner</code> is very clearly meant as a proof-of-concept for this wider pattern - Python is very much capable of recursively crawling a directory structure looking for files that start with a specific byte prefix on its own!</p>\n<p>That said, I think there's a <em>lot</em> to be said for this pattern. Go is a great complement to Python - it's fast, compiles to small self-contained binaries, has excellent concurrency support and a rich ecosystem of libraries.</p>\n<p>Go is similar to Python in that it has a strong standard library. Go is particularly good for HTTP tooling - I've built several HTTP proxies in the past using Go's excellent <code>net/http/httputil.ReverseProxy</code> handler.</p>\n<p>I've also been experimenting with <a href=\"https://github.com/wazero/wazero\">wazero</a>, Go's robust and mature zero dependency WebAssembly runtime as part of my ongoing quest for the ideal sandbox for running untrusted code. <a href=\"https://github.com/simonw/research/tree/main/wasm-repl-cli\">Here's my latest experiment</a> with that library.</p>\n<p>Being able to seamlessly integrate Go binaries into Python projects without the end user having to think about Go at all - they <code>pip install</code> and everything Just Works - feels like a valuable addition to my toolbox.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/go\">go</a>, <a href=\"https://simonwillison.net/tags/packaging\">packaging</a>, <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/pypi\">pypi</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sqlite\">sqlite</a>, <a href=\"https://simonwillison.net/tags/datasette\">datasette</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/uv\">uv</a></p>",
          "content": "<p>I've been exploring Go for building small, fast and self-contained binary applications recently. I'm enjoying how there's generally one obvious way to do things and the resulting code is boring and readable - and something that LLMs are very competent at writing. The one catch is distribution, but it turns out publishing Go binaries to PyPI means any Go binary can be just a <code>uvx package-name</code> call away.</p>\n<h4 id=\"sqlite-scanner\">sqlite-scanner</h4>\n<p><a href=\"https://github.com/simonw/sqlite-scanner\">sqlite-scanner</a> is my new Go CLI tool for scanning a filesystem for SQLite database files.</p>\n<p>It works by checking if the first 16 bytes of the file exactly match the SQLite magic number sequence <code>SQLite format 3\\x00</code>. It can search one or more folders recursively, spinning up concurrent goroutines to accelerate the scan. It streams out results as it finds them in plain text, JSON or newline-delimited JSON. It can optionally display the file sizes as well.</p>\n<p>To try it out you can download a release from the <a href=\"https://github.com/simonw/sqlite-scanner/releases\">GitHub releases</a> - and then <a href=\"https://support.apple.com/en-us/102445\">jump through macOS hoops</a> to execute an \"unsafe\" binary. Or you can clone the repo and compile it with Go. Or... you can run the binary like this:</p>\n<pre><code>uvx sqlite-scanner\n</code></pre>\n<p>By default this will search your current directory for SQLite databases. You can pass one or more directories as arguments:</p>\n<pre><code>uvx sqlite-scanner ~ /tmp\n</code></pre>\n<p>Add <code>--json</code> for JSON output, <code>--size</code> to include file sizes or <code>--jsonl</code> for newline-delimited JSON. Here's a demo:</p>\n<pre><code>uvx sqlite-scanner ~ --jsonl --size\n</code></pre>\n<p><img alt=\"running that command produces a sequence of JSON objects, each with a path and a size key\" src=\"https://static.simonwillison.net/static/2025/sqlite-scanner-demo.gif\" /></p>\n<p>If you haven't",
          "depth": 0.7,
          "published": "2026-02-04T14:59:47+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 37.0,
      "rank": 5
    },
    {
      "canonical_name": "网络安全预防措施",
      "category": "技术探讨",
      "articles": [
        {
          "title": "Weekly Update 489",
          "link": "https://www.troyhunt.com/weekly-update-489/",
          "source": "troyhunt.com",
          "summary": "<p>This week I&apos;m in Hong Kong, and the day after recording, I gave the talk shown in the image above at INTERPOL&apos;s Cybercrime Expert Group. I posted a little about this on Facebook and LinkedIn, but thought I&apos;d expand on what really stuck with</p>",
          "content": "<img alt=\"Weekly Update 489\" src=\"https://www.troyhunt.com/content/images/2026/02/Splash-Template@1x_1.jpg\" /><p>This week I&apos;m in Hong Kong, and the day after recording, I gave the talk shown in the image above at INTERPOL&apos;s Cybercrime Expert Group. I posted a little about this on Facebook and LinkedIn, but thought I&apos;d expand on what really stuck with me after watching other speakers: the effort agencies are putting into cybercrime prevention. It&apos;s very easy for folks to judge law enforcement solely on what they see from the outside, and that&apos;s mostly going after offenders and taking down criminal infrastructure. But the bit I&apos;m increasingly seeing behind the scenes is a push to help kids (the sorts of hackers I usually interact with are teenagers or young adults at most) make better choices when they&apos;re faced with a pathway into cybercrime. The transition from minor offences (game cheats and DDoS&apos;ing) to full-on cybercriminals (hacking and extortion) is very well-known, and intervening at the right time can not only make a difference to the impact of data breaches on all of us, but it can also make a massive difference to these kids&apos; lives. These agencies are underfunded and understaffed compared to the scale of the problem, so making the time to come visit and find some ways to help in our little corner of the data breach world is a no-brainer &#x1f60a;</p>\n<!--kg-card-begin: html-->\n<div><div style=\"width: 170px; display: inline-block; margin-right: 3px;\"><a href=\"https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699?ref=troy-hunt\"><img alt=\"Weekly Update 489\" src=\"https://www.troyhunt.com/content/images/2018/05/Listen-on-Apple-Podcasts.svg\" /></a></div><div style=\"width: 175px; display: inline-block; margin-right: 3px;\"><a href=\"https://www.youtube.com/playlist?list=PL7LAAxaabizMAXnJe0s3xjQ30q12EVmjt&amp;ref=troyhunt.com\"><img alt=\"Weekly Update 489\" src=\"https://www.troyhunt.com/content/im",
          "depth": 0.7,
          "published": "2026-02-04T02:31:18+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 37.0,
      "rank": 6
    },
    {
      "canonical_name": "Mistral AI发布Voxtral Transcribe 2",
      "category": "行业动态",
      "articles": [
        {
          "title": "Voxtral transcribes at the speed of sound",
          "link": "https://simonwillison.net/2026/Feb/4/voxtral-2/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<p><strong><a href=\"https://mistral.ai/news/voxtral-transcribe-2\">Voxtral transcribes at the speed of sound</a></strong></p>\nMistral just released Voxtral Transcribe 2 - a family of two new models, one open weights, for transcribing audio to text. This is the latest in their Whisper-like model family, and a sequel to the original Voxtral which they released <a href=\"https://simonwillison.net/2025/Jul/16/voxtral/\">in July 2025</a>.</p>\n<p>Voxtral Realtime - official name <code>Voxtral-Mini-4B-Realtime-2602</code> - is the open weights (Apache-2.0) model, available as a <a href=\"https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602\">8.87GB download from Hugging Face</a>.</p>\n<p>You can try it out in this <a href=\"https://huggingface.co/spaces/mistralai/Voxtral-Mini-Realtime\">live demo</a> - don't be put off by the \"No microphone found\" message, clicking \"Record\" should have your browser request permission and then start the demo working. I was very impressed by the demo - I talked quickly and used jargon like Django and WebAssembly and it correctly transcribed my text within moments of me uttering each sound. </p>\n<p>The closed weight model is called <code>voxtral-mini-latest</code> and can be accessed via the Mistral API, using calls that look something like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -X POST <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://api.mistral.ai/v1/audio/transcriptions<span class=\"pl-pds\">\"</span></span> \\\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Authorization: Bearer <span class=\"pl-smi\">$MISTRAL_API_KEY</span><span class=\"pl-pds\">\"</span></span> \\\n  -F model=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>voxtral-mini-latest<span class=\"pl-pds\">\"</span></span> \\\n  -F file=@<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Pelican talk at the library.m4a<span class=\"pl-pds\">\"</span></span> \\\n  -F diarize=true \\\n  -F context_bias=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Datasette<span class=\"pl-pds\">\"</span></span> \\\n  -F timestamp_granularities=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>segment<span class=\"pl-pds\">\"</span></span></pre></div>\n\n<p>It's priced at $0.003/minute, which is $0.18/hour.</p>\n<p>The Mistral API console now has a <a href=\"https://console.mistral.ai/build/audio/speech-to-text\">speech-to-text playground</a> for exercising the new model and it is <em>excellent</em>. You can upload an audio file and promptly get a diarized transcript in a pleasant interface, with options to download the result in text, SRT or JSON format.</p>\n<p><img alt=\"Screenshot of a speech-to-text transcription interface for a file named &quot;Pelican talk at the library.m4a&quot;. The toolbar shows &quot;Speech to text&quot; with Code, Transcribe, and Download buttons. The transcript shows timestamped segments from 5:53 to 6:53 with a speaker icon, reading: &quot;5:53 – 6:01 So pelicans love to, they're very good at getting the most they can out of the topography when they're flying. 6:01 – 6:06 And our winds come in from the northwest and they hit those bluffs and they're deflected up. 6:07 – 6:18 And they will sit right, they'll fly north into a wind like five feet off those bluffs, but just five or ten feet off the surface because the winds dissipate. 6:19 – 6:22 And they will surf that bluff all the way north. 6:23 – 6:30 So you'll see a wind from the north at 15 miles an hour, and the pelicans are flying north into that wind and not flapping their wings. 6:31 – 6:33 And it's one of the coolest things. 6:33 – 6:35 You can only find it on San Francisco Coast. 6:36 – 6:39 Where right where the bluffs are steep. 6:41 – 6:43 Pacifica, you can find them there. 6:43 – 6:51 They like their, what we call pier bums, which are typically pelicans that have, are in some sort of trouble. 6:51 – 6:53 They're unable to catch food.&quot; The segment at 6:41–6:43 is highlighted in yellow. An audio waveform is shown at the bottom with a playhead near 6:40. Stats in the lower right show 53.90s, 7946.00s, and #45833.\" src=\"https://static.simonwillison.net/static/2025/mistral-transcript-ui.jpg\" />\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46886735\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/hugging-face\">hugging-face</a>, <a href=\"https://simonwillison.net/tags/mistral\">mistral</a>, <a href=\"https://simonwillison.net/tags/speech-to-text\">speech-to-text</a></p>",
          "content": "<p><strong><a href=\"https://mistral.ai/news/voxtral-transcribe-2\">Voxtral transcribes at the speed of sound</a></strong></p>\nMistral just released Voxtral Transcribe 2 - a family of two new models, one open weights, for transcribing audio to text. This is the latest in their Whisper-like model family, and a sequel to the original Voxtral which they released <a href=\"https://simonwillison.net/2025/Jul/16/voxtral/\">in July 2025</a>.</p>\n<p>Voxtral Realtime - official name <code>Voxtral-Mini-4B-Realtime-2602</code> - is the open weights (Apache-2.0) model, available as a <a href=\"https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602\">8.87GB download from Hugging Face</a>.</p>\n<p>You can try it out in this <a href=\"https://huggingface.co/spaces/mistralai/Voxtral-Mini-Realtime\">live demo</a> - don't be put off by the \"No microphone found\" message, clicking \"Record\" should have your browser request permission and then start the demo working. I was very impressed by the demo - I talked quickly and used jargon like Django and WebAssembly and it correctly transcribed my text within moments of me uttering each sound. </p>\n<p>The closed weight model is called <code>voxtral-mini-latest</code> and can be accessed via the Mistral API, using calls that look something like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -X POST <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://api.mistral.ai/v1/audio/transcriptions<span class=\"pl-pds\">\"</span></span> \\\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Authorization: Bearer <span class=\"pl-smi\">$MISTRAL_API_KEY</span><span class=\"pl-pds\">\"</span></span> \\\n  -F model=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>voxtral-mini-latest<span class=\"pl-pds\">\"</span></span> \\\n  -F file=@<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Pelican talk at the library.m4a<span class=\"pl-pds\">\"</span></span> \\\n  -F diarize=true \\\n  -F context_bias=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Datasette<span c",
          "depth": 0.8,
          "published": "2026-02-04T22:42:34+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 7
    },
    {
      "canonical_name": "超级碗LX音乐赌注活动",
      "category": "体育娱乐",
      "articles": [
        {
          "title": "Super Bowl LX creates an opportunity for symphonic friendly wagering",
          "link": "https://devblogs.microsoft.com/oldnewthing/20260204-01/?p=112039",
          "source": "devblogs.microsoft.com/oldnewthing",
          "summary": "<p>Betting classical music.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/oldnewthing/20260204-01/?p=112039\">Super Bowl LX creates an opportunity for symphonic friendly wagering</a> appeared first on <a href=\"https://devblogs.microsoft.com/oldnewthing\">The Old New Thing</a>.</p>",
          "content": "<p>This upcoming Sunday is Super Bowl LX, the championship game of the top professional <a href=\"https://en.wikipedia.org/wiki/American_football\"> American football</a> league. The Super Bowl thinks that it is so important that it uses Roman numerals.)</p>\n<p>The Super Bowl is the single largest sporting event in the United States. <a href=\"https://devblogs.microsoft.com/oldnewthing/20070202-17/?p=28173\" title=\"Super Bowl Sunday: The day the entire country stops doing anything\"> The entire country grinds to a halt when the game is on</a>. If you aren&#8217;t interested in the game, <a href=\"https://devblogs.microsoft.com/oldnewthing/20150130-01/?p=44783\" title=\"Got errands? Now is the time\"> it&#8217;s a great time to do public photography or run errands</a>.</p>\n<p>Traditionally, the mayors of the home cities of the two teams competing in the game make a friendly wager, with each mayor offering to send the other mayor some local products if their team loses. For example, in 2014, <a href=\"https://www.sbnation.com/nfl/2014/1/27/5351434/super-bowl-2014-seattle-denver-mayor-bet-seahawks-broncos-macklemore\" title=\"Super Bowl 2014: Seattle, Denver mayors agree to bet\"> the mayors of Seattle and Denver wagered local foods and products as well as having to wear clothing inspired by the other team&#8217;s city</a>.</p>\n<p>Sometimes other city organizations get into the friendly wagering spirit. In 2018, <a href=\"https://www.wqxr.org/story/its-philadelphia-orchestra-and-boston-symphony-have-super-bowl-bet/\" title=\"It's On: The Philadelphia Orchestra and the Boston Symphony Have a Super Bowl Bet\"> the Philadelphia Orchestra and Boston Symphony agreed that the losing city&#8217;s conductor would have to wear the winning city&#8217;s jersey at their next rehearsal</a>.</p>\n<p>But certainly we can do better than that.</p>\n<p>The two teams competing in Super Bowl LX are the Seattle Seahawks and the New England Patriots (based near Boston). I think the Seattle Symphony and the Bo",
          "depth": 0.8,
          "published": "2026-02-04T15:00:01+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.8,
      "heat_score": 35.0,
      "rank": 8
    },
    {
      "canonical_name": "文化破坏行为",
      "category": "社会议题",
      "articles": [
        {
          "title": "Spotlighting The World Factbook as We Bid a Fond Farewell",
          "link": "https://simonwillison.net/2026/Feb/5/the-world-factbook/#atom-everything",
          "source": "simonwillison.net",
          "summary": "<p><strong><a href=\"https://www.cia.gov/stories/story/spotlighting-the-world-factbook-as-we-bid-a-fond-farewell/\">Spotlighting The World Factbook as We Bid a Fond Farewell</a></strong></p>\nSomewhat devastating news today from CIA:</p>\n<blockquote>\n<p>One of CIA’s oldest and most recognizable intelligence publications, The World Factbook, has sunset.</p>\n</blockquote>\n<p>There's not even a hint as to <em>why</em> they decided to stop maintaining this publication, which has been their most useful public-facing initiative since 1971 and a cornerstone of the public internet since 1997.</p>\n<p>In a bizarre act of cultural vandalism they've not just removed the entire site (including the archives of previous versions) but they've also set every single page to be a 302 redirect to their closure announcement.</p>\n<p>The Factbook has been released into the public domain since the start. There's no reason not to continue to serve archived versions - a banner at the top of the page saying it's no longer maintained would be much better than removing all of that valuable content entirely.</p>\n<p>Up until 2020 the CIA published annual zip file archives of the entire site. Those are available (along with the rest of the Factbook) <a href=\"https://web.archive.org/web/20260203124934/https://www.cia.gov/the-world-factbook/about/archives/\">on the Internet Archive</a>.</p>\n<p>I downloaded the 384MB <code>.zip</code> file for the year 2020 and extracted it into a new GitHub repository, <a href=\"https://github.com/simonw/cia-world-factbook-2020/\">simonw/cia-world-factbook-2020</a>. I've enabled GitHub Pages for that repository so you can browse the archived copy at <a href=\"https://simonw.github.io/cia-world-factbook-2020\">simonw.github.io/cia-world-factbook-2020/</a>.</p>\n<p><img alt=\"Screenshot of the CIA World Factbook website homepage. Header reads &quot;THE WORLD FACTBOOK&quot; with a dropdown labeled &quot;Please select a country to view.&quot; Navigation tabs: ABOUT, REFERENCES, APPENDICES, FAQs. Section heading &quot;WELCOME TO THE WORLD FACTBOOK&quot; followed by descriptive text: &quot;The World Factbook provides information on the history, people and society, government, economy, energy, geography, communications, transportation, military, and transnational issues for 267 world entities. The Reference tab includes: a variety of world, regional, country, ocean, and time zone maps; Flags of the World; and a Country Comparison function that ranks the country information and data in more than 75 Factbook fields.&quot; A satellite image of Earth is displayed on the right. Below it: &quot;WHAT'S NEW :: Today is: Wednesday, February 4.&quot; Left sidebar links with icons: WORLD TRAVEL FACTS, ONE-PAGE COUNTRY SUMMARIES, REGIONAL AND WORLD MAPS, FLAGS OF THE WORLD, GUIDE TO COUNTRY COMPARISONS. Right side shows news updates dated December 17, 2020 about Electricity access and new Economy fields, and December 10, 2020 about Nepal and China agreeing on the height of Mount Everest at 8,848.86 meters. A &quot;VIEW ALL UPDATES&quot; button appears at the bottom.\" src=\"https://static.simonwillison.net/static/2025/factbook-2020.jpg\" /></p>\n<p>Here's a neat example of the editorial voice of the Factbook from the <a href=\"https://simonw.github.io/cia-world-factbook-2020/docs/whatsnew.html\">What's New page</a>, dated December 10th 2020:</p>\n<blockquote>\n<p>Years of wrangling were brought to a close this week when officials from Nepal and China announced that they have agreed on the height of Mount Everest. The mountain sits on the border between Nepal and Tibet (in western China), and its height changed slightly following an earthquake in 2015. The new height of 8,848.86 meters is just under a meter higher than the old figure of 8,848 meters. <em>The World Factbook</em> rounds the new measurement to 8,849 meters and this new height has been entered throughout the <em>Factbook</em> database.</p>\n</blockquote>\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46891794\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/cia\">cia</a>, <a href=\"https://simonwillison.net/tags/github\">github</a>, <a href=\"https://simonwillison.net/tags/internet-archive\">internet-archive</a></p>",
          "content": "<p><strong><a href=\"https://www.cia.gov/stories/story/spotlighting-the-world-factbook-as-we-bid-a-fond-farewell/\">Spotlighting The World Factbook as We Bid a Fond Farewell</a></strong></p>\nSomewhat devastating news today from CIA:</p>\n<blockquote>\n<p>One of CIA’s oldest and most recognizable intelligence publications, The World Factbook, has sunset.</p>\n</blockquote>\n<p>There's not even a hint as to <em>why</em> they decided to stop maintaining this publication, which has been their most useful public-facing initiative since 1971 and a cornerstone of the public internet since 1997.</p>\n<p>In a bizarre act of cultural vandalism they've not just removed the entire site (including the archives of previous versions) but they've also set every single page to be a 302 redirect to their closure announcement.</p>\n<p>The Factbook has been released into the public domain since the start. There's no reason not to continue to serve archived versions - a banner at the top of the page saying it's no longer maintained would be much better than removing all of that valuable content entirely.</p>\n<p>Up until 2020 the CIA published annual zip file archives of the entire site. Those are available (along with the rest of the Factbook) <a href=\"https://web.archive.org/web/20260203124934/https://www.cia.gov/the-world-factbook/about/archives/\">on the Internet Archive</a>.</p>\n<p>I downloaded the 384MB <code>.zip</code> file for the year 2020 and extracted it into a new GitHub repository, <a href=\"https://github.com/simonw/cia-world-factbook-2020/\">simonw/cia-world-factbook-2020</a>. I've enabled GitHub Pages for that repository so you can browse the archived copy at <a href=\"https://simonw.github.io/cia-world-factbook-2020\">simonw.github.io/cia-world-factbook-2020/</a>.</p>\n<p><img alt=\"Screenshot of the CIA World Factbook website homepage. Header reads &quot;THE WORLD FACTBOOK&quot; with a dropdown labeled &quot;Please select a country to view.&quot; Navigation tabs: ABOUT, REFERENCES, A",
          "depth": 0.7,
          "published": "2026-02-05T00:23:38+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 32.0,
      "rank": 9
    },
    {
      "canonical_name": "14亿美元债务累积",
      "category": "行业动态",
      "articles": [
        {
          "title": "Radio Shack’s 2015 bankruptcy",
          "link": "https://dfarq.homeip.net/radio-shacks-2015-bankruptcy/?utm_source=rss&utm_medium=rss&utm_campaign=radio-shacks-2015-bankruptcy",
          "source": "dfarq.homeip.net",
          "summary": "<p>On February 5, 2015, Radio Shack filed for chapter 11 bankruptcy after posting losses 11 quarters in a row and accumulating $1.4 billion in debt. While not officially the end of Radio Shack, the Radio Shack that still exists today</p>\n<p>The post <a href=\"https://dfarq.homeip.net/radio-shacks-2015-bankruptcy/\">Radio Shack&#8217;s 2015 bankruptcy</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
          "content": "<p>On February 5, 2015, Radio Shack filed for chapter 11 bankruptcy after posting losses 11 quarters in a row and accumulating $1.4 billion in debt. While not officially the end of Radio Shack, the Radio Shack that still exists today</p>\n<p>The post <a href=\"https://dfarq.homeip.net/radio-shacks-2015-bankruptcy/\">Radio Shack&#8217;s 2015 bankruptcy</a> appeared first on <a href=\"https://dfarq.homeip.net\">The Silicon Underground</a>.</p>",
          "depth": 0.7,
          "published": "2026-02-04T12:00:33+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 32.0,
      "rank": 10
    },
    {
      "canonical_name": "Justin Key新书发布",
      "category": "行业动态",
      "articles": [
        {
          "title": "Pluralistic: Justin Key's \"The Hospital at the End Of the World\" (04 Feb 2026)",
          "link": "https://pluralistic.net/2026/02/04/slice-bees/",
          "source": "pluralistic.net",
          "summary": "Today's links Justin Key's \"The Hospital at the End Of the World\": A biopunk medical thriller from a major new talent. Hey look at this: Delights to delectate. Object permanence: Coconut volunteers; Astro Noise; Rich old men behind \"Millennials Rising\"; Stop the \"Stop the Steal\" steal; \"Chasing Shadows.\" Upcoming appearances: Where to find me. Recent appearances: Where I've been. Latest books: You keep readin' em, I'll keep writin' 'em. Upcoming books: Like I said, I'll keep writin' 'em. Colophon: All the rest. Justin Key's \"The Hospital at the End Of the World\" (permalink) Justin C. Key is one of the most exciting new science fiction writers of this decade and today, Harpercollins publishes his debut novel, The Hospital at the End of the World: https://www.harpercollins.com/products/the-hospital-at-the-end-of-the-world-justin-c-key?variant=43822999928866 I've followed Key's work for more than a decade, ever since I met him as a student while teaching at the Clarion West writers' workshop in Seattle. At the time, Key impressed me &#8211; a standout writer in a year full of standouts &#8211; and I wasn't surprised in the least when Harpercollins published a collection of his afrofuturist/Black horror stories, The World Wasn't Ready For You, in 2023: https://pluralistic.net/2023/09/19/justin-c-key/#clarion-west-2015 This is virtually unheard of. Major genre publishers generally don't publish short story collections at all, let alone short story collections by writers who haven't already established themselves as novelists. The exceptions are rare as hell, and they're names to conjure with: Ted Chiang, say, or Kelly Link: https://pluralistic.net/2024/02/13/the-kissing-song/#wrack-and-roll But anyone who read World Wasn't Ready immediately understood why Key's work qualified him for an exception to this iron law of publishing. Key is an MD and a practicing psychiatrist, and he combines keen insights into personal relations and human frailty with a wild imagination, deep compassion, and enviable prose chops. Hospital at the End of the World is Key's first novel, and it's terrific. Set in a not-so-distant future in which an AI-driven health monopolist called The Shepherd Organization controls much of the lives of everyday Americans, Hospital follows Pok, a young New Yorker who dreams of becoming an MD. Pok's father is also a doctor, famous for his empathic, human-centric methods and his scientific theories about the role that \"essence\" (a psychospiritual connection between doctors and patients) plays in clinical settings. The story opens with Pok hotly anticipating an acceptance letter from The Shepherd Organization, and the beginning of his new life as a medical student. But when word arrives, Pok learns that he has been rejected from every medical school in the TSO orbit. In desperate confusion, he works with shadowy hackers in a bid to learn why his impeccable application and his top grades resulted in this total rejection. That's when he learns that someone had sabotaged his application and falsified his grades, and, not long thereafter, he learns that the saboteur was his father. To make things worse, Pok's father has fallen grievously ill &#8211; so ill, in fact, that he ends up in a Shepherd Organization hospital, despite his deep enmity for TSO and its AI-driven practice of medicine. Pok doesn't accompany his father, though &#8211; he has secured a chance to sit a make-up exam in a desperate bid to get into med school. By the time he is finished with his exam, though, he learns that his father has died, and all that is left of him is an AI-powered chatbot that is delivered to Pok's apartment along with a warning to flee, because he is in terrible danger from the Shepherd Organization. Thus begins Pok's tale as he goes underground in a ubiquitous AI surveillance dystopia, seeking sanctuary in New Orleans, hoping to make it to the Hippocrates, the last holdout from America's AI-based medicine and surveillance dystopia. Pok's father learned to practice medicine at Hippocrates, and had urged Pok to study there, even securing a full-ride scholarship for him. But Pok had no interest in the mystical, squishy, sentimental ethos of the Hippocrates, and had been determined to practice the Shepherd Organization's rigorous, cold, data-driven form of medicine. Now, Pok has no choice. Hitchhiking, hopping freight cars, falling into company with other fugitives, Pok makes his way to New Orleans, a city guarded by tall towers that radiate energy that dampens both the punishing weather events that would otherwise drown the city and the data signals by which the Shepherd Organization tracks and controls the American people. This is the book's second act, a medical technothriller that sees Pok as an untrusted outsider in the freshman class at Hippocrates med school, amidst a strange and alarming plague that has sickened the other refugees from TSO America who have taken up residence in New Orleans. Pok has to navigate factions within the med school and in New Orleans society, even as he throws himself into the meat grinder of med school and unravels the secrets of his father and his own birth. What follows is a masterful and suspenseful work of science fiction informed by Key's own medical training and his keen sense of the human psyche. It's one part smart whodunnit, one part heist thriller, and one part revolutionary epic, and at its core is a profound series of provocations and thought experiments about the role that deep human connection and empathy play in medical care. It's a well-structured, well-paced sf novel that probes big, urgent contemporary themes while still engrossing the reader in the intimate human relations of its principals. A wonderful debut novel from a major new writer.` Hey look at this (permalink) Ken MacLeod: Imagined Futures https://plutopia.io/ken-macleod-imagined-futures/ Elbows Up: How Canada Can Disenshittify Its Tech, Reclaim Its Sovereignty, and Launch a New Tech Sector Into a Stable Orbit https://archive.org/details/disenshittification-nation HOPE IS NOW A 501(C)(3) NON-PROFIT ORGANIZATION https://2600.com/content/hope-now-501c3-non-profit-organization Department of Justice appeals Google search monopoly ruling https://www.theverge.com/tech/873438/google-antitrust-case-doj-states-appeal List of Kennedy Center cancellations during the Trump administration https://en.wikipedia.org/wiki/List_of_Kennedy_Center_cancellations_during_the_Trump_administration (h/t Amanda Marcotte) Object permanence (permalink) #20yrsago AOL/Yahoo: our email tax will make the net as good as the post office! https://www.nytimes.com/2006/02/05/technology/postage-is-due-for-companies-sending-email.html #20yrsago Volunteers ferry 15k coconuts every day to Indian temple http://news.bbc.co.uk/2/hi/south_asia/4677320.stm #15yrsago Wikileaks ACTA cables confirm it was a screwjob for the global poor https://arstechnica.com/tech-policy/2011/02/secret-us-cables-reveal-acta-was-far-too-secret/ #10yrsago Laura Poitras’s Astro Noise: indispensable book and gallery show about mass surveillance https://www.wired.com/2016/02/snowdens-chronicler-reveals-her-own-life-under-surveillance/ #10yrsago How to prepare to join the Internet of the dead https://archive.org/details/Online_No_One_Knows_Youre_Dead #10yrsago Who funds the “Millennials Rising” Super PAC? Rich old men. https://web.archive.org/web/20160204223020/https://theintercept.com/2016/02/04/millennials-rising-super-pac-is-95-funded-by-old-men/ #10yrsago They promised us a debate over TPP, then they signed it without any debate https://www.techdirt.com/2016/02/03/countries-sign-tpp-whatever-happened-to-debate-we-were-promised-before-signing/ #5yrsago Stop the \"Stop the Steal\" steal https://pluralistic.net/2021/02/04/vote-machine-tankies/#ess #5yrsago Organic fascism https://pluralistic.net/2021/02/04/vote-machine-tankies/#pastel-q #5yrsago Ron Deibert's \"Chasing Shadows\" https://pluralistic.net/2025/02/04/citizen-lab/#nso-group Upcoming appearances (permalink) Salt Lake City: Enshittification at the Utah Museum of Fine Arts (Tanner Humanities Center), Feb 18 https://tanner.utah.edu/center-events/cory-doctorow/ Montreal (remote): Fedimtl, Feb 24 https://fedimtl.ca/ Victoria: 28th Annual Victoria International Privacy &#38; Security Summit, Mar 3-5 https://www.rebootcommunications.com/event/vipss2026/ Berkeley: Bioneers keynote, Mar 27 https://conference.bioneers.org/ Berlin: Re:publica, May 18-20 https://re-publica.com/de/news/rp26-sprecher-cory-doctorow Berlin: Enshittification at Otherland Books, May 19 https://www.otherland-berlin.de/de/event-details/cory-doctorow.html Hay-on-Wye: HowTheLightGetsIn, May 22-25 https://howthelightgetsin.org/festivals/hay/big-ideas-2 Recent appearances (permalink) Why Everything Got Worse and What to Do About It (Jordan Harbinger) https://www.jordanharbinger.com/cory-doctorow-why-everything-got-worse-and-what-to-do-about-it/ How the Internet Got Worse (Masters in Business) https://www.youtube.com/watch?v=auXlkuVhxMo Enshittification (Jon Favreau/Offline): https://crooked.com/podcast/the-enshittification-of-the-internet-with-cory-doctorow/ Why Big Tech is a Trap for Independent Creators (Stripper News) https://www.youtube.com/watch?v=nmYDyz8AMZ0 Enshittification (Creative Nonfiction podcast) https://brendanomeara.com/episode-507-enshittification-author-cory-doctorow-believes-in-a-new-good-internet/ Latest books (permalink) \"Canny Valley\": A limited edition collection of the collages I create for Pluralistic, self-published, September 2025 \"Enshittification: Why Everything Suddenly Got Worse and What to Do About It,\" Farrar, Straus, Giroux, October 7 2025 https://us.macmillan.com/books/9780374619329/enshittification/ \"Picks and Shovels\": a sequel to \"Red Team Blues,\" about the heroic era of the PC, Tor Books (US), Head of Zeus (UK), February 2025 (https://us.macmillan.com/books/9781250865908/picksandshovels). \"The Bezzle\": a sequel to \"Red Team Blues,\" about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (thebezzle.org). \"The Lost Cause:\" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (http://lost-cause.org). \"The Internet Con\": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (http://seizethemeansofcomputation.org). Signed copies at Book Soup (https://www.booksoup.com/book/9781804291245). \"Red Team Blues\": \"A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before.\" Tor Books http://redteamblues.com. \"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin\", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 https://chokepointcapitalism.com Upcoming books (permalink) \"Unauthorized Bread\": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026 \"Enshittification, Why Everything Suddenly Got Worse and What to Do About It\" (the graphic novel), Firstsecond, 2026 \"The Memex Method,\" Farrar, Straus, Giroux, 2026 \"The Reverse-Centaur's Guide to AI,\" a short book about being a better AI critic, Farrar, Straus and Giroux, June 2026 Colophon (permalink) Today's top sources: Currently writing: \"The Post-American Internet,\" a sequel to \"Enshittification,\" about the better world the rest of us get to have now that Trump has torched America (1011 words today, 21655 total) \"The Reverse Centaur's Guide to AI,\" a short book for Farrar, Straus and Giroux about being an effective AI critic. LEGAL REVIEW AND COPYEDIT COMPLETE. \"The Post-American Internet,\" a short book about internet policy in the age of Trumpism. PLANNING. A Little Brother short story about DIY insulin PLANNING This work &#8211; excluding any serialized fiction &#8211; is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net. https://creativecommons.org/licenses/by/4.0/ Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution. How to get Pluralistic: Blog (no ads, tracking, or data-collection): Pluralistic.net Newsletter (no ads, tracking, or data-collection): https://pluralistic.net/plura-list Mastodon (no ads, tracking, or data-collection): https://mamot.fr/@pluralistic Medium (no ads, paywalled): https://doctorow.medium.com/ Twitter (mass-scale, unrestricted, third-party surveillance and advertising): https://twitter.com/doctorow Tumblr (mass-scale, unrestricted, third-party surveillance and advertising): https://mostlysignssomeportents.tumblr.com/tagged/pluralistic \"When life gives you SARS, you make sarsaparilla\" -Joey \"Accordion Guy\" DeVilla READ CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies (\"BOGUS AGREEMENTS\") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer. ISSN: 3066-764X",
          "content": "<p><!--\nTags:\nnola, louisiana, afrofuturism, medicine, health, science fiction, new orleans, books, reviews, ai, gift guide, justin key, justin c key,\n\nSummary:\nJustin Key's \"The Hospital at the End Of the World\"; Hey look at this; Upcoming appearances; Recent appearances; Latest books; Upcoming books\n\nURL:\nhttps://pluralistic.net/2026/02/04/slice-bees/\n\nTitle:\nPluralistic: Justin Key's \"The Hospital at the End Of the World\" (04 Feb 2026) slice-bees\n\nBullet:\n&#x200d;&#x1f9b8;&#x1f3fb;\n\nSeparator:\n->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->\n\nTop Sources:\nNone\n\n--><br />\n<a href=\"https://pluralistic.net/2026/02/04/slice-bees/\"><img class=\"xmasthead_link\" src=\"https://i0.wp.com/craphound.com/images/04Feb2026.jpg?w=840&#038;ssl=1\" /></a></p>\n<h1 class=\"toch1\">Today's links</h1>\n<ul class=\"toc\">\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#hippocrates\">Justin Key's \"The Hospital at the End Of the World\"</a>: A biopunk medical thriller from a major new talent.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#linkdump\">Hey look at this</a>: Delights to delectate.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#retro\">Object permanence</a>: Coconut volunteers; Astro Noise; Rich old men behind \"Millennials Rising\"; Stop the \"Stop the Steal\" steal; \"Chasing Shadows.\"\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#upcoming\">Upcoming appearances</a>: Where to find me.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#recent\">Recent appearances</a>: Where I've been.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#latest\">Latest books</a>: You keep readin' em, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net/2026/02/04/slice-bees/#upcoming-books\">Upcoming books</a>: Like I said, I'll keep writin' 'em.\n</li>\n<li class=\"xToC\"><a href=\"https://pluralistic.net",
          "depth": 0.7,
          "published": "2026-02-04T15:48:33+00:00",
          "category": "趋势观点"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 32.0,
      "rank": 11
    },
    {
      "canonical_name": "Codex功能增强：技能与自动化支持",
      "category": "行业动态",
      "articles": [
        {
          "title": "OpenAI’s Codex",
          "link": "https://simonwillison.net/2026/Feb/2/introducing-the-codex-app/",
          "source": "daringfireball.net",
          "summary": "<p>Simon Willison:</p>\n\n<blockquote>\n  <p>OpenAI just released a new macOS app for their Codex coding agent. I’ve had a few days of preview access — it’s a solid app that provides a nice UI over the capabilities of the Codex CLI agent and adds some interesting new features, most notably first-class support for Skills, and Automations for running scheduled tasks.</p>\n</blockquote>\n\n<p>Interesting, for sure. But <a href=\"https://daringfireball.net/linked/2026/02/03/xcode-ai-agentic-coding\">super-duper</a> interesting? I don’t know.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2026/02/03/openai-codex\" title=\"Permanent link to ‘OpenAI’s Codex’\">&nbsp;★&nbsp;</a>\n</div>",
          "content": "<p>Simon Willison:</p>\n\n<blockquote>\n  <p>OpenAI just released a new macOS app for their Codex coding agent. I’ve had a few days of preview access — it’s a solid app that provides a nice UI over the capabilities of the Codex CLI agent and adds some interesting new features, most notably first-class support for Skills, and Automations for running scheduled tasks.</p>\n</blockquote>\n\n<p>Interesting, for sure. But <a href=\"https://daringfireball.net/linked/2026/02/03/xcode-ai-agentic-coding\">super-duper</a> interesting? I don’t know.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2026/02/03/openai-codex\" title=\"Permanent link to ‘OpenAI’s Codex’\">&nbsp;★&nbsp;</a>\n</div>",
          "depth": 0.7,
          "published": "2026-02-04T01:40:24+00:00",
          "category": "技术探讨"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 32.0,
      "rank": 12
    },
    {
      "canonical_name": "章节全面重写",
      "category": "文艺作品",
      "articles": [
        {
          "title": "Logic for Programmers New Release and Next Steps",
          "link": "https://buttondown.com/hillelwayne/archive/logic-for-programmers-new-release-and-next-steps/",
          "source": "buttondown.com/hillelwayne",
          "summary": "<p><img alt=\"cover.jpg\" class=\"newsletter-image\" src=\"https://assets.buttondown.email/images/f821145f-d310-403c-88f4-327758a66606.jpg?w=480&amp;fit=max\" /></p>\n<p>It's taken four months, but the next release of <a href=\"https://logicforprogrammers.com\" target=\"_blank\">Logic for Programmers is now available</a>! v0.13 is over 50,000 words, making it both 20% larger than v0.12 and officially the longest thing I have ever written.<sup id=\"fnref:longest\"><a class=\"footnote-ref\" href=\"#fn:longest\">1</a></sup> Full release notes are <a href=\"https://github.com/logicforprogrammers/book-assets/blob/master/CHANGELOG.md\" target=\"_blank\">here</a>, but I'll talk a bit about the biggest changes. </p>\n<p>For one, every chapter has been rewritten. Every single one. They span from <em>relatively</em> minor changes to complete chapter rewrites. After some rough git diffing, I think I deleted about 11,000 words?<sup id=\"fnref:gross-additions\"><a class=\"footnote-ref\" href=\"#fn:gross-additions\">2</a></sup> The biggest change is probably to the Alloy chapter. After many sleepless nights, I realized the right approach wasn't to teach Alloy as a <em>data modeling</em> tool but to teach it as a <em>domain modeling</em> tool. Which technically means the book no longer covers data modeling.</p>\n<p>There's also a lot more connections between the chapters. The introductory math chapter, for example, foreshadows how each bit of math will be used in the future techniques. I also put more emphasis on the general \"themes\" like the expressiveness-guarantees tradeoff (working title). One theme I'm really excited about is compatibility (extremely working title). It turns out that the <a href=\"https://buttondown.com/hillelwayne/archive/the-liskov-substitution-principle-does-more-than/\" target=\"_blank\">Liskov substitution principle</a>/subtyping in general, <a href=\"https://buttondown.com/hillelwayne/archive/refinement-without-specification/\" target=\"_blank\">database migrations</a>, backwards-compatible API changes, and <a href=\"https://hillelwayne.com/post/refinement/\" target=\"_blank\">specification refinement</a> all follow <em>basically</em> the same general principles. I'm calling this \"compatibility\" for now but prolly need a better name.</p>\n<p>Finally, there's just a lot more new topics in the various chapters. <code>Testing</code> properly covers structural and metamorphic properties. <code>Proofs</code> covers proof by induction and proving recursive functions (in an exercise). <code>Logic Programming</code> now finally has a section on answer set programming. You get the picture.</p>\n<h3>Next Steps</h3>\n<p>There's a lot I still want to add to the book: proper data modeling, data structures, type theory, model-based testing, etc. But I've added new material for two year, and if I keep going it will never get done. So with this release, all the content is in!</p>\n<p>Just like all the content was in <a href=\"https://buttondown.com/hillelwayne/archive/five-unusual-raku-features/\" target=\"_blank\">two Novembers ago</a> and <a href=\"https://buttondown.com/hillelwayne/archive/logic-for-programmers-project-update/\" target=\"_blank\">two Januaries ago</a> and <a href=\"https://buttondown.com/hillelwayne/archive/logic-for-programmers-turns-one/\" target=\"_blank\">last July</a>. To make it absolutely 100% for sure that I won't be tempted to add anything else, I passed the whole manuscript over to a copy editor. So if I write more, it won't get edits. That's a pretty good incentive to stop.</p>\n<p>I also need to find a technical reviewer and proofreader. Once all three phases are done then it's \"just\" a matter of fixing the layout and finding a good printer. I don't know what the timeline looks like but I really want to have something I can hold in my hands before the summer.</p>\n<p>(I also need to get notable-people testimonials. Hampered a little in this because I'm trying real hard not to quid-pro-quo, so I'd like to avoid anybody who helped me or is mentioned in the book. And given I tapped most of my network to help me... I've got some ideas though!)</p>\n<p>There's still a lot of work ahead. Even so, for the first time in two years I don't have research to do or sections to write and it feels so crazy. Maybe I'll update my blog again! Maybe I'll run a workshop! Maybe I'll go outside if Chicago ever gets above 6°F! </p>\n<hr />\n<h2>Conference Season</h2>\n<p>After a pretty slow 2025, the 2026 conference season is looking to be pretty busy! Here's where I'm speaking so far:</p>\n<ul>\n<li><a href=\"https://qconlondon.com/\" target=\"_blank\">QCon London</a>, March 16-19</li>\n<li><a href=\"https://craft-conf.com/2026\" target=\"_blank\">Craft Conference</a>, Budapest, June 4-5</li>\n<li><a href=\"https://softwareshould.work/\" target=\"_blank\">Software Should Work</a>, Missouri, July 16-17</li>\n<li><a href=\"https://hfpug.org/\" target=\"_blank\">Houston Functional Programmers</a>, Virtual, December 3</li>\n</ul>\n<p>For the first three I'm giving variations of my talk \"How to find bugs in systems that don't exist\", which I gave last year at <a href=\"https://systemsdistributed.com/\" target=\"_blank\">Systems Distributed</a>. Last one will ideally be a talk based on LfP. </p>\n<div class=\"footnote\">\n<hr />\n<ol>\n<li id=\"fn:longest\">\n<p>The second longest was my 2003 NaNoWriMo. The third longest was <em>Practical TLA+</em>.&#160;<a class=\"footnote-backref\" href=\"#fnref:longest\" title=\"Jump back to footnote 1 in the text\">&#8617;</a></p>\n</li>\n<li id=\"fn:gross-additions\">\n<p>This means I must have written 20,000 words total. For comparison, the v0.1 release was 19,000 words.&#160;<a class=\"footnote-backref\" href=\"#fnref:gross-additions\" title=\"Jump back to footnote 2 in the text\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
          "content": "<p><img alt=\"cover.jpg\" class=\"newsletter-image\" src=\"https://assets.buttondown.email/images/f821145f-d310-403c-88f4-327758a66606.jpg?w=480&amp;fit=max\" /></p>\n<p>It's taken four months, but the next release of <a href=\"https://logicforprogrammers.com\" target=\"_blank\">Logic for Programmers is now available</a>! v0.13 is over 50,000 words, making it both 20% larger than v0.12 and officially the longest thing I have ever written.<sup id=\"fnref:longest\"><a class=\"footnote-ref\" href=\"#fn:longest\">1</a></sup> Full release notes are <a href=\"https://github.com/logicforprogrammers/book-assets/blob/master/CHANGELOG.md\" target=\"_blank\">here</a>, but I'll talk a bit about the biggest changes. </p>\n<p>For one, every chapter has been rewritten. Every single one. They span from <em>relatively</em> minor changes to complete chapter rewrites. After some rough git diffing, I think I deleted about 11,000 words?<sup id=\"fnref:gross-additions\"><a class=\"footnote-ref\" href=\"#fn:gross-additions\">2</a></sup> The biggest change is probably to the Alloy chapter. After many sleepless nights, I realized the right approach wasn't to teach Alloy as a <em>data modeling</em> tool but to teach it as a <em>domain modeling</em> tool. Which technically means the book no longer covers data modeling.</p>\n<p>There's also a lot more connections between the chapters. The introductory math chapter, for example, foreshadows how each bit of math will be used in the future techniques. I also put more emphasis on the general \"themes\" like the expressiveness-guarantees tradeoff (working title). One theme I'm really excited about is compatibility (extremely working title). It turns out that the <a href=\"https://buttondown.com/hillelwayne/archive/the-liskov-substitution-principle-does-more-than/\" target=\"_blank\">Liskov substitution principle</a>/subtyping in general, <a href=\"https://buttondown.com/hillelwayne/archive/refinement-without-specification/\" target=\"_blank\">database migrations</a>, backwards-compatibl",
          "depth": 0.7,
          "published": "2026-02-04T14:00:00+00:00",
          "category": "实战技巧"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.7,
      "heat_score": 32.0,
      "rank": 13
    },
    {
      "canonical_name": "GPT-5半周年纪念",
      "category": "行业动态",
      "articles": [
        {
          "title": "Sam Altman and the day Nvidia’s meteoric rise came to an end",
          "link": "https://garymarcus.substack.com/p/sam-altman-and-the-day-nvidias-meteoric",
          "source": "garymarcus.substack.com",
          "summary": "Happy half anniversary, GPT-5",
          "content": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!zDkI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0509b4-a973-4538-850b-b76417f9cc49_1619x898.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"808\" src=\"https://substackcdn.com/image/fetch/$s_!zDkI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0509b4-a973-4538-850b-b76417f9cc49_1619x898.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></d",
          "depth": 0.6,
          "published": "2026-02-04T20:31:13+00:00",
          "category": "行业动态"
        }
      ],
      "total_mentions": 1,
      "avg_depth": 0.6,
      "heat_score": 29.0,
      "rank": 14
    }
  ]
}